<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Kali更新时出现签名无效、Hash和校验不符的问题]]></title>
    <url>%2FLinux%2FKali%2FKali%E6%9B%B4%E6%96%B0%E6%97%B6%E5%87%BA%E7%8E%B0%E7%AD%BE%E5%90%8D%E6%97%A0%E6%95%88%E3%80%81Hash%E5%92%8C%E6%A0%A1%E9%AA%8C%E4%B8%8D%E7%AC%A6%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[起因今天在整我虚拟机上的Kali系统，安装东西的时候出现：1E: 有几个软件包无法下载，要不运行 apt-get update 或者加上 --fix-missing 的选项再试试？ 问题1然后，使用sudo apt-get update &amp;&amp; sudo apt-get upgrade更新的时候，出现：12345错误:4 http://mirrors.ustc.edu.cn/kali kali-rolling InRelease 下列签名无效： EXPKEYSIG ED444FF07D8D0BF6 Kali Linux Repository &lt;devel@kali.org&gt;已下载 30.5 kB，耗时 1秒 (24.5 kB/s)正在读取软件包列表... 完成W: 校验数字签名时出错。此仓库未被更新，所以仍然使用此前的索引文件。GPG 错误：http://mirrors.ustc.edu.cn/kali kali-rolling InRelease: 下列签名无效： EXPKEYSIG ED444FF07D8D0BF6 Kali Linux Repository &lt;devel@kali.org&gt; 然后，使用apt-key list查看，发现有个keyring.gpg过期了：12345/etc/apt/trusted.gpg.d/kali-archive-keyring.gpg-----------------------------------------------pub rsa4096 2012-03-05 [SC] [expired: 2018-02-02] 44C6 513A 8E4F B3D3 0875 F758 ED44 4FF0 7D8D 0BF6uid [ expired] Kali Linux Repository &lt;devel@kali.org&gt; 解决方法：首先使用以下命令：1wget https://http.kali.org/kali/pool/main/k/kali-archive-keyring/kali-archive-keyring_2018.1_all.deb 然后运行：1apt install ./kali-archive-keyring_2018.1_all.deb 这个问题就解决了 问题2但是这个时候再使用sudo apt-get update &amp;&amp; sudo apt-get upgrade，又出现了另一个错误：1234567891011121314E: 无法下载 http://172.17.0.111/cache/7/02/mirrors.ustc.edu.cn/01fbf595fbfd06d37257ada0be178b0b/Sources.gz Hash 校验和不符 Hashes of expected file: - Filesize:11822681 [weak] - SHA256:32a1f422348290dbe259aa9c57ea3c8e9622b4f6e808dcc7694163d3d8622c26 - SHA1:872723d9ba3977a72101d847ce341c503e681cef [weak] - MD5Sum:c4c623d3ec6792e477ef37c62561021a [weak] Hashes of received file: - SHA256:2ee0958d97d7df9fe0b406f0b18d225125939f0d3699cc3f9b2dd3c948cea941 - SHA1:fe4abbd5e75606c6ce6df3d87968f32bd16c9027 [weak] - MD5Sum:5c7119257cd72146aa5f4d70daf41e4b [weak] - Filesize:11702644 [weak] Last modification reported: Fri, 26 Jan 2018 00:03:11 +0000 Release file created at: Sat, 03 Mar 2018 00:03:33 +0000E: 部分索引文件下载失败。如果忽略它们，那将转而使用旧的索引文件。 解决方法：这是因为我之前的更新没有顺利结束，有临时文件残留，所以先执行：1sudo apt-get clean 再执行：1sudo rm -rf /var/lib/apt/lists/* 就可以了。 另外，这个问题也有可能是网络问题导致的。]]></content>
      <categories>
        <category>Linux</category>
        <category>Kali</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Kali</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫常用库selenium详解]]></title>
    <url>%2FPython%2F%E7%88%AC%E8%99%AB%2FPython%E7%88%AC%E8%99%AB%E5%B8%B8%E7%94%A8%E5%BA%93selenium%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[简介selenium是一款支持多种浏览器的自动化测试工具，爬虫中主要用于解决JavaScript渲染页面的问题。 基本使用12345678910111213141516171819from selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.common.keys import Keysfrom selenium.webdriver.support import expected_conditions as ECfrom selenium.webdriver.support.wait import WebDriverWaitbrowser = webdriver.Chrome()try: browser.get(&apos;https://www.baidu.com&apos;) input = browser.find_element_by_id(&apos;kw&apos;) input.send_keys(&apos;Python&apos;) input.send_keys(Keys.ENTER) wait = WebDriverWait(browser, 10) wait.until(EC.presence_of_element_located((By.ID, &apos;content_left&apos;))) print(browser.current_url) print(browser.get_cookies()) print(browser.page_source)finally: browser.close() 这个代码执行会打开一个Chrome浏览器，然后自行访问百度首页，再找到id为kw的元素，传入Python，等到id为content_left的元素加载完毕，最多等10秒；最后打印出URL、cookie和网页源码：123456https://www.baidu.com/s?ie=utf-8&amp;f=8&amp;rsv_bp=0&amp;rsv_idx=1&amp;tn=baidu&amp;wd=Python&amp;rsv_pq=ecec174c000446b6&amp;rsv_t=e937VWYSLXgLXfqehqBQAlxkvD%2BxfJgrJbQShd9tlzleUDgLT79hlx3OY4I&amp;rqlang=cn&amp;rsv_enter=1&amp;rsv_sug3=6&amp;rsv_sug2=0&amp;inputT=151&amp;rsv_sug4=152[&#123;&apos;domain&apos;: &apos;.baidu.com&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;H_PS_PSSID&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;1431_25548_21083_17001_20927&apos;&#125;, &#123;&apos;domain&apos;: &apos;.baidu.com&apos;, &apos;expiry&apos;: 3667193230.07363, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;BAIDUID&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;A7794CCB63F72DF1DB89412B90FD7594:FG=1&apos;&#125;, &#123;&apos;domain&apos;: &apos;.baidu.com&apos;, &apos;expiry&apos;: 3667193230.073748, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;BIDUPSID&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;A7794CCB63F72DF1DB89412B90FD7594&apos;&#125;, &#123;&apos;domain&apos;: &apos;.baidu.com&apos;, &apos;expiry&apos;: 3667193230.073789, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;PSTM&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;1519709584&apos;&#125;, &#123;&apos;domain&apos;: &apos;www.baidu.com&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;BD_HOME&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;0&apos;&#125;, &#123;&apos;domain&apos;: &apos;www.baidu.com&apos;, &apos;expiry&apos;: 1520573584, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;BD_UPN&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;12314753&apos;&#125;, &#123;&apos;domain&apos;: &apos;www.baidu.com&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;BD_CK_SAM&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;1&apos;&#125;, &#123;&apos;domain&apos;: &apos;.baidu.com&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;PSINO&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;6&apos;&#125;, &#123;&apos;domain&apos;: &apos;www.baidu.com&apos;, &apos;expiry&apos;: 1519712177, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;H_PS_645EC&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;73c5tON4OP%2BcqrdDlwHz6rwaG1DOdU1Z3%2F9ptI2btWk%2BMk40sI5n%2BTm0W0M&apos;&#125;]&lt;!DOCTYPE html&gt;&lt;!--STATUS OK--&gt;&lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot;&gt;&lt;head&gt;&lt;script type=&quot;text/javascript&quot; charset=&quot;gb2312&quot; src=&quot;//www.baidu.com/cache/aladdin/ui/tabs5/tabs5.js?v=20170208&quot; data-for=&quot;A.ui&quot;&gt;&lt;/script&gt;&lt;script charset=&quot;utf-8&quot; async=&quot;&quot; src=&quot;https://ss0.bdstatic.com/-0U0bnSm1A5BphGlnYG/tam-ogel/5d4e9b24-dcc5-483a-b6da-be1e9e621891.js&quot;&gt;&lt;/script&gt;......&lt;/body&gt;&lt;/html&gt; 声明浏览器对象selenium支持多种浏览器：1234567from selenium import webdriverbrowser = webdriver.Chrome()browser = webdriver.Firefox()browser = webdriver.Edge()browser = webdriver.PhantomJS()browser = webdriver.Safari() 访问页面123456from selenium import webdriverbrowser = webdriver.Chrome()browser.get(&apos;https://www.taobao.com&apos;)print(browser.page_source)browser.close() 结果：1234&lt;!DOCTYPE html&gt;&lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot; lang=&quot;zh-CN&quot; class=&quot;ks-webkit537 ks-webkit ks-chrome63 ks-chrome&quot;&gt;&lt;head&gt;&lt;script charset=&quot;utf-8&quot; src=&quot;https://g.alicdn.com/mm/tb-page-peel/0.0.5/index-min.js&quot; async=&quot;&quot;&gt;&lt;/script&gt;&lt;script src=&quot;https://tce.alicdn.com/api/data.htm?ids=1017579&amp;amp;callback=tce_fixedtool_callback&quot; async=&quot;&quot;&gt;&lt;/script&gt;......&lt;/body&gt;&lt;/html&gt; 查找元素单个元素123456789from selenium import webdriverbrowser = webdriver.Chrome()browser.get(&apos;https://www.taobao.com&apos;)input_first = browser.find_element_by_id(&apos;q&apos;)input_second = browser.find_element_by_css_selector(&apos;#q&apos;)input_third = browser.find_element_by_xpath(&apos;//*[@id=&quot;q&quot;]&apos;)print(input_first, &apos;\n&apos;, input_second, &apos;\n&apos;, input_third)browser.close() 打开淘宝页面，查找id为q的元素：123&lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;a1d52c692ca1e8a73e7238f098e13ce9&quot;, element=&quot;0.678986922879961-1&quot;)&gt; &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;a1d52c692ca1e8a73e7238f098e13ce9&quot;, element=&quot;0.678986922879961-1&quot;)&gt; &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;a1d52c692ca1e8a73e7238f098e13ce9&quot;, element=&quot;0.678986922879961-1&quot;)&gt; selenium支持包括css选择器、xpath等多种选择方法： find_element_by_name find_element_by_xpath find_element_by_link_text find_element_by_partial_link_text find_element_by_tag_name find_element_by_class_name find_element_by_css_selector 还有一种方法是把选择方式当参数传入，如browser.find_element(By.ID, &#39;q&#39;)：12345678from selenium import webdriverfrom selenium.webdriver.common.by import Bybrowser = webdriver.Chrome()browser.get(&apos;https://www.taobao.com&apos;)input_first = browser.find_element(By.ID, &apos;q&apos;)print(input_first)browser.close() 结果：1&lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;1f209c0d11551c40d9d20ad964fef244&quot;, element=&quot;0.07914603542731591-1&quot;)&gt; 多个元素查找多个元素用find_elements：1234567from selenium import webdriverbrowser = webdriver.Chrome()browser.get(&apos;https://www.taobao.com&apos;)lis = browser.find_elements_by_css_selector(&apos;.service-bd li&apos;)print(lis)browser.close() 结果返回一个列表：1[&lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c688cf3c4681d66e813217aa5311a77e&quot;, element=&quot;0.3350212468864553-1&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c688cf3c4681d66e813217aa5311a77e&quot;, element=&quot;0.3350212468864553-2&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c688cf3c4681d66e813217aa5311a77e&quot;, element=&quot;0.3350212468864553-3&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c688cf3c4681d66e813217aa5311a77e&quot;, element=&quot;0.3350212468864553-4&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c688cf3c4681d66e813217aa5311a77e&quot;, element=&quot;0.3350212468864553-5&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c688cf3c4681d66e813217aa5311a77e&quot;, element=&quot;0.3350212468864553-6&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c688cf3c4681d66e813217aa5311a77e&quot;, element=&quot;0.3350212468864553-7&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c688cf3c4681d66e813217aa5311a77e&quot;, element=&quot;0.3350212468864553-8&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c688cf3c4681d66e813217aa5311a77e&quot;, element=&quot;0.3350212468864553-9&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c688cf3c4681d66e813217aa5311a77e&quot;, element=&quot;0.3350212468864553-10&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c688cf3c4681d66e813217aa5311a77e&quot;, element=&quot;0.3350212468864553-11&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c688cf3c4681d66e813217aa5311a77e&quot;, element=&quot;0.3350212468864553-12&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c688cf3c4681d66e813217aa5311a77e&quot;, element=&quot;0.3350212468864553-13&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c688cf3c4681d66e813217aa5311a77e&quot;, element=&quot;0.3350212468864553-14&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c688cf3c4681d66e813217aa5311a77e&quot;, element=&quot;0.3350212468864553-15&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c688cf3c4681d66e813217aa5311a77e&quot;, element=&quot;0.3350212468864553-16&quot;)&gt;] 对应的，查找多种元素也有多种方法： find_elements_by_name find_elements_by_xpath find_elements_by_link_text find_elements_by_partial_link_text find_elements_by_tag_name find_elements_by_class_name find_elements_by_css_selector 元素交互操作对获取的元素可以调用一些交互方法，如：123456789101112from selenium import webdriverimport timebrowser = webdriver.Chrome()browser.get(&apos;https://www.taobao.com&apos;)input = browser.find_element_by_id(&apos;q&apos;)input.send_keys(&apos;iPhone&apos;)time.sleep(1)input.clear()input.send_keys(&apos;iPad&apos;)button = browser.find_element_by_class_name(&apos;btn-search&apos;)button.click() 这个代码会打开Chrome，找到搜索框，先输入iPhone，等待1秒，把输入框清空，在输入iPad然后点击搜索按钮。 更多有关元素交互操作的内容可以点击这里查看文档。 交互动作将动作附加到动作链中串行执行，如：123456789101112from selenium import webdriverfrom selenium.webdriver import ActionChainsbrowser = webdriver.Chrome()url = &apos;http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable&apos;browser.get(url)browser.switch_to.frame(&apos;iframeResult&apos;)source = browser.find_element_by_css_selector(&apos;#draggable&apos;)target = browser.find_element_by_css_selector(&apos;#droppable&apos;)actions = ActionChains(browser)actions.drag_and_drop(source, target)actions.perform() 这串代码会执行一个iframe的拖拽操作。 更多的交互动作点击这里查看。 执行JavaScript使用execute_script()执行js代码：123456from selenium import webdriverbrowser = webdriver.Chrome()browser.get(&apos;https://www.zhihu.com/explore&apos;)browser.execute_script(&apos;window.scrollTo(0, document.body.scrollHeight)&apos;)browser.execute_script(&apos;alert(&quot;To Bottom&quot;)&apos;) 上面的代码会打开知乎页面并把滚动条下拉到底部然后弹出提示。 获取元素信息获取属性获取属性使用get_attribute()：123456789from selenium import webdriverfrom selenium.webdriver import ActionChainsbrowser = webdriver.Chrome()url = &apos;https://www.zhihu.com/explore&apos;browser.get(url)logo = browser.find_element_by_id(&apos;zh-top-link-logo&apos;)print(logo)print(logo.get_attribute(&apos;class&apos;)) 结果：12&lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;767d4093cfd43cd8c5d9cd4dc12dc204&quot;, element=&quot;0.4229578279847983-1&quot;)&gt;zu-top-link-logo 获取文本值获取文本值使用.text：1234567from selenium import webdriverbrowser = webdriver.Chrome()url = &apos;https://www.zhihu.com/explore&apos;browser.get(url)input = browser.find_element_by_class_name(&apos;zu-top-add-question&apos;)print(input.text) 结果：1提问 获取ID、位置、标签名、大小12345678910from selenium import webdriverbrowser = webdriver.Chrome()url = &apos;https://www.zhihu.com/explore&apos;browser.get(url)input = browser.find_element_by_class_name(&apos;zu-top-add-question&apos;)print(input.id)print(input.location)print(input.tag_name)print(input.size) 结果：12340.6822924344980397-1&#123;&apos;y&apos;: 7, &apos;x&apos;: 774&#125;button&#123;&apos;height&apos;: 32, &apos;width&apos;: 66&#125; Frame网页中有frame时，不能直接查找元素，需要切换到元素所在frame才能查找到：123456789101112131415161718import timefrom selenium import webdriverfrom selenium.common.exceptions import NoSuchElementExceptionbrowser = webdriver.Chrome()url = &apos;http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable&apos;browser.get(url)browser.switch_to.frame(&apos;iframeResult&apos;)source = browser.find_element_by_css_selector(&apos;#draggable&apos;)print(source)try: logo = browser.find_element_by_class_name(&apos;logo&apos;)except NoSuchElementException: print(&apos;NO LOGO&apos;)browser.switch_to.parent_frame()logo = browser.find_element_by_class_name(&apos;logo&apos;)print(logo)print(logo.text) 结果：1234&lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;4bb8ac03ced4ecbdefef03ffdc0e4ccd&quot;, element=&quot;0.44746093888932004-1&quot;)&gt;NO LOGO&lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;4bb8ac03ced4ecbdefef03ffdc0e4ccd&quot;, element=&quot;0.13792611320464965-2&quot;)&gt;RUNOOB.COM 等待隐式等待当使用了隐式等待执行测试的时候，如果WebDriver没有在DOM中找到元素，将继续等待，超出设定时间后则抛出找不到元素的异常, 换句话说，当查找元素或元素并没有立即出现的时候，隐式等待将等待一段时间再查找 DOM，默认的时间是0。1234567from selenium import webdriverbrowser = webdriver.Chrome()browser.implicitly_wait(10)browser.get(&apos;https://www.zhihu.com/explore&apos;)input = browser.find_element_by_class_name(&apos;zu-top-add-question&apos;)print(input) implicitly_wait(10)指如果网速过慢等情况下，元素没有加载出来将额外等待10秒，10秒后还没有加载出来就抛出异常。一般情况下没有必要加隐式等待。 显式等待比较常用的是显式等待，即指定一个等待条件和最长等待时间，它会在最长等待时间内判断条件是否成立，成立则直接返回，超出等待时间则抛出异常，如：1234567891011from selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECbrowser = webdriver.Chrome()browser.get(&apos;https://www.taobao.com/&apos;)wait = WebDriverWait(browser, 10)input = wait.until(EC.presence_of_element_located((By.ID, &apos;q&apos;)))button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, &apos;.btn-search&apos;)))print(input, button) presence_of_element_located((By.ID, &#39;q&#39;))判断元素是否出现；element_to_be_clickable((By.CSS_SELECTOR, &#39;.btn-search&#39;))判断指定按钮是否可点击；可以看到它们都传入了一个元组。结果：1&lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;07dd2fbc2d5b1ce40e82b9754aba8fa8&quot;, element=&quot;0.5642646294074107-1&quot;)&gt; &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;07dd2fbc2d5b1ce40e82b9754aba8fa8&quot;, element=&quot;0.5642646294074107-2&quot;)&gt; 常用的判断条件有： title_is 标题是某内容 title_contains 标题包含某内容 presence_of_element_located 元素加载出，传入定位元组，如(By.ID, p’) visibility_of_element_located 元素可见，传入定位元组 visibility_of 可见，传入元素对象 presence_of_all_elements_located 所有元素加载出 text_to_be_present_in_element 某个元素文本包含某文字 text_to_be_present_in_element_value 某个元素值包含某文字 frame_to_be_available_and_switch_to_it frame加载并切换 invisibility_of_element_located 元素不可见 element_to_be_clickable 元素可点击 staleness_of 判断一个元素是否仍在DOM，可判断页面是否已经刷新 element_to_be_selected 元素可选择，传元素对象 element_located_to_be_selected 元素可选择，传入定位元组 element_selection_state_to_be 传入元素对象以及状态，相等返回True，否则返回False element_located_selection_state_to_be 传入定位元组以及状态，相等回True，否则返回False alert_is_present 是否出现Alert 详细内容可以点击这里查看文档。 前进后退back()和forward()控制后退和前进：1234567891011import timefrom selenium import webdriverbrowser = webdriver.Chrome()browser.get(&apos;https://www.baidu.com/&apos;)browser.get(&apos;https://www.taobao.com/&apos;)browser.get(&apos;https://www.python.org/&apos;)browser.back()time.sleep(1)browser.forward()browser.close() 它会先访问百度，再访问淘宝，再访问Python官网，然后返回淘宝，等待1秒后再前进到Python官网。 cookies123456789from selenium import webdriverbrowser = webdriver.Chrome()browser.get(&apos;https://www.zhihu.com/explore&apos;)print(browser.get_cookies())browser.delete_all_cookies()print(browser.get_cookies())browser.add_cookie(&#123;&apos;name&apos;: &apos;name&apos;, &apos;domain&apos;: &apos;www.zhihu.com&apos;, &apos;value&apos;: &apos;jeff&apos;&#125;)print(browser.get_cookies()) 上面代码会打开知乎页面，然后打印出cookies，然后删除掉所有cookies，最后往cookies添加一些内容再打印：123[&#123;&apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;expiry&apos;: 1614321990.390978, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;d_c0&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;&quot;AAAsNHqUNQ2PTmyB9_dLW6YtcYCfwvIaBac=|1519713991&quot;&apos;&#125;, &#123;&apos;domain&apos;: &apos;www.zhihu.com&apos;, &apos;httpOnly&apos;: True, &apos;name&apos;: &apos;aliyungf_tc&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;AQAAADeoZFYSBAYACrwVt1hMfg3RqO/a&apos;&#125;, &#123;&apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;l_n_c&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;1&apos;&#125;, &#123;&apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;expiry&apos;: 1519715791, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;__utmb&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;51854390.0.10.1519713992&apos;&#125;, &#123;&apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;expiry&apos;: 1614321988.412313, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;q_c1&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;bff234c12f284b39b3b49bb9be57735d|1519713988000|1519713988000&apos;&#125;, &#123;&apos;domain&apos;: &apos;www.zhihu.com&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;_xsrf&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;6f356dbeb54748552c389235dc679975&apos;&#125;, &#123;&apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;expiry&apos;: 1522305988.412491, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;r_cap_id&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;&quot;YjAzMTliOWEyOGRiNGEyMGE5NzVmYzY2NDg1MWZjZjQ=|1519713988|a448a57c0fa2941a00ca61a0bb8ba1c521adb298&quot;&apos;&#125;, &#123;&apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;expiry&apos;: 1522305988.412595, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;cap_id&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;&quot;NGUzM2U2ZGVmNmFiNGY0MWI0M2MwMGE4ZGJhMjc0NGE=|1519713988|707d2d360638930efdf20eec53827e09891b1d53&quot;&apos;&#125;, &#123;&apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;expiry&apos;: 1522305988.412739, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;l_cap_id&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;&quot;MzdlNzAyOWU4NmNiNDBlMjhlZDBhNGI4NWE1MGYwMDM=|1519713988|3a82264b18882e235680e56235a7dee96be9e1c2&quot;&apos;&#125;, &#123;&apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;n_c&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;1&apos;&#125;, &#123;&apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;expiry&apos;: 1582785991, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;_zap&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;310e0d7a-3b5f-4293-867e-e7c2052b3734&apos;&#125;, &#123;&apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;expiry&apos;: 1582785991, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;__utma&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;51854390.1700258999.1519713992.1519713992.1519713992.1&apos;&#125;, &#123;&apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;__utmc&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;51854390&apos;&#125;, &#123;&apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;expiry&apos;: 1535481991, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;__utmz&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;51854390.1519713992.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none)&apos;&#125;, &#123;&apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;expiry&apos;: 1582785991, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;__utmv&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;51854390.000--|3=entry_date=20180227=1&apos;&#125;][][&#123;&apos;domain&apos;: &apos;www.zhihu.com&apos;, &apos;expiry&apos;: 2150433993, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;name&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: True, &apos;value&apos;: &apos;jeff&apos;&#125;] cookies可以在开发者工具的Application的Cookie看到。 选项卡管理最简单的就是使用js代码window.open()打开新窗口：123456789101112import timefrom selenium import webdriverbrowser = webdriver.Chrome()browser.get(&apos;https://www.baidu.com&apos;)browser.execute_script(&apos;window.open()&apos;)print(browser.window_handles)browser.switch_to_window(browser.window_handles[1])browser.get(&apos;https://www.taobao.com&apos;)time.sleep(1)browser.switch_to_window(browser.window_handles[0])browser.get(&apos;https://python.org&apos;) 输出：1[&apos;CDwindow-(CCB96210C849FFC0EC59E7230C77B934)&apos;, &apos;CDwindow-(C67B5EFB619A8F76ED9A2609C0E79842)&apos;] 使用window_handles定位选项卡，通过switch_to_window()可以切换选项卡进行操作。 异常处理如查找一个不存在的元素：12345from selenium import webdriverbrowser = webdriver.Chrome()browser.get(&apos;https://www.baidu.com&apos;)browser.find_element_by_id(&apos;hello&apos;) 报错：1234567891011---------------------------------------------------------------------------NoSuchElementException Traceback (most recent call last)&lt;ipython-input-23-978945848a1b&gt; in &lt;module&gt;() 3 browser = webdriver.Chrome() 4 browser.get(&apos;https://www.baidu.com&apos;)----&gt; 5 browser.find_element_by_id(&apos;hello&apos;)......NoSuchElementException: Message: no such element: Unable to locate element: &#123;&quot;method&quot;:&quot;id&quot;,&quot;selector&quot;:&quot;hello&quot;&#125; (Session info: chrome=63.0.3239.132) (Driver info: chromedriver=2.35.528161 (5b82f2d2aae0ca24b877009200ced9065a772e73),platform=Windows NT 10.0.16299 x86_64) 异常处理：1234567891011121314from selenium import webdriverfrom selenium.common.exceptions import TimeoutException, NoSuchElementExceptionbrowser = webdriver.Chrome()try: browser.get(&apos;https://www.baidu.com&apos;)except TimeoutException: print(&apos;Time Out&apos;)try: browser.find_element_by_id(&apos;hello&apos;)except NoSuchElementException: print(&apos;No Element&apos;)finally: browser.close() 输出：1No Element 关于异常处理 的更多内容点击这里查看文档。 结语详细的说明和使用可以点击这里查看文档。]]></content>
      <categories>
        <category>Python</category>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows安装配置MongoDB]]></title>
    <url>%2FMongoDB%2FWindows%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AEMongoDB%2F</url>
    <content type="text"><![CDATA[准备MongoDB是一个灵活的文档数据库，它将数据存储在类似JSON的文档中。下面是Windows下安装与配置的过程。 可以点击这里下载Windows的MongoDB安装包。 安装下载完安装包后直接双击打开就开始安装，安装过程其实很简单，只需要不断点击Next就可以了。 只是如果需要选择安装路径的话，在Choose Setup Type时选择Custom就可以自定义路径。 创建目录MongoDB将数据等存在db目录下，但这个目录我们需要自行创建。 我的安装目录是：1D:\Program Files\MongoDB\Server\3.4 我就在这个目录下新建一个data目录，在data里再建一个db目录：1D:\Program Files\MongoDB\Server\3.4\data\db 运行服务cmd切换到bin目录下执行以下命令：1mongod --dbpath &quot;D:\Program Files\MongoDB\Server\3.4\data\db&quot; 注意：我的目录里Program Files有空格，所以在命令里需要用双引号引起。 运行命令后，可以看到类似以下输出：123456789101112131415161718192021222018-02-24T01:24:39.882-0700 I CONTROL [initandlisten] MongoDB starting : pid=15252 port=27017 dbpath=D:\Program Files\MongoDB\Server\3.4\data\db 64-bit host=DESKTOP-H1N98FT2018-02-24T01:24:39.882-0700 I CONTROL [initandlisten] targetMinOS: Windows 7/Windows Server 2008 R22018-02-24T01:24:39.883-0700 I CONTROL [initandlisten] db version v3.4.92018-02-24T01:24:39.883-0700 I CONTROL [initandlisten] git version: 876ebee8c7dd0e2d992f36a848ff4dc50ee6603e2018-02-24T01:24:39.883-0700 I CONTROL [initandlisten] OpenSSL version: OpenSSL 1.0.1u-fips 22 Sep 20162018-02-24T01:24:39.883-0700 I CONTROL [initandlisten] allocator: tcmalloc2018-02-24T01:24:39.883-0700 I CONTROL [initandlisten] modules: none2018-02-24T01:24:39.884-0700 I CONTROL [initandlisten] build environment:2018-02-24T01:24:39.884-0700 I CONTROL [initandlisten] distmod: 2008plus-ssl2018-02-24T01:24:39.884-0700 I CONTROL [initandlisten] distarch: x86_642018-02-24T01:24:39.884-0700 I CONTROL [initandlisten] target_arch: x86_642018-02-24T01:24:39.884-0700 I CONTROL [initandlisten] options: &#123; storage: &#123; dbPath: &quot;D:\Program Files\MongoDB\Server\3.4\data\db&quot; &#125; &#125;2018-02-24T01:24:39.914-0700 I - [initandlisten] Detected data files in D:\Program Files\MongoDB\Server\3.4\data\db created by the &apos;wiredTiger&apos; storage engine, so setting the active storage engine to &apos;wiredTiger&apos;.2018-02-24T01:24:39.915-0700 I STORAGE [initandlisten] wiredtiger_open config: create,cache_size=5565M,session_max=20000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),checkpoint=(wait=60,log_size=2GB),statistics_log=(wait=0),2018-02-24T01:24:41.078-0700 I CONTROL [initandlisten]2018-02-24T01:24:41.078-0700 I CONTROL [initandlisten] ** WARNING: Access control is not enabled for the database.2018-02-24T01:24:41.078-0700 I CONTROL [initandlisten] ** Read and write access to data and configuration is unrestricted.2018-02-24T01:24:41.078-0700 I CONTROL [initandlisten]2018-02-24T16:24:41.504+0800 I FTDC [initandlisten] Initializing full-time diagnostic data capture with directory &apos;D:/Program Files/MongoDB/Server/3.4/data/db/diagnostic.data&apos;2018-02-24T16:24:41.507+0800 I NETWORK [thread1] waiting for connections on port 270172018-02-24T16:25:06.603+0800 I NETWORK [thread1] connection accepted from 127.0.0.1:2718 #1 (1 connection now open)2018-02-24T16:25:06.605+0800 I NETWORK [conn1] received client metadata from 127.0.0.1:2718 conn1: &#123; application: &#123; name: &quot;MongoDB Shell&quot; &#125;, driver: &#123; name: &quot;MongoDB Internal Client&quot;, version: &quot;3.4.9&quot; &#125;, os: &#123; type: &quot;Windows&quot;, name: &quot;Microsoft Windows 8&quot;, architecture: &quot;x86_64&quot;, version: &quot;6.2 (build 9200)&quot; &#125; &#125; 这时在浏览器访问http://localhost:27017/可以看到这样一句话：1It looks like you are trying to access MongoDB over HTTP on the native driver port. 这就说明服务已经启动了。 然后就可以在bin目录打开cmd，执行mongo命令进入MongoDB客户端了（小技巧：在打开的文件管理器的地址栏输入cmd就可以在当前路径打开命令行，或者按shift + 右键也可以选择在当前目录打开cmd，不过我的win10更新后用shift + 右键只能打开Power Shell了，不是很习惯…）：12345678910D:\Program Files\MongoDB\Server\3.4\bin&gt;mongoMongoDB shell version v3.4.9connecting to: mongodb://127.0.0.1:27017MongoDB server version: 3.4.9Server has startup warnings:2018-02-24T01:24:41.078-0700 I CONTROL [initandlisten]2018-02-24T01:24:41.078-0700 I CONTROL [initandlisten] ** WARNING: Access control is not enabled for the database.2018-02-24T01:24:41.078-0700 I CONTROL [initandlisten] ** Read and write access to data and configuration is unrestricted.2018-02-24T01:24:41.078-0700 I CONTROL [initandlisten]&gt; 此外，我们可以把D:\Program Files\MongoDB\Server\3.4\bin路径配置到环境变量的Path里，这样就不用每次使用都切换到bin目录下了。环境变量的配置：右键我的电脑 –&gt; 属性 –&gt; 高级系统设置 –&gt; 环境变量，然后选择Path把路径添加就可以了。 配置系统服务每次都通过命令行打开MongoDB服务比较麻烦，我们可以把它配置成系统服务。 首先，先在data目录下新建一个log目录，然后新建一个mongo.log作为日志文件。 我们需要以管理员权限运行cmd命令行，然后执行：1mongod --bind_ip 0.0.0.0 --logpath &quot;D:\Program Files\MongoDB\Server\3.4\data\logs\mongo.log&quot; --logappend --dbpath &quot;D:\Program Files\MongoDB\Server\3.4\data\db&quot; --port 27017 --serviceName &quot;MongoDB&quot; --serviceDisplayName &quot;MongoDB&quot; --install 执行完这个命令后，就可以在通过win + R输入services.msc打开系统服务，在这里控制MongoDB服务的开启和关闭而不用通过命令行了。 完成这样，Windows下MongoDB的安装和配置就完成了。]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django项目遇到的问题记录]]></title>
    <url>%2FPython%2FDjango%2FDjango%E9%A1%B9%E7%9B%AE%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[在使用Django框架的时候，由于不熟悉，碰到了一些小问题。 环境 Python3.6 Django1.11.7 问题记录在编写HTML页面时，我写了如下代码：1&lt;input type=&quot;hidden&quot; name=&quot;article_id&quot; value=&quot;&#123;% if article %&#125;&#123;&#123; article.id &#125;&#125;&#123;% else %&#125; &apos;0&apos;&#123;% endif %&#125;&quot;/&gt; 使用时就报错：1invalid literal for int() with base 10: &quot;&apos;0&apos;&quot; 网上搜这个报错说是字符串转换成int，仔细一看才意识到&#39;0&#39;不应该加引号了… 然后去掉0的引号之后又报错：1234DoesNotExist at /blog/edit/actionException Type: DoesNotExistException Value: Article matching query does not exist. 我的后台有：12345if article_id == &apos;0&apos;: ... return ...article = models.Arcticle.objects.get(pk=article_id)return ... 所以article_id == &#39;0&#39;的值为FALSE了，把article_id打印出来发现是’ 0 ‘两边各有一个空格。 前端的语句应该写成这样：1&lt;input type=&quot;hidden&quot; name=&quot;article_id&quot; value=&quot;&#123;% if article %&#125;&#123;&#123; article.id &#125;&#125;&#123;% else %&#125;0&#123;% endif %&#125;&quot;/&gt; 这样的小错误也折腾了不少时间。]]></content>
      <categories>
        <category>Python</category>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫常用库beautifulsoup详解]]></title>
    <url>%2FPython%2F%E7%88%AC%E8%99%AB%2FPython%E7%88%AC%E8%99%AB%E5%B8%B8%E7%94%A8%E5%BA%93beautifulsoup%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[简介Beautiful Soup是一款支持多种解析器的灵活的网页解析库。 解析库： 解析器 使用方法 优势 劣势 Python标准库 BeautifulSoup(markup, “html.parser”) Python的内置标准库、执行速度适中 、文档容错能力强 Python 2.7.3 or 3.2.2)前的版本中文容错能力差 lxml HTML 解析器 BeautifulSoup(markup, “lxml”) 速度快、文档容错能力强 需要安装C语言库 lxml XML 解析器 BeautifulSoup(markup, “xml”) 速度快、唯一支持XML的解析器 需要安装C语言库 html5lib BeautifulSoup(markup, “html5lib”) 最好的容错性、以浏览器的方式解析文档、生成HTML5格式的文档 速度慢、不依赖外部扩展 基本使用123456789101112131415html = &quot;&quot;&quot;&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;The Dormouse&apos;s story&lt;/b&gt;&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were&lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and&lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&quot;&quot;&quot;from bs4 import BeautifulSoupsoup = BeautifulSoup(html, &apos;lxml&apos;)print(soup.prettify())print(soup.title.string) lxml是比较常用的一种解析库。可以看到上面的代码是不完整的HTML代码，但是prettify()可以帮助格式化HTML代码，soup.title.string可以获取标签内容：12345678910111213141516171819202122232425262728293031323334&lt;html&gt; &lt;head&gt; &lt;title&gt; The Dormouse&apos;s story &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt; &lt;b&gt; The Dormouse&apos;s story &lt;/b&gt; &lt;/p&gt; &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt; &lt;!-- Elsie --&gt; &lt;/a&gt; , &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt; Lacie &lt;/a&gt; and &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt; Tillie &lt;/a&gt; ;and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=&quot;story&quot;&gt; ... &lt;/p&gt; &lt;/body&gt;&lt;/html&gt;The Dormouse&apos;s story 标签选择器选择元素1234567891011121314151617html = &quot;&quot;&quot;&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;The Dormouse&apos;s story&lt;/b&gt;&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were&lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and&lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&quot;&quot;&quot;from bs4 import BeautifulSoupsoup = BeautifulSoup(html, &apos;lxml&apos;)print(soup.title)print(type(soup.title))print(soup.head)print(soup.p) 输出如下：1234&lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;&lt;class &apos;bs4.element.Tag&apos;&gt;&lt;head&gt;&lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;&lt;/head&gt;&lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;The Dormouse&apos;s story&lt;/b&gt;&lt;/p&gt; 可以看到它们是bs4.element.Tag对象，而且这种方法只会输出第一个指定标签。 获取名称1234567891011121314html = &quot;&quot;&quot;&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;The Dormouse&apos;s story&lt;/b&gt;&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were&lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and&lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&quot;&quot;&quot;from bs4 import BeautifulSoupsoup = BeautifulSoup(html, &apos;lxml&apos;)print(soup.title.name) 结果是把选中标签名输出：1title 获取属性123456789101112131415html = &quot;&quot;&quot;&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;The Dormouse&apos;s story&lt;/b&gt;&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were&lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and&lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&quot;&quot;&quot;from bs4 import BeautifulSoupsoup = BeautifulSoup(html, &apos;lxml&apos;)print(soup.p.attrs[&apos;name&apos;])print(soup.p[&apos;name&apos;]) p.attrs[&#39;name&#39;]和p[&#39;name&#39;]都可以获取属性：12dromousedromouse 获取内容1234567891011121314html = &quot;&quot;&quot;&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p clss=&quot;title&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;The Dormouse&apos;s story&lt;/b&gt;&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were&lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and&lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&quot;&quot;&quot;from bs4 import BeautifulSoupsoup = BeautifulSoup(html, &apos;lxml&apos;)print(soup.p.string) 结果：1The Dormouse&apos;s story 嵌套选择1234567891011121314html = &quot;&quot;&quot;&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;The Dormouse&apos;s story&lt;/b&gt;&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were&lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and&lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&quot;&quot;&quot;from bs4 import BeautifulSoupsoup = BeautifulSoup(html, &apos;lxml&apos;)print(soup.head.title.string) 经过选择的结果都是bs4.element.Tag对象，还可以用同样的方式选择，结果和前面是一样的：1The Dormouse&apos;s story 子节点和子孙节点123456789101112131415161718192021html = &quot;&quot;&quot;&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse&apos;s story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&quot;&quot;&quot;from bs4 import BeautifulSoupsoup = BeautifulSoup(html, &apos;lxml&apos;)print(soup.p.contents) .contents以列表的形式返回子孙节点：123[&apos;\n Once upon a time there were three little sisters; and their names were\n &apos;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;&lt;span&gt;Elsie&lt;/span&gt;&lt;/a&gt;, &apos;\n&apos;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;, &apos; \n and\n &apos;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;, &apos;\n and they lived at the bottom of a well.\n &apos;] 另外，还有另外一种方式.children：1234567891011121314151617181920212223html = &quot;&quot;&quot;&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse&apos;s story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&quot;&quot;&quot;from bs4 import BeautifulSoupsoup = BeautifulSoup(html, &apos;lxml&apos;)print(soup.p.children)for i, child in enumerate(soup.p.children): print(i, child) 结果：12345678910111213141516&lt;list_iterator object at 0x08CCBAF0&gt;0 Once upon a time there were three little sisters; and their names were 1 &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;&lt;span&gt;Elsie&lt;/span&gt;&lt;/a&gt;2 3 &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;4 and 5 &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;6 and they lived at the bottom of a well. 可以看出，两者的不同之处就是.children返回的是list_iterator一个迭代器而.contents返回列表。enumerate()方法遍历可以得到索引和值。 .descendants获取所有子孙节点：1234567891011121314151617181920212223html = &quot;&quot;&quot;&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse&apos;s story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&quot;&quot;&quot;from bs4 import BeautifulSoupsoup = BeautifulSoup(html, &apos;lxml&apos;)print(soup.p.descendants)for i, child in enumerate(soup.p.descendants): print(i, child) 结果返回generator：123456789101112131415161718192021222324&lt;generator object descendants at 0x086AEB70&gt;0 Once upon a time there were three little sisters; and their names were 1 &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;&lt;span&gt;Elsie&lt;/span&gt;&lt;/a&gt;2 3 &lt;span&gt;Elsie&lt;/span&gt;4 Elsie5 6 7 &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;8 Lacie9 and 10 &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;11 Tillie12 and they lived at the bottom of a well. 父节点和祖先节点123456789101112131415161718192021html = &quot;&quot;&quot;&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse&apos;s story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&quot;&quot;&quot;from bs4 import BeautifulSoupsoup = BeautifulSoup(html, &apos;lxml&apos;)print(soup.a.parent) .parent获取父节点：12345678910&lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;&lt;span&gt;Elsie&lt;/span&gt;&lt;/a&gt;&lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; .parents输出所有祖先节点：123456789101112131415161718192021html = &quot;&quot;&quot;&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse&apos;s story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&quot;&quot;&quot;from bs4 import BeautifulSoupsoup = BeautifulSoup(html, &apos;lxml&apos;)print(list(enumerate(soup.a.parents))) 结果：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354[(0, &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;&lt;span&gt;Elsie&lt;/span&gt;&lt;/a&gt;&lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt;), (1, &lt;body&gt;&lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;&lt;span&gt;Elsie&lt;/span&gt;&lt;/a&gt;&lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt;&lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&lt;/body&gt;), (2, &lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;&lt;span&gt;Elsie&lt;/span&gt;&lt;/a&gt;&lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt;&lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;), (3, &lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;&lt;span&gt;Elsie&lt;/span&gt;&lt;/a&gt;&lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt;&lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;)] 兄弟节点12345678910111213141516171819202122html = &quot;&quot;&quot;&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse&apos;s story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&quot;&quot;&quot;from bs4 import BeautifulSoupsoup = BeautifulSoup(html, &apos;lxml&apos;)print(list(enumerate(soup.a.next_siblings)))print(list(enumerate(soup.a.previous_siblings))) .next_siblings获取后面的兄弟节点，.previous_siblings获取前面的兄弟节点：12[(0, &apos;\n&apos;), (1, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;), (2, &apos; \n and\n &apos;), (3, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;), (4, &apos;\n and they lived at the bottom of a well.\n &apos;)][(0, &apos;\n Once upon a time there were three little sisters; and their names were\n &apos;)] 标准选择器前面的标签选择器虽然速度很快，但是很多时候并不能满足我们的需求。 find_all()可根据标签名、属性、内容查找文档，返回所有符合元素。用法：1find_all(name, attrs, recursive, text, **kwargs) 根据name查找：12345678910111213141516171819202122html=&apos;&apos;&apos;&lt;div class=&quot;panel&quot;&gt; &lt;div class=&quot;panel-heading&quot;&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=&quot;panel-body&quot;&gt; &lt;ul class=&quot;list&quot; id=&quot;list-1&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=&quot;list list-small&quot; id=&quot;list-2&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;&apos;&apos;&apos;from bs4 import BeautifulSoupsoup = BeautifulSoup(html, &apos;lxml&apos;)for ul in soup.find_all(&apos;ul&apos;): print(ul.find_all(&apos;li&apos;)) 结果：12[&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;][&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;] 根据attrs查找：12345678910111213141516171819202122html=&apos;&apos;&apos;&lt;div class=&quot;panel&quot;&gt; &lt;div class=&quot;panel-heading&quot;&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=&quot;panel-body&quot;&gt; &lt;ul class=&quot;list&quot; id=&quot;list-1&quot; name=&quot;elements&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=&quot;list list-small&quot; id=&quot;list-2&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;&apos;&apos;&apos;from bs4 import BeautifulSoupsoup = BeautifulSoup(html, &apos;lxml&apos;)print(soup.find_all(attrs=&#123;&apos;id&apos;: &apos;list-1&apos;&#125;))print(soup.find_all(attrs=&#123;&apos;name&apos;: &apos;elements&apos;&#125;)) 结果：12345678910[&lt;ul class=&quot;list&quot; id=&quot;list-1&quot; name=&quot;elements&quot;&gt;&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;&lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;&lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;&lt;/ul&gt;][&lt;ul class=&quot;list&quot; id=&quot;list-1&quot; name=&quot;elements&quot;&gt;&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;&lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;&lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;&lt;/ul&gt;] 还可以简单的写成，需要注意的是，class是Python的关键字，要写成class_：12345678910111213141516171819202122html=&apos;&apos;&apos;&lt;div class=&quot;panel&quot;&gt; &lt;div class=&quot;panel-heading&quot;&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=&quot;panel-body&quot;&gt; &lt;ul class=&quot;list&quot; id=&quot;list-1&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=&quot;list list-small&quot; id=&quot;list-2&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;&apos;&apos;&apos;from bs4 import BeautifulSoupsoup = BeautifulSoup(html, &apos;lxml&apos;)print(soup.find_all(id=&apos;list-1&apos;))print(soup.find_all(class_=&apos;element&apos;)) 结果：123456[&lt;ul class=&quot;list&quot; id=&quot;list-1&quot;&gt;&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;&lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;&lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;&lt;/ul&gt;][&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;] 根据text选择：123456789101112131415161718192021html=&apos;&apos;&apos;&lt;div class=&quot;panel&quot;&gt; &lt;div class=&quot;panel-heading&quot;&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=&quot;panel-body&quot;&gt; &lt;ul class=&quot;list&quot; id=&quot;list-1&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=&quot;list list-small&quot; id=&quot;list-2&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;&apos;&apos;&apos;from bs4 import BeautifulSoupsoup = BeautifulSoup(html, &apos;lxml&apos;)print(soup.find_all(text=&apos;Foo&apos;)) 它是直接把text输出：1[&apos;Foo&apos;, &apos;Foo&apos;] find()与find_all()相似，不过find()返回单个元素，用法：1find(name, attrs, recursive, text, **kwargs) 1234567891011121314151617181920212223html=&apos;&apos;&apos;&lt;div class=&quot;panel&quot;&gt; &lt;div class=&quot;panel-heading&quot;&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=&quot;panel-body&quot;&gt; &lt;ul class=&quot;list&quot; id=&quot;list-1&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=&quot;list list-small&quot; id=&quot;list-2&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;&apos;&apos;&apos;from bs4 import BeautifulSoupsoup = BeautifulSoup(html, &apos;lxml&apos;)print(soup.find(&apos;ul&apos;))print(type(soup.find(&apos;ul&apos;)))print(soup.find(&apos;page&apos;)) 不存在则返回None，结果：1234567&lt;ul class=&quot;list&quot; id=&quot;list-1&quot;&gt;&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;&lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;&lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;&lt;/ul&gt;&lt;class &apos;bs4.element.Tag&apos;&gt;None find_parents()、find_parent()find_parents()返回所有祖先节点，find_parent()返回直接父节点。 find_next_siblings()、find_next_sibling()find_next_siblings()返回后面所有兄弟节点，find_next_sibling()返回后面第一个兄弟节点。 find_previous_siblings()、find_previous_sibling()find_previous_siblings()返回前面所有兄弟节点，find_previous_sibling()返回前面第一个兄弟节点。 find_all_next()、find_next()find_all_next()返回节点后所有符合条件的节点, find_next()返回第一个符合条件的节点 find_all_previous()、find_previous()find_all_previous()返回节点后所有符合条件的节点, find_previous()返回第一个符合条件的节点 css选择器通过select()直接传入CSS选择器即可完成选择。 使用123456789101112131415161718192021222324html=&apos;&apos;&apos;&lt;div class=&quot;panel&quot;&gt; &lt;div class=&quot;panel-heading&quot;&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=&quot;panel-body&quot;&gt; &lt;ul class=&quot;list&quot; id=&quot;list-1&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=&quot;list list-small&quot; id=&quot;list-2&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;&apos;&apos;&apos;from bs4 import BeautifulSoupsoup = BeautifulSoup(html, &apos;lxml&apos;)print(soup.select(&apos;.panel .panel-heading&apos;))print(soup.select(&apos;ul li&apos;))print(soup.select(&apos;#list-2 .element&apos;))print(type(soup.select(&apos;ul&apos;)[0])) 结果也是bs4.element.Tag对象：123456[&lt;div class=&quot;panel-heading&quot;&gt;&lt;h4&gt;Hello&lt;/h4&gt;&lt;/div&gt;][&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;][&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;]&lt;class &apos;bs4.element.Tag&apos;&gt; 12345678910111213141516171819202122html=&apos;&apos;&apos;&lt;div class=&quot;panel&quot;&gt; &lt;div class=&quot;panel-heading&quot;&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=&quot;panel-body&quot;&gt; &lt;ul class=&quot;list&quot; id=&quot;list-1&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=&quot;list list-small&quot; id=&quot;list-2&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;&apos;&apos;&apos;from bs4 import BeautifulSoupsoup = BeautifulSoup(html, &apos;lxml&apos;)for ul in soup.select(&apos;ul&apos;): print(ul.select(&apos;li&apos;)) 结果：12[&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;][&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;] 获取属性获取属性也有两种方法：1234567891011121314151617181920212223html=&apos;&apos;&apos;&lt;div class=&quot;panel&quot;&gt; &lt;div class=&quot;panel-heading&quot;&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=&quot;panel-body&quot;&gt; &lt;ul class=&quot;list&quot; id=&quot;list-1&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=&quot;list list-small&quot; id=&quot;list-2&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;&apos;&apos;&apos;from bs4 import BeautifulSoupsoup = BeautifulSoup(html, &apos;lxml&apos;)for ul in soup.select(&apos;ul&apos;): print(ul[&apos;id&apos;]) print(ul.attrs[&apos;id&apos;]) 结果：1234list-1list-1list-2list-2 获取内容get_text()就可以获取文本：12345678910111213141516171819202122html=&apos;&apos;&apos;&lt;div class=&quot;panel&quot;&gt; &lt;div class=&quot;panel-heading&quot;&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=&quot;panel-body&quot;&gt; &lt;ul class=&quot;list&quot; id=&quot;list-1&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=&quot;list list-small&quot; id=&quot;list-2&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;&apos;&apos;&apos;from bs4 import BeautifulSoupsoup = BeautifulSoup(html, &apos;lxml&apos;)for li in soup.select(&apos;li&apos;): print(li.get_text()) 结果：12345FooBarJayFooBar 结语 推荐使用lxml解析库，必要时使用html.parser 标签选择筛选功能弱但是速度快 建议使用find()、find_all() 查询匹配单个结果或者多个结果 如果对CSS选择器熟悉建议使用select() 记住常用的获取属性和文本值的方法 详细的说明和使用可以点击这里查看文档。]]></content>
      <categories>
        <category>Python</category>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django入门与实践]]></title>
    <url>%2FPython%2FDjango%2FDjango%E5%85%A5%E9%97%A8%E4%B8%8E%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[Django是一个功能强大的Python的高级Web框架，这是学习慕课网django入门与实践的课程笔记。通过一个小型博客功能的实现来学习Django的使用。 简介 Django是一个基于Python的高级Web开发框架，它能够让开发人员进行高效且快速的开发。 是高度集成的，不用自己造轮子，只需专注于网站本身的开发 Django免费且开源 浏览器上网基本原理上网过程：浏览器输入网址 –&gt; 回车（向目标URL发送一个HTTP请求） –&gt; 看到网页（服务器响应请求，把代码返回给浏览器解析成看到的页面） 本质是网络通信，即通过网络进行数据传递 浏览器经过通信后获取到该页面的源代码文档（HTML等） 浏览器解析文档后以适当的形式展现给用户 总之，请求响应过程就是浏览器发送HTTP请求给网站服务器，服务器通过后台代码处理请求，然后返回HTTP响应，把HTML文档等返回给浏览器，通过浏览器解析成用户看到的页面。 环境搭建安装Pythonwindows下访问官网点击下载直接安装，课程里用的是Python2.7，我这里是Python3.6安装时选上Add python.exe to PATH。 安装Django课程里用的是Django1.10.2，我这里是Django1.11.7。最简单方便的安装方式，可以指定安装版本：1pip install Django==2.0.2 或者使用源码安装，下载源码，进入根目录：1python setup.py install 具体可以点击这里查看官网的说明 使用以下命令可以查看安装的版本：1python -m django --version 开发工具使用的是Pycharm，也可以根据自己习惯选择Eclipse+Pydev、Sublime Text、Atom或Visual Studio Code等。 创建项目创建项目，了解目录下文件的作用在想要放置项目的目录里打开cmd命令行，执行以下命令：1django-admin startproject project_name 如django-admin startproject myblog，然后可以看到项目的目录结构：123456├─manage.py└─myblog ├─__init__.py ├─settings.py ├─urls.py └─wsgi.py manage.py：与项目进行交互的命令行工具集的入口，相当于项目管理器，执行python manage.py来查看所有命令。 python manage.py runserver启动自带服务器 python manage.py runserver 9999指定端口号 myblog目录：项目的一个容器，包含项目最基本的一些配置。目录名称允许修改，但不建议修改，因为许多配置文件里已经使用了这个目录名称做配置。 wsgi.py：WSGI(Python Web Server Gateway Interface)中文叫Python服务器网关接口。就是Python应用与Web服务器之间的接口 urls.py：URL配置文件，Django项目中所有的地址（页面）都需要我们自己去配置其URL settings.py:项目的总配置文件，里面包含了数据库、Web应用、时间等各种配置: BASE_DIR：项目的根目录 SECRET_KEY：项目自动生成的安全码 DEBUG：调试模式，生产环境中不要打开 ALLOWED_HOSTS：Django只允许通过这里面的地址访问网站，屏蔽其他所有地址，使其变成Bad Request (400) INSTALLED_APPS：已安装应用，Django项目是由许多应用组成的，生成项目时会有一些自动生成的应用，自行创建的用用要添加到这里 MIDDLEWARE：中间件，Django自带的一些工具集 ROOT_URLCONF：URL的根文件，这里指向urls.py TEMPLATES：模板的一些配置 WSGI_APPLICATION：wsgi.py相关，暂时不管 DATABASES：数据库配置 AUTH_PASSWORD_VALIDATORS：与密码认证有关，暂时不管 LANGUAGE_CODE = &#39;en-us&#39;、TIME_ZONE = &#39;UTC&#39;、USE_I18N = True、USE_L10N = True、USE_TZ = True：这五个是国际化相关的配置，语言、时区等 STATIC_URL：静态文件目录 __init__.py：Python中声明模块的文件，内容默认为空 创建应用，了解应用目录下各文件的作用应用创建步骤：进入manage.py同级目录，打开命令行输入：1python manage.py startapp app_name 如python manage.py startapp blog。（注意：命名不能与Python内置模块重名，会报错。）添加应用名到settings.py中的INSTALLED_APPS里。目录结构：1234567891011├─manage.py├─……└─blog ├─migrations ├─__init__.py ├─__init__.py ├─admin.py ├─apps.py ├─models.py ├─tests.py └─views.py migrations目录：数据移植（迁移）模块，内容自动生成 admin.py：该应用的后台管理系统配置 apps.py：该应用的一些配置，Django1.9以后自动生成 models.py：数据模块，使用ORM框架，类似于MVC结构中的Models（模型） tests.py：自动化测试模块，Django提供了自动化测试功能，可以在这里编写测试脚本（语句） views.py：执行响应的代码所在的模块，代码逻辑处理的主要地点，项目中的大部分代码均在这里编写 创建第一个页面响应编辑blog目录下的views.py文件：1234from django.http import HttpResponsedef index(request): return HttpResponse(&quot;Hello, World!&quot;) 每个响应对应一个函数，函数必须返回一个响应 函数必须存在一个参数，一般约定为request 每一个响应（函数）对应一个URL 然后修改urls.py配置URL：123456import blog.views as bvurlpatterns = [ url(r&apos;^admin/&apos;, admin.site.urls), url(r&apos;^index/&apos;, bv.index), # 增加这一行] 每个URL都一url的形式写出来 url函数放在urlpatterns列表中 url函数三个参数：URL（正则）、对应方法、名称 然后python manage.py runserver启动服务器，浏览器访问http://localhost:8000/index/就会出现经典的Hello, World!了。 第一个Template配置URL的第二种方法（包含其他URL配置）修改根urls.py，引入include，url函数第二个参数改为include(&#39;blog.urls&#39;)：1234567from django.conf.urls import url, includefrom django.contrib import adminurlpatterns = [ url(r&apos;^admin/&apos;, admin.site.urls), url(r&apos;^blog/&apos;, include(&apos;blog.urls&apos;)),] 在应用目录blog目录下创建urls.py文件，格式与根urls.py相同：123456from django.conf.urls import urlfrom . import viewsurlpatterns = [ url(r&apos;^index/$&apos;, views.index), # 限制index，注意加一个‘/’，否则访问“http://localhost:8000/blog/index/”会出现404] 注意事项： 根urls.py针对APP配置的URL名称，是该APP所有URL的总路径，即在blog.urls配置的URL前都要加上根urls.py配置的URL 配置URL为空时不要直接r&#39;&#39;，因为这样为空时可以访问，输入任意字符也可以访问，要使用r&#39;^$&#39; 配置URL时注意正则表达式结尾符号$和/ 开发第一个Template Django里的Templates其实就是HTML文件 Templates是用来Django模版语言（Django Template Language, DTL） 也可以使用第三方模版（如Jinja2） 要修改模版引擎，在settings.py文件中找到TEMPLATES修改键值对&#39;BACKEND&#39;: &#39;django.template.backends.django.DjangoTemplates&#39;,改为要使用的模版引擎就可以了。 步骤： 在APP的根目录下创建名叫templates的目录，如myblog/blog/templates 在templates目录下创建HTML文件,如index.html 12345678910&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Blog&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Hello, Blog!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; 在views.py中返回render()： 1234from django.shortcuts import renderdef index(request): return render(request, &apos;index.html&apos;) 然后访问http://localhost:8000/blog/index/就会出现index.html页面了。 DTL初步使用： render()函数中支持一个dict类型参数 该字典是后台传递到模版的参数，键为参数名 在模版中用来直接使用 所以，上面的views.py可以改成：1234from django.shortcuts import renderdef index(request): return render(request, &apos;index.html&apos;, &#123;&apos;hello&apos;: &apos;Hello, Blog!&apos;&#125;) 然后，index.html中的&lt;h1&gt;可以改成:1&lt;h1&gt;&#123;&#123; hello &#125;&#125;&lt;/h1&gt; 注意：Django按照INSTALLED_APPS中的添加顺序查找templates，不同APP下templs目录中的同名HTML文件会造成冲突 解决templates冲突方案：在APP的templates目录下创建以APP名为名称的目录，将html文件放入新创建的目录下。所以，新建templates目录时都在该目录下新建一个与应用名同名的目录。 ModelsDjango中的Models： 通常，一个Model对应数据库的一张表 Django中的Models以类的形式表现 它包含了一些基本字段以及数据的一些行为 ORM： 对象关系映射（Object Relation Mapping） 实现了对象和数据库之间的映射 隐藏了数据访问的细节，不需要编写SQL语句 编写Models 在应用根目录下创建models.py，并引入models模块 创建类，继承models.Model，该类即是一张数据表 在类中创建字段 字段即类里面的属性（变量）,字段创建：1attr = models.CharField(max_length=64) 关于类的字段及可选参数等设置可以点击官网查看 生成数据表命令行中进入manage.py同级目录执行以下命令：1python manage.py makemigrations app_name（可选） 不写应用名app_name默认该项目下所有应用都生成数据迁移。执行之后可以看到：123Migrations for &apos;blog&apos;: blog\migrations\0001_initial.py - Create model Arcticle 之后，再执行以下命令：1python manage.py migrate 可以看到结果：1234567891011121314151617Operations to perform: Apply all migrations: admin, auth, blog, contenttypes, sessionsRunning migrations: Applying contenttypes.0001_initial... OK Applying auth.0001_initial... OK Applying admin.0001_initial... OK Applying admin.0002_logentry_remove_auto_add... OK Applying contenttypes.0002_remove_content_type_name... OK Applying auth.0002_alter_permission_name_max_length... OK Applying auth.0003_alter_user_email_max_length... OK Applying auth.0004_alter_user_username_opts... OK Applying auth.0005_alter_user_last_login_null... OK Applying auth.0006_require_contenttypes_0002... OK Applying auth.0007_alter_validators_add_error_messages... OK Applying auth.0008_alter_user_username_max_length... OK Applying blog.0001_initial... OK Applying sessions.0001_initial... OK 上面那些auth、admin等是settings里INSTALLED_APPS自带应用的数据迁移。 查看Django会自动在app_name/migrations/目录下生成移植文件。可以看到在blog\migrations\0001_initial.py文件里的fields有多一个：1(&apos;id&apos;, models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name=&apos;ID&apos;)), 这是因为在创建模型时我们没有人为添加主键，所以Django为数据表创建了这个主键，如果人为添加主键就不会有这个id了。 执行以下命令查看SQL语句：1python manage.py sqlmigrate 应用名 文件id 如：python manage.py sqlmigrate blog 0001可以看到：123456BEGIN;---- Create model Arcticle--CREATE TABLE &quot;blog_arcticle&quot; (&quot;id&quot; integer NOT NULL PRIMARY KEY AUTOINCREMENT, &quot;title&quot; varchar(32) NOT NULL, &quot;content&quot; text NULL);COMMIT; 默认sqllite3的数据库在项目根目录下db.sqlite3 查看并编辑db.sqlite3使用轻量级第三方免费软件：SQLite Expert Personal就可以在blog_article表中增加一条测试数据。 页面呈现数据后台步骤：views.py中import models获取模型累的一个具体对象:1article = models.Article.objects.get(pk=1) get()的参数就是要指定的数据的标识，除了pk也可以用title或content。其实就相当于SELECT语句。之后使用render()把对象传递到前端：1render(request, page, &#123;&apos;article&apos;: article&#125;) 前端步骤：模版可直接使用对象以及对象的“.”操作，如:12&lt;h1&gt;&#123;&#123; article.title &#125;&#125;&lt;/h1&gt;&lt;h3&gt;&#123;&#123; article.content &#125;&#125;&lt;/h3&gt; Admin Admin是Django自带的一个功能强大的自动化数据管理界面 被授权的用户可直接在Admin中管理数据库 Django提供了许多针对Admin的定制功能 创建超级用户执行以下命令：1python manage.py createsuperuser 然后输入用户名、密码，邮箱可以留空。 通过http://localhost:8000/admin访问管理系统，系统默认是英文的，要改成中文可以修改settings.py中的LANGUAGE_CODE：1LANGUAGE_CODE = &apos;zh-Hans&apos; 配置应用在应用下admin.py中引入自身的models模块（或里面的模型类），然后编辑admin.py：1admin.site.register(models.Article) 然后Admin管理系统里就会出现Blog管理，包括Article的增删改查操作。 修改数据默认显示名称可以看到在Article管理界面，数据默认显示名称是Arcticle object，要修改成显示文章title： 在Article类下添加一个方法 方法名：Python3：__str__(self)、Python2：__unicode__(self) return self.title 完善博客页面概要： 博客主页面 博客文章内容页面 博客撰写页面 博客页面开发博客主页面： 文章标题列表，超链接 发表博客按钮（超链接） 文章标题列表编写思路： 取出数据库中所有文章对象 将文章对象打包成列表，传递到前端 前端页面把文章以标题超链接的形式逐个排列，模版for循环（注意这里不是两个花括号！）：123&#123;% for xx in xxs %&#125;HTML语句&#123;% endfor %&#125; views.py的index()函数改为：123def index(request): articles = models.Arcticle.objects.all() # 获取所有文章 return render(request, &apos;blog/index.html&apos;, &#123;&apos;articles&apos;: articles&#125;) index.html页面内容改成：123456&lt;h1&gt; &lt;a href=&quot;&quot;&gt;新增文章&lt;/a&gt;&lt;/h1&gt;&#123;% for article in articles %&#125; &lt;a href=&quot;&quot;&gt;&#123;&#123; article.title &#125;&#125;&lt;/a&gt;&lt;br/&gt;&#123;% endfor %&#125; 博客文章页面开发页面内容： 标题 文章内容 修改文章按钮（超链接） views.py新增函数article_page()：123def article_page(request, article_id): article = models.Arcticle.objects.get(pk=article_id) return render(request, &apos;blog/article_page.html&apos;, &#123;&apos;article&apos;: article&#125;) 新增一个templates/blog/article_page.html填如下内容：12345&lt;h1&gt;&#123;&#123; article.title &#125;&#125;&lt;/h1&gt;&lt;br&gt;&lt;h3&gt;&#123;&#123; article.content &#125;&#125;&lt;/h3&gt;&lt;br&gt;&lt;br&gt;&lt;a href=&quot;&quot;&gt;修改文章&lt;/a&gt; 修改blog/urls.py是应用里自行新建的urls.py增加文章的URL配置：1234urlpatterns = [ url(r&apos;^index/$&apos;, views.index), url(r&apos;^article/(?P&lt;article_id&gt;[0-9]+)/$&apos;, views.article_page),] 注意写法：把匹配到的数字以article_id作为组名，组名必须和响应函数的参数名一致，否则会报错。 Django模版中的超链接配置超链接目标地址： href后面是目标地址 template中可以用以下语句配置： 1&#123;% url &apos;app_name:url_name&apos; param %&#125; 其中app_name和url_name都在url中配置 url()函数的名称参数： 根urls.py，写在include()的第二个参数位置，namespace=&#39;blog&#39; 应用下则写在url()的第三个参数位置，name=&#39;article&#39; 主要取决于是否使用include引用了另一个URL配置文件 所以我们先修改根urls.py,include()的第二个参数写上namespace：1234urlpatterns = [ url(r&apos;^admin/&apos;, admin.site.urls), url(r&apos;^blog/&apos;, include(&apos;blog.urls&apos;, namespace=&apos;blog&apos;)),] 然后修改应用blog下的urls,py，在url()的第三个参数位置，写上name：1234urlpatterns = [ url(r&apos;^index/$&apos;, views.index), url(r&apos;^article/(?P&lt;article_id&gt;[0-9]+)/$&apos;, views.article_page, name=&apos;article_page&apos;),] index.html页面for循环里的&lt;a&gt;标签做些修改：123456&lt;h1&gt; &lt;a href=&quot;&quot;&gt;新增文章&lt;/a&gt;&lt;/h1&gt;&#123;% for article in articles %&#125; &lt;a href=&quot;&#123;% url &apos;blog:article_page&apos; article.id %&#125;&quot;&gt;&#123;&#123; article.title &#125;&#125;&lt;/a&gt;&lt;br/&gt;&#123;% endfor %&#125; 博客撰写页面页面内容： 标题编辑栏 文章内容编辑区域 提交按钮 编辑响应函数 使用request.POST[&#39;参数名&#39;]获取表单数据，或者request.POST.get(&#39;title&#39;, &#39;TITLE&#39;)，TITLE为设置的默认值 使用以下语句创建对象：1models.Article.objects.create(title,content) 修改views.py添加edit_page()函数用于显示一个edit_page.html修改页面：12def edit_page(request): return render(request, &apos;blog/edit_page.html&apos;) 修改views.py添加edit_action()函数：123456def edit_action(request): title = request.POST.get(&apos;title&apos;, &apos;TITLE&apos;) content = request.POST.get(&apos;content&apos;, &apos;CONTENT&apos;) models.Arcticle.objects.create(title=title, content=content) articles = models.Arcticle.objects.all() return render(request, &apos;blog/index.html&apos;, &#123;&apos;articles&apos;: articles&#125;) 修改blog/urls.py的URL配置新增修改页面的配置：12url(r&apos;^edit/$&apos;, views.edit_page, name=&apos;edit_page&apos;),url(r&apos;^edit/action/$&apos;, views.edit_action, name=&apos;edit_action&apos;), 新建一个templates/blog/edit_page.html123456789101112&lt;form action=&quot;&#123;% url &apos;blog:edit_action&apos; %&#125;&quot; method=&quot;post&quot;&gt; &#123;% csrf_token %&#125; &lt;label&gt;文章标题： &lt;input type=&quot;text&quot; name=&quot;title&quot;/&gt; &lt;/label&gt; &lt;br/&gt;&lt;br/&gt; &lt;label&gt;文章内容： &lt;input type=&quot;text&quot; name=&quot;content&quot;/&gt; &lt;/label&gt; &lt;br/&gt;&lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;提交&quot;&gt;&lt;/form&gt; 出于安全性，POST请求都需要加以下语句：1&#123;% csrf_token %&#125; 否则会报错403。 我们在新建博客成功之后，会跳转到首页，可以发现浏览器里地址还是指向刚才的提交表单地址，这时候刷新页面，会多添加一条数据。将edit_action()函数的：12articles = models.Arcticle.objects.all()return render(request, &apos;blog/index.html&apos;, &#123;&apos;articles&apos;: articles&#125;) 改成：1return HttpResponseRedirect(&apos;/blog/index&apos;) 防止这种情况。 接下来看修改文章链接的完善，它与新文章编辑页面的区别： 新文章为空，修改文章有内容 修改文章页面有文章对象 这个文章对象通过文章ID指定 修改数据： article.title = title article.content = content article().save() 修改views.py的edit_page()函数，主键ID是从1开始的，所以新建文章时传入参数0，返回空的新建文章页面，而修改时则传入文章ID，找到这篇文章并将该对象返回前端页面显示：12345def edit_page(request, article_id): if str(article_id) == &apos;0&apos;: return render(request, &apos;blog/edit_page.html&apos;) article = models.Arcticle.objects.get(pk=article_id) return render(request, &apos;blog/edit_page.html&apos;, &#123;&apos;article&apos;: article&#125;) 修改views.py的edit_action()函数实现article_id为0则新建文章，为已有文章ID则修改：12345678910111213def edit_action(request): title = request.POST.get(&apos;title&apos;, &apos;TITLE&apos;) # 也可以用request.POST[&apos;title&apos;]，这里的TITLE为默认值 content = request.POST.get(&apos;content&apos;, &apos;CONTENT&apos;) article_id = request.POST.get(&apos;article_id&apos;, &apos;0&apos;) if article_id == &apos;0&apos;: models.Arcticle.objects.create(title=title, content=content) return HttpResponseRedirect(&apos;/blog/index&apos;) article = models.Arcticle.objects.get(pk=article_id) article.title = title article.content = content article.save() return render(request, &apos;blog/article_page.html&apos;, &#123;&apos;article&apos;: article&#125;) 修改blog/urls.py的URL配置url(r&#39;^edit/$&#39;, views.edit_page, name=&#39;edit_page&#39;),改为：1url(r&apos;^edit/(?P&lt;article_id&gt;[0-9]+)/$&apos;, views.edit_page, name=&apos;edit_page&apos;), 修改article_page.html里的&lt;a&gt;标签：1&lt;a href=&quot;&#123;% url &apos;blog:edit_page&apos; article.id %&#125;&quot;&gt;修改文章&lt;/a&gt; 修改edit_page.html的form表单：1234567891011121314151617181920212223242526&lt;form action=&quot;&#123;% url &apos;blog:edit_action&apos; %&#125;&quot; method=&quot;post&quot;&gt; &lt;!--出于安全性，POST请求都需要加这句话，否则会报错403--&gt; &#123;% csrf_token %&#125; &#123;% if article %&#125; &lt;input type=&quot;hidden&quot; name=&quot;article_id&quot; value=&quot;&#123;&#123; article.id &#125;&#125;&quot;/&gt; &lt;label&gt;文章标题： &lt;input type=&quot;text&quot; name=&quot;title&quot; value=&quot;&#123;&#123; article.title &#125;&#125;&quot;/&gt; &lt;/label&gt; &lt;br/&gt;&lt;br/&gt; &lt;label&gt;文章内容： &lt;input type=&quot;text&quot; name=&quot;content&quot; value=&quot;&#123;&#123; article.content &#125;&#125;&quot;/&gt; &lt;/label&gt; &lt;br/&gt;&lt;br/&gt; &#123;% else %&#125; &lt;input type=&quot;hidden&quot; name=&quot;article_id&quot; value=&quot;0&quot;/&gt; &lt;label&gt;文章标题： &lt;input type=&quot;text&quot; name=&quot;title&quot;/&gt; &lt;/label&gt; &lt;br/&gt;&lt;br/&gt; &lt;label&gt;文章内容： &lt;input type=&quot;text&quot; name=&quot;content&quot;/&gt; &lt;/label&gt; &lt;br/&gt;&lt;br/&gt; &#123;% endif %&#125; &lt;input type=&quot;submit&quot; value=&quot;提交&quot;&gt;&lt;/form&gt; 也可以写的比较简洁：1234567891011121314&lt;form action=&quot;&#123;% url &apos;blog:edit_action&apos; %&#125;&quot; method=&quot;post&quot;&gt; &lt;!--出于安全性，POST请求都需要加这句话，否则会报错403--&gt; &#123;% csrf_token %&#125; &lt;input type=&quot;hidden&quot; name=&quot;article_id&quot; value=&quot;&#123;% if article %&#125;&#123;&#123; article.id &#125;&#125;&#123;% else %&#125;0&#123;% endif %&#125;&quot;/&gt; &lt;label&gt;文章标题： &lt;input type=&quot;text&quot; name=&quot;title&quot; value=&quot;&#123;% if article %&#125;&#123;&#123; article.title &#125;&#125;&#123;% endif %&#125;&quot;/&gt; &lt;/label&gt; &lt;br/&gt;&lt;br/&gt; &lt;label&gt;文章内容： &lt;input type=&quot;text&quot; name=&quot;content&quot; value=&quot;&#123;% if article %&#125;&#123;&#123; article.content &#125;&#125;&#123;% endif %&#125;&quot;/&gt; &lt;/label&gt; &lt;br/&gt;&lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;提交&quot;&gt;&lt;/form&gt; 要稍微注意的就是第一个用于传递article_id的&lt;input&gt;标签里0左右不要有空格，否则会一起传值到后台产生错误。 补充Templates过滤器过滤器： 写在模版中，属于Django模版语言 可以修改模版中的变量，从而显示不同的内容 使用：1&#123;&#123; value | filter &#125;&#125; 例：1&#123;&#123; list_nums | length &#125;&#125; 过滤器可叠加：1&#123;&#123; value | filter1 | filter2 |...&#125;&#125; 注意：如果模版中出现了不存在的变量，Django不会报错，只会给一个空值（空字符串）。 使用过滤器，前面edit_page.html的form表单可以更简单地改成：12345678910&lt;input type=&quot;hidden&quot; name=&quot;article_id&quot; value=&quot;&#123;&#123; article.id | default:&apos;0&apos; &#125;&#125;&quot;/&gt; &lt;label&gt;文章标题： &lt;input type=&quot;text&quot; name=&quot;title&quot; value=&quot;&#123;&#123; article.title &#125;&#125;&quot;/&gt; &lt;/label&gt; &lt;br/&gt;&lt;br/&gt; &lt;label&gt;文章内容： &lt;input type=&quot;text&quot; name=&quot;content&quot; value=&quot;&#123;&#123; article.content &#125;&#125;&quot;/&gt; &lt;/label&gt; &lt;br/&gt;&lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;提交&quot;&gt; 更多Django内建过滤器访问官方文档 Django ShellDjango Shell： 它是一个Python的交互式命令行程序 它自动引入了我们的项目环境 我们可以使用它与我们的项目进行交互 使用：运行以下命令：1python manage.py shell 如果安装了ipython，这个命令就会加载ipython解释器，否则会加载Python自带解释器。 然后可以在该环境中直接与项目交互：12from blog.models import ArticleArticle.objects.all() 以上命令会得到：1234In [1]: from blog.models import ArticleIn [2]: Article.objects.all()Out[2]: &lt;QuerySet [&lt;Article: 我的第一篇文章&gt;, &lt;Article: Title&gt;]&gt; 以title是因为我们之前在models.py写了__str__(self)方法设置显示title。 用途： 可以使用Django Shell进行一些调试工作 测试未知的（文档或网上得知的）方法或函数 如：12In [3]: Article.objects.all().values()Out[3]: &lt;QuerySet [&#123;&apos;id&apos;: 1, &apos;title&apos;: &apos;我的第一篇文章&apos;, &apos;content&apos;: &apos;太阳当空照，花儿对我笑，小鸟说早早早，你为什么背上炸药包？……&apos;&#125;, &#123;&apos;id&apos;: 2, &apos;title&apos;: &apos;Title&apos;, &apos;content&apos;: &apos;哈哈哈哈wwww&apos;&#125;]&gt; Admin增强我们可以自行修改Admin管理系统中的显示内容，比如除了显示文章的title，把内容等也在之后的列显示。方法如下： 创建admin配置类：1class ArticleAdmin(admin.ModelAdmin) 改变注册：1admin.site.register(Article, ArticleAdmin) 修改admin.py：1234567from django.contrib import adminfrom blog.models import Articleclass ArticleAdmin(admin.ModelAdmin): passadmin.site.register(Article, ArticleAdmin) 这时候浏览器打开Admin管理系统，没报错就可以开始实现功能了。 显示其他字段：1list_display = (&apos;title&apos;, &apos;content&apos;) list_display属性的值是一个包含字符串格式的字段名的tuple或list。建议使用tuple，因为tuple不可变，比较安全。 注意：字段名必须是字符串！而且要与Model一致不要写错。 过滤器：和Templates过滤器没有关系。这是用于筛选数据方便管理的。1list_filter = (&apos;pub_time&apos;, ) # tuple只有一个元素时记得加个逗号 更多关于Admin的功能可以查看官方文档 结语教程到这里就结束了，代码可以点击我的GitHub查看。]]></content>
      <categories>
        <category>Python</category>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫常用库pyquery详解]]></title>
    <url>%2FPython%2F%E7%88%AC%E8%99%AB%2FPython%E7%88%AC%E8%99%AB%E5%B8%B8%E7%94%A8%E5%BA%93pyquery%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[简介pyquery是一个强大的网页解析库。如果熟悉jquery，那么pyquery用起来也会很简单。 官方的说法是： pyquery allows you to make jquery queries on xml documents. The API is as much as possible the similar to jquery. pyquery uses lxml for fast xml and html manipulation. 初始化通过字符串初始化导入一般都这样写：from pyquery import PyQuery as pq12345678910111213141516171819202122In [1]: from pyquery import PyQuery as pqIn [2]: html = &apos;&apos;&apos; ...: &lt;div&gt; ...: &lt;ul&gt; ...: &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; ...: &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; ...: &lt;/ul&gt; ...: &lt;/div&gt; ...: &apos;&apos;&apos;In [3]: doc = pq(html)In [4]: print(doc(&apos;li&apos;))&lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; 使用pq(html)初始化对象后就可以像jquery一样使用选择器了。 通过URL初始化123456In [5]: from pyquery import PyQuery as pqIn [6]: doc = pq(url=&apos;http://www.baidu.com&apos;)In [7]: print(doc(&apos;head&apos;))&lt;head&gt;&lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html;charset=utf-8&quot;/&gt;&lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=Edge&quot;/&gt;&lt;meta content=&quot;always&quot; name=&quot;referrer&quot;/&gt;&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css&quot;/&gt;&lt;title&gt;ç¾åº¦ä¸ä¸ï¼ä½ å°±ç¥é&lt;/title&gt;&lt;/head&gt; 通过文件初始化12345678910In [8]: from pyquery import PyQuery as pqIn [9]: doc = pq(filename=&apos;demo.html&apos;)In [10]: print(doc(&apos;li&apos;))&lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; 当然，这需要在当前目录下有一个demo.html文件。 基本CSS选择器12345678910111213141516171819202122In [11]: from pyquery import PyQuery as pqIn [12]: html = &apos;&apos;&apos; ...: &lt;div id=&quot;container&quot;&gt; ...: &lt;ul class=&quot;list&quot;&gt; ...: &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; ...: &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; ...: &lt;/ul&gt; ...: &lt;/div&gt; ...: &apos;&apos;&apos;In [13]: doc = pq(html)In [14]: print(doc(&apos;#container .list li&apos;))&lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; doc(&#39;#container .list li&#39;)选择id为container的元素的下层元素里class为list的元素里查找li标签。这些元素不需要是直接的父与子元素，只要是有层级关系就可以就可以。 查找元素子元素123456789101112131415161718192021222324252627282930313233343536373839404142In [15]: from pyquery import PyQuery as pqIn [16]: html = &apos;&apos;&apos; ...: &lt;div id=&quot;container&quot;&gt; ...: &lt;ul class=&quot;list&quot;&gt; ...: &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; ...: &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; ...: &lt;/ul&gt; ...: &lt;/div&gt; ...: &apos;&apos;&apos;In [17]: doc = pq(html)In [18]: items = doc(&apos;.list&apos;)In [19]: print(type(items))&lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt;In [20]: print(items)&lt;ul class=&quot;list&quot;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;In [21]: lis = items.find(&apos;li&apos;)In [22]: print(type(lis))&lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt;In [23]: print(lis)&lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; find()找出所有符合条件的下层元素。可以看到，结果都是PyQuery对象。 此外，还有一个chilren()，它是找出直接子元素：1234567891011In [24]: lis = items.children()In [25]: print(type(lis))&lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt;In [26]: print(lis)&lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; 当然，这里输出的结果和上面一样。children()也可以传入参数，如查找class为active的子标签：12345In [27]: lis = items.children(&apos;.active&apos;)In [28]: print(lis)&lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; 父元素123456789101112131415161718192021222324252627282930313233In [29]: from pyquery import PyQuery as pqIn [30]: html = &apos;&apos;&apos; ...: &lt;div id=&quot;container&quot;&gt; ...: &lt;ul class=&quot;list&quot;&gt; ...: &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; ...: &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; ...: &lt;/ul&gt; ...: &lt;/div&gt; ...: &apos;&apos;&apos;In [31]: doc = pq(html)In [32]: items = doc(&apos;.list&apos;)In [33]: container = items.parent()In [34]: print(type(container))&lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt;In [35]: print(container)&lt;div id=&quot;container&quot;&gt; &lt;ul class=&quot;list&quot;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; parent()找出父元素，父元素只有一个，此外，还有parents()，它找出所有祖先元素：123456789101112131415161718192021222324252627282930313233343536373839404142434445In [36]: from pyquery import PyQuery as pqIn [37]: html = &apos;&apos;&apos; ...: &lt;div class=&quot;wrap&quot;&gt; ...: &lt;div id=&quot;container&quot;&gt; ...: &lt;ul class=&quot;list&quot;&gt; ...: &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; ...: &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; ...: &lt;/ul&gt; ...: &lt;/div&gt; ...: &lt;/div&gt; ...: &apos;&apos;&apos;In [38]: doc = pq(html)In [39]: items = doc(&apos;.list&apos;)In [40]: parents = items.parents()In [41]: print(type(parents))&lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt;In [42]: print(parents)&lt;div class=&quot;wrap&quot;&gt; &lt;div id=&quot;container&quot;&gt; &lt;ul class=&quot;list&quot;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;&lt;div id=&quot;container&quot;&gt; &lt;ul class=&quot;list&quot;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; parents()还可以传入一个css选择器进行筛选：1234567891011121314In [43]: parent = items.parents(&apos;.wrap&apos;)In [44]: print(parent)&lt;div class=&quot;wrap&quot;&gt; &lt;div id=&quot;container&quot;&gt; &lt;ul class=&quot;list&quot;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; 兄弟元素12345678910111213141516171819202122232425In [45]: from pyquery import PyQuery as pqIn [46]: html = &apos;&apos;&apos; ...: &lt;div class=&quot;wrap&quot;&gt; ...: &lt;div id=&quot;container&quot;&gt; ...: &lt;ul class=&quot;list&quot;&gt; ...: &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; ...: &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; ...: &lt;/ul&gt; ...: &lt;/div&gt; ...: &lt;/div&gt; ...: &apos;&apos;&apos;In [47]: doc = pq(html)In [48]: li = doc(&apos;.list .item-0.active&apos;)In [49]: print(li.siblings())&lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; 这里doc(&#39;.list .item-0.active&#39;)在class为list的元素里筛选class包含item-0和active的元素。注意：这里.item-0.active中间没有空格，中间的.表示并列。结果只会有&lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;这一条，然后siblings()找出兄弟节点。 当然，siblings()也可以传入选择器：12345678910111213141516171819202122In [50]: from pyquery import PyQuery as pqIn [51]: html = &apos;&apos;&apos; ...: &lt;div class=&quot;wrap&quot;&gt; ...: &lt;div id=&quot;container&quot;&gt; ...: &lt;ul class=&quot;list&quot;&gt; ...: &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; ...: &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; ...: &lt;/ul&gt; ...: &lt;/div&gt; ...: &lt;/div&gt; ...: &apos;&apos;&apos;In [52]: doc = pq(html)In [53]: li = doc(&apos;.list .item-0.active&apos;)In [54]: print(li.siblings(&apos;.active&apos;))&lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; 遍历1234567891011121314151617181920212223242526272829303132333435In [55]: from pyquery import PyQuery as pqIn [56]: html = &apos;&apos;&apos; ...: &lt;div class=&quot;wrap&quot;&gt; ...: &lt;div id=&quot;container&quot;&gt; ...: &lt;ul class=&quot;list&quot;&gt; ...: &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; ...: &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; ...: &lt;/ul&gt; ...: &lt;/div&gt; ...: &lt;/div&gt; ...: &apos;&apos;&apos;In [57]: doc = pq(html)In [58]: lis = doc(&apos;li&apos;).items()In [59]: print(type(lis))&lt;class &apos;generator&apos;&gt;In [60]: for li in lis: ...: print(li) ...:&lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt;&lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; 使用.items()把结果变成一个生成器。然后就可以用for循环遍历。 获取信息获取属性12345678910111213141516171819202122232425262728In [61]: from pyquery import PyQuery as pqIn [62]: html = &apos;&apos;&apos; ...: &lt;div class=&quot;wrap&quot;&gt; ...: &lt;div id=&quot;container&quot;&gt; ...: &lt;ul class=&quot;list&quot;&gt; ...: &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; ...: &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; ...: &lt;/ul&gt; ...: &lt;/div&gt; ...: &lt;/div&gt; ...: &apos;&apos;&apos;In [63]: doc = pq(html)In [64]: a = doc(&apos;.item-0.active a&apos;)In [65]: print(a)&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;In [66]: print(a.attr(&apos;href&apos;))link3.htmlIn [67]: print(a.attr.href)link3.html 使用.attr(&#39;href&#39;)或者直接使用.都可以访问属性。 获取文本直接使用.text()就可以获取文本：12345678910111213141516171819202122232425In [68]: from pyquery import PyQuery as pqIn [69]: html = &apos;&apos;&apos; ...: &lt;div class=&quot;wrap&quot;&gt; ...: &lt;div id=&quot;container&quot;&gt; ...: &lt;ul class=&quot;list&quot;&gt; ...: &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; ...: &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; ...: &lt;/ul&gt; ...: &lt;/div&gt; ...: &lt;/div&gt; ...: &apos;&apos;&apos;In [70]: doc = pq(html)In [71]: a = doc(&apos;.item-0.active a&apos;)In [72]: print(a)&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;In [73]: print(a.text())third item 获取HTML.html()获取选中标签包含的HTML代码：1234567891011121314151617181920212223242526In [74]: from pyquery import PyQuery as pqIn [75]: html = &apos;&apos;&apos; ...: &lt;div class=&quot;wrap&quot;&gt; ...: &lt;div id=&quot;container&quot;&gt; ...: &lt;ul class=&quot;list&quot;&gt; ...: &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; ...: &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; ...: &lt;/ul&gt; ...: &lt;/div&gt; ...: &lt;/div&gt; ...: &apos;&apos;&apos;In [76]: doc = pq(html)In [77]: li = doc(&apos;.item-0.active&apos;)In [78]: print(li)&lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;In [79]: print(li.html())&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt; DOM操作addClass()、removeClass()添加或删除class：123456789101112131415161718192021222324252627282930313233343536In [80]: from pyquery import PyQuery as pqIn [81]: html = &apos;&apos;&apos; ...: &lt;div class=&quot;wrap&quot;&gt; ...: &lt;div id=&quot;container&quot;&gt; ...: &lt;ul class=&quot;list&quot;&gt; ...: &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; ...: &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; ...: &lt;/ul&gt; ...: &lt;/div&gt; ...: &lt;/div&gt; ...: &apos;&apos;&apos;In [82]: doc = pq(html)In [83]: li = doc(&apos;.item-0.active&apos;)In [84]: print(li)&lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;In [85]: li.removeClass(&apos;active&apos;)Out[85]: [&lt;li.item-0&gt;]In [86]: print(li)&lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;In [87]: li.addClass(&apos;active&apos;)Out[87]: [&lt;li.item-0.active&gt;]In [88]: print(li)&lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; attr()、css()修改属性或css：123456789101112131415161718192021222324252627282930313233343536In [89]: from pyquery import PyQuery as pqIn [90]: html = &apos;&apos;&apos; ...: &lt;div class=&quot;wrap&quot;&gt; ...: &lt;div id=&quot;container&quot;&gt; ...: &lt;ul class=&quot;list&quot;&gt; ...: &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; ...: &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; ...: &lt;/ul&gt; ...: &lt;/div&gt; ...: &lt;/div&gt; ...: &apos;&apos;&apos;In [91]: doc = pq(html)In [92]: li = doc(&apos;.item-0.active&apos;)In [93]: print(li)&lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;In [94]: li.attr(&apos;name&apos;, &apos;link&apos;)Out[94]: [&lt;li.item-0.active&gt;]In [95]: print(li)&lt;li class=&quot;item-0 active&quot; name=&quot;link&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;In [96]: li.css(&apos;font-size&apos;, &apos;14px&apos;)Out[96]: [&lt;li.item-0.active&gt;]In [97]: print(li)&lt;li class=&quot;item-0 active&quot; name=&quot;link&quot; style=&quot;font-size: 14px&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; remove()删除指定元素：12345678910111213141516171819202122In [98]: from pyquery import PyQuery as pqIn [99]: html = &apos;&apos;&apos; ...: &lt;div class=&quot;wrap&quot;&gt; ...: Hello, World ...: &lt;p&gt;This is a paragraph.&lt;/p&gt; ...: &lt;/div&gt; ...: &apos;&apos;&apos;In [100]: doc = pq(html)In [101]: wrap = doc(&apos;.wrap&apos;)In [102]: print(wrap.text())Hello, WorldThis is a paragraph.In [103]: wrap.find(&apos;p&apos;).remove()Out[103]: [&lt;p&gt;]In [104]: print(wrap.text())Hello, World 其他DOM方法其他DOM方法可以参考文档。 伪类选择器123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354In [105]: from pyquery import PyQuery as pqIn [106]: html = &apos;&apos;&apos; ...: &lt;div class=&quot;wrap&quot;&gt; ...: &lt;div id=&quot;container&quot;&gt; ...: &lt;ul class=&quot;list&quot;&gt; ...: &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; ...: &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; ...: &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; ...: &lt;/ul&gt; ...: &lt;/div&gt; ...: &lt;/div&gt; ...: &apos;&apos;&apos;In [107]: doc = pq(html)In [108]: li = doc(&apos;li:first-child&apos;)In [109]: print(li)&lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt;In [110]: li = doc(&apos;li:last-child&apos;)In [111]: print(li)&lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;In [112]: li = doc(&apos;li:nth-child(2)&apos;)In [113]: print(li)&lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;In [114]: li = doc(&apos;li:gt(2)&apos;)In [115]: print(li)&lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;In [116]: li = doc(&apos;li:nth-child(2n)&apos;)In [117]: print(li)&lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;In [118]: li = doc(&apos;li:contains(second)&apos;)In [119]: print(li)&lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; 结语更多有关css选择器的内容可以参考w3school，更多有关pyquery的内容可以参考官方文档。]]></content>
      <categories>
        <category>Python</category>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python2和3版本共存]]></title>
    <url>%2FPython%2FPython2%E5%92%8C3%E7%89%88%E6%9C%AC%E5%85%B1%E5%AD%98%2F</url>
    <content type="text"><![CDATA[前言尽管Python官方也说Python2是遗留语言，Python3才是未来: Python 2.x is legacy, Python 3.x is the present and future of the language 但是有些时候因为一些特殊需求我们可能还是需要用到另一个版本，这时候就需要配置让两个版本共存了。 环境变量首先我们要先知道环境变量，主要是里面的Path变量。它的意思是，你需要系统运行一个程序但是没有指定程序所在的路径时，系统出了在当前的路径中寻找这个程序之外，还应该道Path中指定的路径去找。 比如，当我们安装了Java、maven或Python之类的程序后，如果没有配置好环境变量，那么打开命令行的位置不是安装路径的bin目录的话，运行相应命令时就会出现:12&apos;xxx&apos; 不是内部或外部命令，也不是可运行的程序或批处理文件。 而配置好环境变量之后就可以在非安装目录下运行程序，这就是环境变量Path的用处。 配置Python共存我电脑上安装了： Python3.6.1（D:\Program Files\Python3.6.1） Python2.7.14（D:\Program Files\Python2.7.14） 首先，我们把它们的路径都配置到环境变量Path里：1D:\Program Files\Python3.6.1\Scripts\;D:\Program Files\Python3.6.1\;D:\Program Files\Python2.7.14\Scripts\;D:\Program Files\Python2.7.14\; 另外，我电脑上还安装了一个Anaconda3，这个在安装时选择了Add Anaconda to my PATH environment variable，就不用自己配置环境变量了。 然后我们打开cmd命令行执行where python:1234C:\Users\ASUS&gt;where pythonD:\Program Files\Python3.6.1\python.exeD:\Program Files\Python2.7.14\python.exeD:\ProgramFiles\Anaconda3\python.exe 出现了我们现在安装了的几个Python。 然后，执行python：1234C:\Users\ASUS&gt;pythonPython 3.6.1 (v3.6.1:69c0db5, Mar 21 2017, 17:54:52) [MSC v.1900 32 bit (Intel)] on win32Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; 可以看到默认执行的是环境变量里配置的第一个版本。 所以，我们只需要给可执行文件取个名字用于区分版本就可以了。具体做法： 在Python3的文件夹里找到python.exe把它ctrl + C然后ctrl + V复制一份，重命名python3.exe 同理，Python2文件夹也复制一份python.exe重命名python2.exe 同样，Anaconda文件夹里的python.exe重命名python-conda.exe 然后测试一下：123456789101112C:\Users\ASUS&gt;where pythonD:\Program Files\Python3.6.1\python.exeD:\Program Files\Python2.7.14\python.exeD:\ProgramFiles\Anaconda3\python.exeC:\Users\ASUS&gt;where python3D:\Program Files\Python3.6.1\python3.exeC:\Users\ASUS&gt;python-condaPython 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 10:21:59) [MSC v.1900 32 bit (Intel)] on win32Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; 但是，输入python进入的还是默认在环境变量里配置在最前面的版本，我的是Python3.6.1：1234C:\Users\ASUS&gt;pythonPython 3.6.1 (v3.6.1:69c0db5, Mar 21 2017, 17:54:52) [MSC v.1900 32 bit (Intel)] on win32Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; 配置pip共存对于pip的配置也是同样的方法，只是可以看到在Python2和Python3的Scripts目录下除了pip.exe以外还都已经自带了pip2.exe或pip3.exe 12345678910C:\Users\ASUS&gt;where pipD:\Program Files\Python3.6.1\Scripts\pip.exeD:\Program Files\Python2.7.14\Scripts\pip.exeD:\ProgramFiles\Anaconda3\Scripts\pip.exeC:\Users\ASUS&gt;where pip2D:\Program Files\Python2.7.14\Scripts\pip2.exeC:\Users\ASUS&gt;where pip3D:\Program Files\Python3.6.1\Scripts\pip3.exe pip有个查看版本的命令pip -V：12345678C:\Users\ASUS&gt;pip -Vpip 9.0.1 from d:\program files\python3.6.1\lib\site-packages (python 3.6)C:\Users\ASUS&gt;pip2 -Vpip 9.0.1 from d:\program files\python2.7.14\lib\site-packages (python 2.7)C:\Users\ASUS&gt;pip3 -Vpip 9.0.1 from d:\program files\python3.6.1\lib\site-packages (python 3.6) 虽然Anaconda有conda命令，不过还是配置一下Anaconda的pip吧。 找到Anaconda安装目录的Scripts目录，复制一份pip.exe重命名为pip-conda.exe12C:\Users\ASUS&gt;where pip-condaD:\ProgramFiles\Anaconda3\Scripts\pip-conda.exe 但是，运行pip-conda -V会发现：12C:\Users\ASUS&gt;pip-conda -VScript file &apos;D:\ProgramFiles\Anaconda3\Scripts\pip-conda-script.py&apos; is not present. 有点不一样的是：Anaconda的pip需要查找一份pip-script.py，所以我们还需要复制一份这个重命名为pip-conda-script.py，这时候再运行pip-conda -V：12C:\Users\ASUS&gt;pip-conda -Vpip 9.0.1 from D:\ProgramFiles\Anaconda3\lib\site-packages (python 3.6) 使用pip默认的也是环境变量里最前面配置的版本。 完成这样，Windows下Python版本共存的配置就完成了。 在Linux下配置的方法也是一样的，使用创建软链接的命令ln -s实现。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy选择器详解]]></title>
    <url>%2FPython%2F%E7%88%AC%E8%99%AB%2Fscrapy%E9%80%89%E6%8B%A9%E5%99%A8%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[简介Scrapy提取数据的一套机制称作选择器(seletors)，它们通过特定的XPath或者CSS表达式来“选择” HTML文件中的某个部分。详细的选择器说明点击这里参考文档。 使用Scrapy提供了一个样例页面https://doc.scrapy.org/en/latest/_static/selectors-sample1.html用于测试，页面源码：123456789101112131415&lt;html&gt; &lt;head&gt; &lt;base href=&apos;http://example.com/&apos; /&gt; &lt;title&gt;Example website&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;div id=&apos;images&apos;&gt; &lt;a href=&apos;image1.html&apos;&gt;Name: My image 1 &lt;br /&gt;&lt;img src=&apos;image1_thumb.jpg&apos; /&gt;&lt;/a&gt; &lt;a href=&apos;image2.html&apos;&gt;Name: My image 2 &lt;br /&gt;&lt;img src=&apos;image2_thumb.jpg&apos; /&gt;&lt;/a&gt; &lt;a href=&apos;image3.html&apos;&gt;Name: My image 3 &lt;br /&gt;&lt;img src=&apos;image3_thumb.jpg&apos; /&gt;&lt;/a&gt; &lt;a href=&apos;image4.html&apos;&gt;Name: My image 4 &lt;br /&gt;&lt;img src=&apos;image4_thumb.jpg&apos; /&gt;&lt;/a&gt; &lt;a href=&apos;image5.html&apos;&gt;Name: My image 5 &lt;br /&gt;&lt;img src=&apos;image5_thumb.jpg&apos; /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/body&gt;&lt;/html&gt; 打开命令行，输入：1scrapy shell http://doc.scrapy.org/en/latest/_static/selectors-sample1.html 接着就可以获得response变量了，可以用它在终端做一些测试，如response.selector.xpath()或response.selector.css()，如：1234567891011In [3]: response.selector.xpath(&apos;//*[@id=&quot;images&quot;]/a[2]&apos;)Out[3]: [&lt;Selector xpath=&apos;//*[@id=&quot;images&quot;]/a[2]&apos; data=&apos;&lt;a href=&quot;image2.html&quot;&gt;Name: My image 2 &lt;&apos;&gt;]In [4]: response.selector.xpath(&apos;//*[@id=&quot;images&quot;]/a[2]/text()&apos;)Out[4]: [&lt;Selector xpath=&apos;//*[@id=&quot;images&quot;]/a[2]/text()&apos; data=&apos;Name: My image 2 &apos;&gt;]In [5]: response.selector.xpath(&apos;//*[@id=&quot;images&quot;]/a[2]/text()&apos;).extract()Out[5]: [&apos;Name: My image 2 &apos;]In [6]: response.selector.css(&apos;title::text&apos;).extract()Out[6]: [&apos;Example website&apos;] extract()返回结果文本的一个列表，extract_first()返回第一个结果。 为了方便，Scrapy还提供了简化的使用方法response.xpath()和response.css()，另外，两种选择器返回相同的选择器列表，因此可以嵌套使用，如：12345678910In [7]: response.selector.xpath(&apos;//div[@id=&quot;images&quot;]&apos;).css(&apos;img::attr(src)&apos;).extract()Out[7]:[&apos;image1_thumb.jpg&apos;, &apos;image2_thumb.jpg&apos;, &apos;image3_thumb.jpg&apos;, &apos;image4_thumb.jpg&apos;, &apos;image5_thumb.jpg&apos;]In [8]: response.selector.xpath(&apos;//div[@id=&quot;images&quot;]&apos;).css(&apos;img::attr(src)&apos;).extract_first()Out[8]: &apos;image1_thumb.jpg&apos; extract_first()还可以传递一个参数·default表示默认值，如果找不到，就使用这个默认值：12In [11]: response.selector.xpath(&apos;//div[@id=&quot;images&quot;]&apos;).css(&apos;img::attr(src2)&apos;).extract_first(default=&apos;test&apos;)Out[11]: &apos;test&apos; 选择属性：123456789101112131415161718192021222324In [14]: response.xpath(&apos;//a/@href&apos;).extract()Out[14]: [&apos;image1.html&apos;, &apos;image2.html&apos;, &apos;image3.html&apos;, &apos;image4.html&apos;, &apos;image5.html&apos;]In [15]: response.xpath(&apos;//a/@href&apos;)Out[15]:[&lt;Selector xpath=&apos;//a/@href&apos; data=&apos;image1.html&apos;&gt;, &lt;Selector xpath=&apos;//a/@href&apos; data=&apos;image2.html&apos;&gt;, &lt;Selector xpath=&apos;//a/@href&apos; data=&apos;image3.html&apos;&gt;, &lt;Selector xpath=&apos;//a/@href&apos; data=&apos;image4.html&apos;&gt;, &lt;Selector xpath=&apos;//a/@href&apos; data=&apos;image5.html&apos;&gt;]In [16]: response.xpath(&apos;//a/@href&apos;).extract()Out[16]: [&apos;image1.html&apos;, &apos;image2.html&apos;, &apos;image3.html&apos;, &apos;image4.html&apos;, &apos;image5.html&apos;]In [17]: response.css(&apos;a::attr(href)&apos;)Out[17]:[&lt;Selector xpath=&apos;descendant-or-self::a/@href&apos; data=&apos;image1.html&apos;&gt;, &lt;Selector xpath=&apos;descendant-or-self::a/@href&apos; data=&apos;image2.html&apos;&gt;, &lt;Selector xpath=&apos;descendant-or-self::a/@href&apos; data=&apos;image3.html&apos;&gt;, &lt;Selector xpath=&apos;descendant-or-self::a/@href&apos; data=&apos;image4.html&apos;&gt;, &lt;Selector xpath=&apos;descendant-or-self::a/@href&apos; data=&apos;image5.html&apos;&gt;]In [18]: response.css(&apos;a::attr(href)&apos;).extract()Out[18]: [&apos;image1.html&apos;, &apos;image2.html&apos;, &apos;image3.html&apos;, &apos;image4.html&apos;, &apos;image5.html&apos;] 选择文本：12345678910111213141516171819202122232425262728293031In [22]: response.xpath(&apos;//a/text()&apos;)Out[22]:[&lt;Selector xpath=&apos;//a/text()&apos; data=&apos;Name: My image 1 &apos;&gt;, &lt;Selector xpath=&apos;//a/text()&apos; data=&apos;Name: My image 2 &apos;&gt;, &lt;Selector xpath=&apos;//a/text()&apos; data=&apos;Name: My image 3 &apos;&gt;, &lt;Selector xpath=&apos;//a/text()&apos; data=&apos;Name: My image 4 &apos;&gt;, &lt;Selector xpath=&apos;//a/text()&apos; data=&apos;Name: My image 5 &apos;&gt;]In [23]: response.xpath(&apos;//a/text()&apos;).extract()Out[23]:[&apos;Name: My image 1 &apos;, &apos;Name: My image 2 &apos;, &apos;Name: My image 3 &apos;, &apos;Name: My image 4 &apos;, &apos;Name: My image 5 &apos;]In [24]: response.css(&apos;a::text&apos;)Out[24]:[&lt;Selector xpath=&apos;descendant-or-self::a/text()&apos; data=&apos;Name: My image 1 &apos;&gt;, &lt;Selector xpath=&apos;descendant-or-self::a/text()&apos; data=&apos;Name: My image 2 &apos;&gt;, &lt;Selector xpath=&apos;descendant-or-self::a/text()&apos; data=&apos;Name: My image 3 &apos;&gt;, &lt;Selector xpath=&apos;descendant-or-self::a/text()&apos; data=&apos;Name: My image 4 &apos;&gt;, &lt;Selector xpath=&apos;descendant-or-self::a/text()&apos; data=&apos;Name: My image 5 &apos;&gt;]In [25]: response.css(&apos;a::text&apos;).extract()Out[25]:[&apos;Name: My image 1 &apos;, &apos;Name: My image 2 &apos;, &apos;Name: My image 3 &apos;, &apos;Name: My image 4 &apos;, &apos;Name: My image 5 &apos;] 选择属性名称包含image的链接：123456789101112131415161718192021In [27]: response.xpath(&apos;//a[contains(@href, &quot;image&quot;)]/@href&apos;)Out[27]:[&lt;Selector xpath=&apos;//a[contains(@href, &quot;image&quot;)]/@href&apos; data=&apos;image1.html&apos;&gt;, &lt;Selector xpath=&apos;//a[contains(@href, &quot;image&quot;)]/@href&apos; data=&apos;image2.html&apos;&gt;, &lt;Selector xpath=&apos;//a[contains(@href, &quot;image&quot;)]/@href&apos; data=&apos;image3.html&apos;&gt;, &lt;Selector xpath=&apos;//a[contains(@href, &quot;image&quot;)]/@href&apos; data=&apos;image4.html&apos;&gt;, &lt;Selector xpath=&apos;//a[contains(@href, &quot;image&quot;)]/@href&apos; data=&apos;image5.html&apos;&gt;]In [28]: response.xpath(&apos;//a[contains(@href, &quot;image&quot;)]/@href&apos;).extract()Out[28]: [&apos;image1.html&apos;, &apos;image2.html&apos;, &apos;image3.html&apos;, &apos;image4.html&apos;, &apos;image5.html&apos;]In [29]: response.css(&apos;a[href*=image]::attr(href)&apos;)Out[29]:[&lt;Selector xpath=&quot;descendant-or-self::a[@href and contains(@href, &apos;image&apos;)]/@href&quot; data=&apos;image1.html&apos;&gt;, &lt;Selector xpath=&quot;descendant-or-self::a[@href and contains(@href, &apos;image&apos;)]/@href&quot; data=&apos;image2.html&apos;&gt;, &lt;Selector xpath=&quot;descendant-or-self::a[@href and contains(@href, &apos;image&apos;)]/@href&quot; data=&apos;image3.html&apos;&gt;, &lt;Selector xpath=&quot;descendant-or-self::a[@href and contains(@href, &apos;image&apos;)]/@href&quot; data=&apos;image4.html&apos;&gt;, &lt;Selector xpath=&quot;descendant-or-self::a[@href and contains(@href, &apos;image&apos;)]/@href&quot; data=&apos;image5.html&apos;&gt;]In [30]: response.css(&apos;a[href*=image]::attr(href)&apos;).extract()Out[30]: [&apos;image1.html&apos;, &apos;image2.html&apos;, &apos;image3.html&apos;, &apos;image4.html&apos;, &apos;image5.html&apos;] 选择属性名称包含image的&lt;a&gt;里&lt;img&gt;的src：123456789101112131415In [31]: response.xpath(&apos;//a[contains(@href, &quot;image&quot;)]/img/@src&apos;).extract()Out[31]:[&apos;image1_thumb.jpg&apos;, &apos;image2_thumb.jpg&apos;, &apos;image3_thumb.jpg&apos;, &apos;image4_thumb.jpg&apos;, &apos;image5_thumb.jpg&apos;]In [32]: response.css(&apos;a[href*=image] img::attr(src)&apos;).extract()Out[32]:[&apos;image1_thumb.jpg&apos;, &apos;image2_thumb.jpg&apos;, &apos;image3_thumb.jpg&apos;, &apos;image4_thumb.jpg&apos;, &apos;image5_thumb.jpg&apos;] 选择器还可以使用re()结合正则表达式使用，但是re()返回的不是选择器对象而是字符串列表；还有一个类似extract_first()的方法re_first()选择第一个匹配内容。 如获取&lt;a&gt;文本里Name:后面的内容：123456789101112131415161718192021In [33]: response.css(&apos;a::text&apos;).extract()Out[33]:[&apos;Name: My image 1 &apos;, &apos;Name: My image 2 &apos;, &apos;Name: My image 3 &apos;, &apos;Name: My image 4 &apos;, &apos;Name: My image 5 &apos;]In [34]: response.css(&apos;a::text&apos;).re(&apos;Name\:(.*)&apos;)Out[34]:[&apos; My image 1 &apos;, &apos; My image 2 &apos;, &apos; My image 3 &apos;, &apos; My image 4 &apos;, &apos; My image 5 &apos;]In [35]: response.css(&apos;a::text&apos;).re_first(&apos;Name\:(.*)&apos;)Out[35]: &apos; My image 1 &apos;In [36]: response.css(&apos;a::text&apos;).re_first(&apos;Name\:(.*)&apos;).strip()Out[36]: &apos;My image 1&apos; 小技巧Chrome浏览器的开发者工具的console界面除了可以调试js，还可以调试xpath和css，所以我们的选择器也可以在这里测试，可以结合页面查看比较方便。只是要注意，Scrapy的a::text这种语法就不支持了。 xpath选择器使用方法$x()：1$x(&apos;//*[@id=&quot;images&quot;]/a[1]&apos;) css使用方法$$()：1$$(&apos;#images &gt; a:nth-child(1)&apos;) 另外，elements界面按esc键可以直接在elements界面打开console，还可以直接在elements界面右键Copy –&gt; Copy Xpath或Copy Selector把选中元素的选择器复制下来。]]></content>
      <categories>
        <category>Python</category>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy命令行详解]]></title>
    <url>%2FPython%2F%E7%88%AC%E8%99%AB%2Fscrapy%E5%91%BD%E4%BB%A4%E8%A1%8C%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[简介Scrapy是通过scrapy命令行工具进行控制的。详细的命令行工具说明点击这里参考文档。 一般来说，使用 scrapy 工具的第一件事就是创建您的Scrapy项目：1scrapy startproject project_name 该命令将会在project_name目录中创建一个Scrapy项目。接下来，进入到项目目录中:1cd project_name 一个Scrapy项目的目录结构如下：123456789101112scrapy.cfgproject_name/ __init__.py items.py middlewares.py pipelines.py settings.py spiders/ __init__.py spider1.py spider2.py ... 其中，scrapy.cfg存放的目录被认为是项目的根目录。这时候就可以在项目中使用scrapy工具来对其进行控制和管理。比如，创建一个新的spider:1scrapy genspider mydomain mydomain.com 这个命令会在spiders/目录下创建一个mydomain.py文件，就可以修改编写爬虫了。 Scrapy提供了两种类型的命令。一种必须在Scrapy项目中运行(项目命令)，另外一种则不需要(全局命令)。 全局命令全局命令就是不需要在项目中使用的命令。 startproject在project_name文件夹下创建一个名为project_name的Scrapy项目。用法：1scrapy startproject &lt;project_name&gt; [project_dir] 如果没有指定project_dir，将与project_name使用相同名字。 settings这个命令在项目中运行时会输出项目的设定值，否则输出Scrapy默认设定。用法：1scrapy settings [options] runspider可以在未创建项目的情况下，运行一个编写在Python文件中的spider。用法：1scrapy runspider &lt;spider_file.py&gt; shell是一个交互终端，可以在未启动spider的情况下尝试及调试爬取代码。以给定的URL(如果给出)或者空(没有给出URL)启动Scrapy shell。这个终端可以用来测试XPath或CSS表达式，也可以作为正常Python终端。如果安装了IPython，Scrapy终端将使用IPython替代标准Python终端。1scrapy shell [url] Scrapy终端根据下载的页面会自动创建一些方便使用的对象: crawler - 当前 Crawler 对象. spider - 处理URL的spider。 对当前URL没有处理的Spider时则为一个 Spider 对象。 request - 最近获取到的页面的 Request 对象。 可以使用replace()修改该request。或者 使用fetch快捷方式来获取新的request。 response - 包含最近获取到的页面的 Response 对象。 sel - 根据最近获取到的response构建的 Selector 对象。 settings - 当前的 Scrapy settings 更多关于shell的内容，可以参考文档。 fetch使用Scrapy下载器下载给定的URL，并将获取到的内容输出，用法：1scrapy fetch &lt;url&gt; 还可以加一些其他参数，--nolog不输出日志，--headers输出headers：1scrapy fetch --nolog --headers http://www.example.com/ view在浏览器中打开给定的URL，并以Scrapy spider获取到的形式展现。相当于保存获取的源码到本地，有些时候因为页面使用Ajax+js渲染页面，spider获取到的页面和用户浏览器看到的并不相同，可以用来检查spider所获取到的页面。用法：1scrapy view &lt;url&gt; version输出Scrapy版本。配合-v运行时，该命令同时输出Python, Twisted以及平台的信息。用法：1scrapy version [-v] 项目命令项目命令就是必须依赖于项目，要在项目里运行的命令。 crawl使用spider进行爬取。用法：1scrapy crawl spider_name check运行contract检查代码是否有错误，用法：1scrapy check list列出当前项目中所有可用的spider。用法：1scrapy list edit使用EDITOR中设定的编辑器编辑给定的spider，用法：1scrapy edit spider_name 用的不多，直接使用IDE比较好。 parse获取给定的URL并使用相应的spider分析处理。如果提供--callback选项，则使用spider的该方法处理，否则使用parse。用法：1scrapy parse &lt;url&gt; [options] 支持的选项： --spider=SPIDER: 跳过自动检测spider并强制使用特定的spider --a NAME=VALUE: 设置spider的参数(可能被重复) --callback or -c: spider中用于解析返回(response)的回调函数 --pipelines: 在pipeline中处理item --rules or -r: 使用 CrawlSpider 规则来发现用来解析返回(response)的回调函数 --noitems: 不显示爬取到的item --nolinks: 不显示提取到的链接 --nocolour: 避免使用pygments对输出着色 --depth or -d: 指定跟进链接请求的层次数(默认: 1) --verbose or -v: 显示每个请求的详细信息 genspider该方法可以使用提前定义好的模板来生成spider，使用scrapy genspider -l可以查看可用模版，scrapy genspider -d [template]可以查看模版具体内容。使用：1scrapy genspider [-t template] &lt;name&gt; &lt;domain&gt; bench运行benchmark测试，用法：1scrapy bench]]></content>
      <categories>
        <category>Python</category>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫常用库requests详解]]></title>
    <url>%2FPython%2F%E7%88%AC%E8%99%AB%2FPython%E7%88%AC%E8%99%AB%E5%B8%B8%E7%94%A8%E5%BA%93requests%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[简介官方的说法是： Requests: 让 HTTP 服务人类Requests 唯一的一个非转基因的 Python HTTP 库，人类可以安全享用。 警告：非专业使用其他 HTTP 库会导致危险的副作用，包括：安全缺陷症、冗余代码症、重新发明轮子症、啃文档症、抑郁、头疼、甚至死亡。 Requests 是以 PEP 20 的箴言为中心开发的： Beautiful is better than ugly.(美丽优于丑陋) Explicit is better than implicit.(直白优于含蓄) Simple is better than complex.(简单优于复杂) Complex is better than complicated.(复杂优于繁琐) Readability counts.(可读性很重要) 总之，这是一个Python实现的简单方便的HTTP库。 请求GET请求12345678910111213141516171819202122In [1]: import requestsIn [2]: response = requests.get(&apos;https://www.baidu.com/&apos;)In [3]: print(type(response))&lt;class &apos;requests.models.Response&apos;&gt;In [4]: print(response.status_code)200In [5]: print(type(response.text))&lt;class &apos;str&apos;&gt;In [6]: print(response.cookies)&lt;RequestsCookieJar[&lt;Cookie BDORZ=27315 for .baidu.com/&gt;]&gt;In [7]: print(response.text)&lt;!DOCTYPE html&gt;&lt;!--STATUS OK--&gt;&lt;html&gt; &lt;head&gt;&lt;meta http-equiv=content-type content=text/html;charset=utf-8&gt;&lt;meta http-equiv=X-UA-Compatible content=IE=Edge......&lt;/body&gt; &lt;/html&gt; response.status_code获取状态码，response.text获取网页源码，相当于urllib里的read()方法，也不用使用decode()转码。response.cookies可以直接获取cookie不用引入额外的模块。 带参数的GET请求：1234567891011121314151617181920In [14]: import requestsIn [15]: response = requests.get(&quot;http://httpbin.org/get?name=jeff&amp;age=21&quot;)In [16]: print(response.text)&#123; &quot;args&quot;: &#123; &quot;age&quot;: &quot;21&quot;, &quot;name&quot;: &quot;jeff&quot; &#125;, &quot;headers&quot;: &#123; &quot;Accept&quot;: &quot;*/*&quot;, &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, &quot;Connection&quot;: &quot;close&quot;, &quot;Host&quot;: &quot;httpbin.org&quot;, &quot;User-Agent&quot;: &quot;python-requests/2.18.4&quot; &#125;, &quot;origin&quot;: &quot;183.21.190.87&quot;, &quot;url&quot;: &quot;http://httpbin.org/get?name=jeff&amp;age=21&quot;&#125; 也可以使用一个参数params：12345678910111213141516171819202122232425In [17]: import requestsIn [18]: data = &#123; ...: &apos;name&apos;: &apos;jeff&apos;, ...: &apos;age&apos;: 21 ...: &#125;In [19]: response = requests.get(&quot;http://httpbin.org/get&quot;, params=data)In [20]: print(response.text)&#123; &quot;args&quot;: &#123; &quot;age&quot;: &quot;21&quot;, &quot;name&quot;: &quot;jeff&quot; &#125;, &quot;headers&quot;: &#123; &quot;Accept&quot;: &quot;*/*&quot;, &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, &quot;Connection&quot;: &quot;close&quot;, &quot;Host&quot;: &quot;httpbin.org&quot;, &quot;User-Agent&quot;: &quot;python-requests/2.18.4&quot; &#125;, &quot;origin&quot;: &quot;183.21.190.87&quot;, &quot;url&quot;: &quot;http://httpbin.org/get?name=jeff&amp;age=21&quot;&#125; 解析json1234567891011121314151617In [21]: import requestsIn [22]: response = requests.get(&quot;http://httpbin.org/get&quot;)In [23]: print(type(response.text))&lt;class &apos;str&apos;&gt;In [24]: print(response.json())&#123;&apos;args&apos;: &#123;&#125;, &apos;headers&apos;: &#123;&apos;Accept&apos;: &apos;*/*&apos;, &apos;Accept-Encoding&apos;: &apos;gzip, deflate&apos;, &apos;Connection&apos;: &apos;close&apos;, &apos;Host&apos;: &apos;httpbin.org&apos;, &apos;User-Agent&apos;: &apos;python-requests/2.18.4&apos;&#125;, &apos;origin&apos;: &apos;183.21.190.87&apos;, &apos;url&apos;: &apos;http://httpbin.org/get&apos;&#125;In [25]: print(type(response.json()))&lt;class &apos;dict&apos;&gt;In [26]: import jsonIn [27]: print(json.loads(response.text))&#123;&apos;args&apos;: &#123;&#125;, &apos;headers&apos;: &#123;&apos;Accept&apos;: &apos;*/*&apos;, &apos;Accept-Encoding&apos;: &apos;gzip, deflate&apos;, &apos;Connection&apos;: &apos;close&apos;, &apos;Host&apos;: &apos;httpbin.org&apos;, &apos;User-Agent&apos;: &apos;python-requests/2.18.4&apos;&#125;, &apos;origin&apos;: &apos;183.21.190.87&apos;, &apos;url&apos;: &apos;http://httpbin.org/get&apos;&#125; 可以看到直接使用response.json()和使用json.loads(response.text)结果是一样的。 获取二进制数据1234567891011In [33]: import requestsIn [34]: response = requests.get(&quot;https://github.com/favicon.ico&quot;)In [35]: print(type(response.text), type(response.content))&lt;class &apos;str&apos;&gt; &lt;class &apos;bytes&apos;&gt;In [36]: print(response.content)b&apos;\x00\x00\x01\x00\x02\x00\x10\x10\x00\x00\x01\x00 \x00(\x05\x00\x00&amp;\x00\x00\x00 \x00\x00\x01\x00 \x00(\x14\x00\x00N\x05\x00\x00(\x00\x00\x00\x10\x00\x00\x00 \x00\x00\x00\x01\x00 \x00\x00\x00\x00\x00\x00\x05\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x11\x11\x13v\x13\x13\x13\xc5\x0e\x0e\x0e\x12\x00\x00\x00\x00\x00\x00\x00\x00\x0f\x0f\x0f\x11\x11\x11\x14\xb1\x13\x13\x13i\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x14\x14\x14\x96\x13\x13\x14\xfc\x13\x13\x14\xed\x00\x00\x00\x19\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x18\x15\x15\x17\xff\x15\x15\x17\xff\x11\x11\x13\x85\x00\x00\x00...... 保存：1234567In [37]: import requestsIn [38]: response = requests.get(&quot;https://github.com/favicon.ico&quot;)In [39]: with open(&apos;favicon.ico&apos;, &apos;wb&apos;) as f: ...: f.write(response.content) ...: 添加headers例如知乎，没有添加headers请求会返回500：12345678In [40]: import requestsIn [41]: response = requests.get(&quot;https://www.zhihu.com&quot;)In [42]: print(response.text)&lt;html&gt;&lt;body&gt;&lt;h1&gt;500 Server Error&lt;/h1&gt;An internal server error occured.&lt;/body&gt;&lt;/html&gt; 使用headers参数：123456789101112131415In [45]: import requestsIn [46]: headers = &#123; ...: &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63. ...: 0.3239.132 Safari/537.36&apos; ...: &#125;In [47]: response = requests.get(&quot;https://www.zhihu.com&quot;, headers=headers)In [48]: print(response.text)&lt;!doctype html&gt;&lt;html lang=&quot;zh&quot; data-hairline=&quot;true&quot; data-theme=&quot;light&quot;&gt;&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;/&gt;&lt;title data-react-helmet=&quot;true&quot;&gt;知 乎 - 发现更大的世界&lt;/title&gt;&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,initial-scale=1,maximum-scale=1&quot;/&gt;&lt;meta name=&quot;renderer&quot; content=&quot;webkit&quot;/&gt;......&lt;/body&gt;&lt;/html&gt; POST请求1234567891011121314151617181920212223242526272829303132In [50]: import requestsIn [51]: ata = &#123;&apos;name&apos;: &apos;jeff&apos;, &apos;age&apos;: &apos;21&apos;&#125;In [52]: import requestsIn [53]: data = &#123;&apos;name&apos;: &apos;jeff&apos;, &apos;age&apos;: &apos;21&apos;&#125;In [54]: response = requests.post(&quot;http://httpbin.org/post&quot;, data=data)In [55]: print(response.text)&#123; &quot;args&quot;: &#123;&#125;, &quot;data&quot;: &quot;&quot;, &quot;files&quot;: &#123;&#125;, &quot;form&quot;: &#123; &quot;age&quot;: &quot;21&quot;, &quot;name&quot;: &quot;jeff&quot; &#125;, &quot;headers&quot;: &#123; &quot;Accept&quot;: &quot;*/*&quot;, &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, &quot;Connection&quot;: &quot;close&quot;, &quot;Content-Length&quot;: &quot;16&quot;, &quot;Content-Type&quot;: &quot;application/x-www-form-urlencoded&quot;, &quot;Host&quot;: &quot;httpbin.org&quot;, &quot;User-Agent&quot;: &quot;python-requests/2.18.4&quot; &#125;, &quot;json&quot;: null, &quot;origin&quot;: &quot;183.21.190.87&quot;, &quot;url&quot;: &quot;http://httpbin.org/post&quot;&#125; 加headers：12345678910111213In [56]: import requestsIn [57]: data = &#123;&apos;name&apos;: &apos;jeff&apos;, &apos;age&apos;: &apos;21&apos;&#125;In [58]: headers = &#123; ...: &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63. ...: 0.3239.132 Safari/537.36&apos; ...: &#125;In [59]: response = requests.post(&quot;http://httpbin.org/post&quot;, data=data, headers=headers)In [60]: print(response.json())&#123;&apos;args&apos;: &#123;&#125;, &apos;data&apos;: &apos;&apos;, &apos;files&apos;: &#123;&#125;, &apos;form&apos;: &#123;&apos;age&apos;: &apos;21&apos;, &apos;name&apos;: &apos;jeff&apos;&#125;, &apos;headers&apos;: &#123;&apos;Accept&apos;: &apos;*/*&apos;, &apos;Accept-Encoding&apos;: &apos;gzip, deflate&apos;, &apos;Connection&apos;: &apos;close&apos;, &apos;Content-Length&apos;: &apos;16&apos;, &apos;Content-Type&apos;: &apos;application/x-www-form-urlencoded&apos;, &apos;Host&apos;: &apos;httpbin.org&apos;, &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36&apos;&#125;, &apos;json&apos;: None, &apos;origin&apos;: &apos;183.21.190.87&apos;, &apos;url&apos;: &apos;http://httpbin.org/post&apos;&#125; 各种类型请求1234567import requestsrequests.get(&apos;http://httpbin.org/get&apos;)requests.post(&apos;http://httpbin.org/post&apos;)requests.put(&apos;http://httpbin.org/put&apos;)requests.delete(&apos;http://httpbin.org/delete&apos;)requests.head(&apos;http://httpbin.org/get&apos;)requests.options(&apos;http://httpbin.org/get&apos;) 响应Response信息123456789101112131415161718In [62]: import requestsIn [63]: response = requests.get(&apos;http://jeffyang.top/&apos;)In [64]: print(type(response.status_code), response.status_code)&lt;class &apos;int&apos;&gt; 200In [65]: print(type(response.headers), response.headers)&lt;class &apos;requests.structures.CaseInsensitiveDict&apos;&gt; &#123;&apos;Server&apos;: &apos;GitHub.com&apos;, &apos;Content-Type&apos;: &apos;text/html; charset=utf-8&apos;, &apos;Last-Modified&apos;: &apos;Sat, 24 Feb 2018 12:56:30 GMT&apos;, &apos;Access-Control-Allow-Origin&apos;: &apos;*&apos;, &apos;Expires&apos;: &apos;Mon, 26 Feb 2018 09:14:14 GMT&apos;, &apos;Cache-Control&apos;: &apos;max-age=600&apos;, &apos;Content-Encoding&apos;: &apos;gzip&apos;, &apos;X-GitHub-Request-Id&apos;: &apos;BB66:6BA9:E59020:F12729:5A93CD8D&apos;, &apos;Content-Length&apos;: &apos;12829&apos;, &apos;Accept-Ranges&apos;: &apos;bytes&apos;, &apos;Date&apos;: &apos;Mon, 26 Feb 2018 09:58:00 GMT&apos;, &apos;Via&apos;: &apos;1.1 varnish&apos;, &apos;Age&apos;: &apos;14&apos;, &apos;Connection&apos;: &apos;keep-alive&apos;, &apos;X-Served-By&apos;: &apos;cache-hnd18737-HND&apos;, &apos;X-Cache&apos;: &apos;HIT&apos;, &apos;X-Cache-Hits&apos;: &apos;1&apos;, &apos;X-Timer&apos;: &apos;S1519639081.677214,VS0,VE0&apos;, &apos;Vary&apos;: &apos;Accept-Encoding&apos;, &apos;X-Fastly-Request-ID&apos;: &apos;38d73d44dc2f717cdb15f5a317f4ae28b5761489&apos;&#125;In [66]: print(type(response.cookies), response.cookies)&lt;class &apos;requests.cookies.RequestsCookieJar&apos;&gt; &lt;RequestsCookieJar[]&gt;In [67]: print(type(response.url), response.url)&lt;class &apos;str&apos;&gt; http://jeffyang.top/In [68]: print(type(response.history), response.history)&lt;class &apos;list&apos;&gt; [] 状态码判断123456In [69]: import requestsIn [70]: response = requests.get(&apos;http://jeffyang.top/&apos;)In [71]: exit() if not response.status_code == 200 else print(&apos;Request Successfully&apos;)Request Successfully 123456In [72]: import requestsIn [73]: response = requests.get(&apos;http://jeffyang.top/404.html&apos;)In [74]: exit() if not response.status_code == requests.codes.not_found else print(&apos;404 Not Found&apos;)404 Not Found 可以看到，我们既能使用数字判断，也可以使用状态码对应的状态判断，如response.status_code == 200或response.status_code == requests.codes.not_found状态码及对应状态：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475100: (&apos;continue&apos;,),101: (&apos;switching_protocols&apos;,),102: (&apos;processing&apos;,),103: (&apos;checkpoint&apos;,),122: (&apos;uri_too_long&apos;, &apos;request_uri_too_long&apos;),200: (&apos;ok&apos;, &apos;okay&apos;, &apos;all_ok&apos;, &apos;all_okay&apos;, &apos;all_good&apos;, &apos;\\o/&apos;, &apos;✓&apos;),201: (&apos;created&apos;,),202: (&apos;accepted&apos;,),203: (&apos;non_authoritative_info&apos;, &apos;non_authoritative_information&apos;),204: (&apos;no_content&apos;,),205: (&apos;reset_content&apos;, &apos;reset&apos;),206: (&apos;partial_content&apos;, &apos;partial&apos;),207: (&apos;multi_status&apos;, &apos;multiple_status&apos;, &apos;multi_stati&apos;, &apos;multiple_stati&apos;),208: (&apos;already_reported&apos;,),226: (&apos;im_used&apos;,),# Redirection.300: (&apos;multiple_choices&apos;,),301: (&apos;moved_permanently&apos;, &apos;moved&apos;, &apos;\\o-&apos;),302: (&apos;found&apos;,),303: (&apos;see_other&apos;, &apos;other&apos;),304: (&apos;not_modified&apos;,),305: (&apos;use_proxy&apos;,),306: (&apos;switch_proxy&apos;,),307: (&apos;temporary_redirect&apos;, &apos;temporary_moved&apos;, &apos;temporary&apos;),308: (&apos;permanent_redirect&apos;, &apos;resume_incomplete&apos;, &apos;resume&apos;,), # These 2 to be removed in 3.0# Client Error.400: (&apos;bad_request&apos;, &apos;bad&apos;),401: (&apos;unauthorized&apos;,),402: (&apos;payment_required&apos;, &apos;payment&apos;),403: (&apos;forbidden&apos;,),404: (&apos;not_found&apos;, &apos;-o-&apos;),405: (&apos;method_not_allowed&apos;, &apos;not_allowed&apos;),406: (&apos;not_acceptable&apos;,),407: (&apos;proxy_authentication_required&apos;, &apos;proxy_auth&apos;, &apos;proxy_authentication&apos;),408: (&apos;request_timeout&apos;, &apos;timeout&apos;),409: (&apos;conflict&apos;,),410: (&apos;gone&apos;,),411: (&apos;length_required&apos;,),412: (&apos;precondition_failed&apos;, &apos;precondition&apos;),413: (&apos;request_entity_too_large&apos;,),414: (&apos;request_uri_too_large&apos;,),415: (&apos;unsupported_media_type&apos;, &apos;unsupported_media&apos;, &apos;media_type&apos;),416: (&apos;requested_range_not_satisfiable&apos;, &apos;requested_range&apos;, &apos;range_not_satisfiable&apos;),417: (&apos;expectation_failed&apos;,),418: (&apos;im_a_teapot&apos;, &apos;teapot&apos;, &apos;i_am_a_teapot&apos;),421: (&apos;misdirected_request&apos;,),422: (&apos;unprocessable_entity&apos;, &apos;unprocessable&apos;),423: (&apos;locked&apos;,),424: (&apos;failed_dependency&apos;, &apos;dependency&apos;),425: (&apos;unordered_collection&apos;, &apos;unordered&apos;),426: (&apos;upgrade_required&apos;, &apos;upgrade&apos;),428: (&apos;precondition_required&apos;, &apos;precondition&apos;),429: (&apos;too_many_requests&apos;, &apos;too_many&apos;),431: (&apos;header_fields_too_large&apos;, &apos;fields_too_large&apos;),444: (&apos;no_response&apos;, &apos;none&apos;),449: (&apos;retry_with&apos;, &apos;retry&apos;),450: (&apos;blocked_by_windows_parental_controls&apos;, &apos;parental_controls&apos;),451: (&apos;unavailable_for_legal_reasons&apos;, &apos;legal_reasons&apos;),499: (&apos;client_closed_request&apos;,),# Server Error.500: (&apos;internal_server_error&apos;, &apos;server_error&apos;, &apos;/o\\&apos;, &apos;✗&apos;),501: (&apos;not_implemented&apos;,),502: (&apos;bad_gateway&apos;,),503: (&apos;service_unavailable&apos;, &apos;unavailable&apos;),504: (&apos;gateway_timeout&apos;,),505: (&apos;http_version_not_supported&apos;, &apos;http_version&apos;),506: (&apos;variant_also_negotiates&apos;,),507: (&apos;insufficient_storage&apos;,),509: (&apos;bandwidth_limit_exceeded&apos;, &apos;bandwidth&apos;),510: (&apos;not_extended&apos;,),511: (&apos;network_authentication_required&apos;, &apos;network_auth&apos;, &apos;network_authentication&apos;), 进阶文件上传123456789101112131415161718192021222324252627In [75]: import requestsIn [76]: files = &#123;&apos;ico&apos;: open(&apos;favicon.ico&apos;, &apos;rb&apos;)&#125;In [77]: response = requests.post(&quot;http://httpbin.org/post&quot;, files=files)In [78]: print(response.text)&#123; &quot;args&quot;: &#123;&#125;, &quot;data&quot;: &quot;&quot;, &quot;files&quot;: &#123; &quot;ico&quot;: &quot;data:application/octet-stream;base64,AAABAAIAEB... ...AAAAAAAA=&quot; &#125;, &quot;form&quot;: &#123;&#125;, &quot;headers&quot;: &#123; &quot;Accept&quot;: &quot;*/*&quot;, &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, &quot;Connection&quot;: &quot;close&quot;, &quot;Content-Length&quot;: &quot;6664&quot;, &quot;Content-Type&quot;: &quot;multipart/form-data; boundary=e98882dde92244bf8e7fdaa6f03255fc&quot;, &quot;Host&quot;: &quot;httpbin.org&quot;, &quot;User-Agent&quot;: &quot;python-requests/2.18.4&quot; &#125;, &quot;json&quot;: null, &quot;origin&quot;: &quot;183.21.190.87&quot;, &quot;url&quot;: &quot;http://httpbin.org/post&quot;&#125; 这里的&#39;ico&#39;可以自己随便起名。 获取cookie1234567891011In [79]: import requestsIn [80]: response = requests.get(&quot;https://www.baidu.com&quot;)In [81]: print(response.cookies)&lt;RequestsCookieJar[&lt;Cookie BDORZ=27315 for .baidu.com/&gt;]&gt;In [82]: for key, value in response.cookies.items(): ...: print(key + &apos;=&apos; + value) ...:BDORZ=27315 使用requests就不用像urllib一样经过CookieJar、handler和opener了。 会话维持用于模拟登陆：1234567891011In [83]: import requestsIn [84]: requests.get(&apos;http://httpbin.org/cookies/set/number/123456789&apos;)Out[84]: &lt;Response [200]&gt;In [85]: response = requests.get(&apos;http://httpbin.org/cookies&apos;)In [86]: print(response.text)&#123; &quot;cookies&quot;: &#123;&#125;&#125; http://httpbin.org/cookies是用于测试cookie的链接。这里设置了number但是下面输出是空的是因为我们使用的两次get是两个独立的操作，没有任何关联。对于这种问题，requests提供了Session对象：123456789101112131415In [87]: import requestsIn [88]: s = requests.Session()In [89]: s.get(&apos;http://httpbin.org/cookies/set/number/123456789&apos;)Out[89]: &lt;Response [200]&gt;In [90]: response = s.get(&apos;http://httpbin.org/cookies&apos;)In [91]: print(response.text)&#123; &quot;cookies&quot;: &#123; &quot;number&quot;: &quot;123456789&quot; &#125;&#125; 证书验证Requests 可以为 HTTPS 请求验证 SSL 证书，就像 web 浏览器一样。SSL 验证默认是开启的，如果证书验证失败，Requests 会抛出 SSLError。123456789In [92]: import requestsIn [93]: response = requests.get(&apos;https://www.12306.cn&apos;)---------------------------------------------------------------------------Error Traceback (most recent call last)d:\program files\python3.6.1\lib\site-packages\urllib3\contrib\pyopenssl.py in wrap_socket(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname)......SSLError: HTTPSConnectionPool(host=&apos;www.12306.cn&apos;, port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(&quot;bad handshake: Error([(&apos;SSL routines&apos;, &apos;tls_process_server_certificate&apos;, &apos;certificate verify failed&apos;)],)&quot;,),)) 可以通过设置verify=False：12345678In [94]: import requestsIn [95]: response = requests.get(&apos;https://www.12306.cn&apos;, verify=False)d:\program files\python3.6.1\lib\site-packages\urllib3\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings InsecureRequestWarning)In [96]: print(response.status_code)200 这里虽然状态码200，但还是有警告，提醒Adding certificate verification is strongly advised.可以通过设置urllib3.disable_warnings()去掉：12345678910In [97]: import requestsIn [98]: from requests.packages import urllib3In [99]: urllib3.disable_warnings()In [100]: response = requests.get(&apos;https://www.12306.cn&apos;, verify=False)In [101]: print(response.status_code)200 也可以指定一个本地证书用作客户端证书，可以是单个文件（包含密钥和证书）或一个包含两个文件路径的元组：1response = requests.get(&apos;https://www.12306.cn&apos;, cert=(&apos;/path/server.crt&apos;, &apos;/path/key&apos;)) 文档还提到其他用法： 你可以为 verify 传入 CA_BUNDLE 文件的路径，或者包含可信任 CA 证书文件的文件夹路径：12&gt;requests.get(&apos;https://github.com&apos;, verify=&apos;/path/to/certfile&apos;)&gt; 或者将其保持在会话中：123&gt;s = requests.Session()&gt;s.verify = &apos;/path/to/certfile&apos;&gt; 更多使用方法请参考文档 代理设置1234567891011121314151617181920212223In [105]: import requestsIn [106]: proxies = &#123; ...: &quot;http&quot;: &quot;http://183.145.201.137:28500&quot;, ...: &quot;https&quot;: &quot;https://183.145.201.137:28500&quot;, ...: &#125;In [107]: response = requests.get(&quot;http://httpbin.org/get&quot;, proxies=proxies)In [108]: print(response.text)&#123; &quot;args&quot;: &#123;&#125;, &quot;headers&quot;: &#123; &quot;Accept&quot;: &quot;*/*&quot;, &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, &quot;Cache-Control&quot;: &quot;max-age=259200&quot;, &quot;Connection&quot;: &quot;close&quot;, &quot;Host&quot;: &quot;httpbin.org&quot;, &quot;User-Agent&quot;: &quot;python-requests/2.18.4&quot; &#125;, &quot;origin&quot;: &quot;183.145.201.137&quot;, &quot;url&quot;: &quot;http://httpbin.org/get&quot;&#125; 如果代理是需要用户名和密码的可以这样写：12345678910111213141516171819202122In [109]: import requestsIn [110]: proxies = &#123; ...: &quot;http&quot;: &quot;http://user:password@183.145.201.137:28500/&quot;, ...: &#125;In [111]: response = requests.get(&quot;http://httpbin.org/get&quot;, proxies=proxies)In [112]: print(response.text)&#123; &quot;args&quot;: &#123;&#125;, &quot;headers&quot;: &#123; &quot;Accept&quot;: &quot;*/*&quot;, &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, &quot;Cache-Control&quot;: &quot;max-age=259200&quot;, &quot;Connection&quot;: &quot;close&quot;, &quot;Host&quot;: &quot;httpbin.org&quot;, &quot;User-Agent&quot;: &quot;python-requests/2.18.4&quot; &#125;, &quot;origin&quot;: &quot;183.145.201.137&quot;, &quot;url&quot;: &quot;http://httpbin.org/get&quot;&#125; 除了基本的HTTP代理Requests还支持SOCKS协议的代理。这是一个可选功能，使用需要安装第三方库:1pip install requests[socks] 使用：1234proxies = &#123; &apos;http&apos;: &apos;socks5://user:pass@host:port&apos;, &apos;https&apos;: &apos;socks5://user:pass@host:port&apos;&#125; 超时设置1234567891011In [133]: import requestsIn [134]: from requests.exceptions import ConnectTimeoutIn [135]: try: ...: response = requests.get(&quot;http://httpbin.org/get&quot;, timeout = 0.2) ...: print(response.status_code) ...: except ConnectTimeout: ...: print(&apos;ConnectTimeout&apos;) ...:ConnectTimeout 这个timeout值将会用作connect和read二者的timeout。如果要分别制定，就传入一个元组：1response = requests.get(&apos;https://github.com&apos;, timeout=(3, 2)) 如果传入timeout=None就会让requests永远等待。 身份认证12345678In [145]: import requestsIn [146]: from requests.auth import HTTPBasicAuthIn [147]: response = requests.get(&apos;https://httpbin.org/basic-auth/user/passwd&apos;, auth=HTTPBasicAuth(&apos;user&apos;, &apos;w&apos;))In [148]: print(response.status_code)401 上面是使用错误密码，下面是使用正确的密码请求的结果：1234567891011121314In [150]: import requestsIn [151]: from requests.auth import HTTPBasicAuthIn [152]: response = requests.get(&apos;https://httpbin.org/basic-auth/user/passwd&apos;, auth=HTTPBasicAuth(&apos;user&apos;, &apos;passwd&apos;))In [153]: print(response.status_code)200In [154]: print(response.text)&#123; &quot;authenticated&quot;: true, &quot;user&quot;: &quot;user&quot;&#125; 异常处理异常处理时，我们可以先捕获子类异常，再捕获父类异常：123456789101112131415In [155]: import requestsIn [156]: from requests.exceptions import ReadTimeout, ConnectionError, RequestExceptionIn [157]: try: ...: response = requests.get(&quot;http://httpbin.org/get&quot;, timeout = 0.5) ...: print(response.status_code) ...: except ReadTimeout: ...: print(&apos;Timeout&apos;) ...: except ConnectionError: ...: print(&apos;Connection error&apos;) ...: except RequestException: ...: print(&apos;Error&apos;) ...:200 更多有关异常的内容可以参考文档。 结语详细的说明和使用可以点击这里查看文档。]]></content>
      <categories>
        <category>Python</category>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫常用库urllib详解]]></title>
    <url>%2FPython%2F%E7%88%AC%E8%99%AB%2FPython%E7%88%AC%E8%99%AB%E5%B8%B8%E7%94%A8%E5%BA%93urllib%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[简介urllib库是Python内置的HTTP请求库，官方文档里说的很清楚： urllib is a package that collects several modules for working with URLs: urllib.request for opening and reading URLs urllib.error containing the exceptions raised by urllib.request urllib.parse for parsing URLs urllib.robotparser for parsing robots.txt files 这个库的使用在Python2里有一些区别，具体差异可以参考： Python 2 Python 3 urllib.urlretrieve() urllib.request.urlretrieve() urllib.urlcleanup() urllib.request.urlcleanup() urllib.quote() urllib.parse.quote() urllib.quote_plus() urllib.parse.quote_plus() urllib.unquote() urllib.parse.unquote() urllib.unquote_plus() urllib.parse.unquote_plus() urllib.urlencode() urllib.parse.urlencode() urllib.pathname2url() urllib.request.pathname2url() urllib.url2pathname() urllib.request.url2pathname() urllib.getproxies() urllib.request.getproxies() urllib.URLopener urllib.request.URLopener urllib.FancyURLopener urllib.request.FancyURLopener urllib.ContentTooShortError urllib.error.ContentTooShortError urllib2.urlopen() urllib.request.urlopen() urllib2.install_opener() urllib.request.install_opener() urllib2.build_opener() urllib.request.build_opener() urllib2.URLError urllib.error.URLError urllib2.HTTPError urllib.error.HTTPError urllib2.Request urllib.request.Request urllib2.OpenerDirector urllib.request.OpenerDirector urllib2.BaseHandler urllib.request.BaseHandler urllib2.HTTPDefaultErrorHandler urllib.request.HTTPDefaultErrorHandler urllib2.HTTPRedirectHandler urllib.request.HTTPRedirectHandler urllib2.HTTPCookieProcessor urllib.request.HTTPCookieProcessor urllib2.ProxyHandler urllib.request.ProxyHandler urllib2.HTTPPasswordMgr urllib.request.HTTPPasswordMgr urllib2.HTTPPasswordMgrWithDefaultRealm urllib.request.HTTPPasswordMgrWithDefaultRealm urllib2.AbstractBasicAuthHandler urllib.request.AbstractBasicAuthHandler urllib2.HTTPBasicAuthHandler urllib.request.HTTPBasicAuthHandler urllib2.ProxyBasicAuthHandler urllib.request.ProxyBasicAuthHandler urllib2.AbstractDigestAuthHandler urllib.request.AbstractDigestAuthHandler urllib2.HTTPDigestAuthHandler urllib.request.HTTPDigestAuthHandler urllib2.ProxyDigestAuthHandler urllib.request.ProxyDigestAuthHandler urllib2.HTTPHandler urllib.request.HTTPHandler urllib2.HTTPSHandler urllib.request.HTTPSHandler urllib2.FileHandler urllib.request.FileHandler urllib2.FTPHandler urllib.request.FTPHandler urllib2.CacheFTPHandler urllib.request.CacheFTPHandler urllib2.UnknownHandler urllib.request.UnknownHandler 我这里用的是Python3.6版本。 urllib.request请求模块urlopen函数属于urllib.request模块，用法如下：1urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None) url为访问地址，data为指定要发送到服务器的附加数据，timeout参数设定超时，后面的是证书和SSL相关参数。12345678910111213In [1]: import urllib.requestIn [2]: response = urllib.request.urlopen(&apos;http://www.baidu.com&apos;)In [3]: print(response.read().decode(&apos;utf-8&apos;))&lt;!DOCTYPE html&gt;&lt;!--STATUS OK--&gt;......&lt;/body&gt;&lt;/html&gt;In [4]: https://httpbin.org/是供给人们测试HTTP请求的网站。下面测试一个POST请求：12345678910In [10]: import urllib.parseIn [11]: import urllib.requestIn [12]: data = bytes(urllib.parse.urlencode(&#123;&apos;name&apos;:&apos;Jeff&apos;&#125;), encoding=&apos;utf8&apos;)In [13]: response = urllib.request.urlopen(&apos;http://httpbin.org/post&apos;, data=data)In [14]: print(response.read())b&apos;&#123;\n &quot;args&quot;: &#123;&#125;, \n &quot;data&quot;: &quot;&quot;, \n &quot;files&quot;: &#123;&#125;, \n &quot;form&quot;: &#123;\n &quot;name&quot;: &quot;Jeff&quot;\n &#125;, \n &quot;headers&quot;: &#123;\n &quot;Accept-Encoding&quot;: &quot;identity&quot;, \n &quot;Connection&quot;: &quot;close&quot;, \n &quot;Content-Length&quot;: &quot;9&quot;, \n &quot;Content-Type&quot;: &quot;application/x-www-form-urlencoded&quot;, \n &quot;Host&quot;: &quot;httpbin.org&quot;, \n &quot;User-Agent&quot;: &quot;Python-urllib/3.6&quot;\n &#125;, \n &quot;json&quot;: null, \n &quot;origin&quot;: &quot;183.21.190.87&quot;, \n &quot;url&quot;: &quot;http://httpbin.org/post&quot;\n&#125;\n&apos; 下面是timeout参数的使用：1234567891011121314151617181920In [15]: import urllib.requestIn [16]: response = urllib.request.urlopen(&apos;http://httpbin.org/get&apos;, timeout=1)In [17]: print(response.read())b&apos;&#123;\n &quot;args&quot;: &#123;&#125;, \n &quot;headers&quot;: &#123;\n &quot;Accept-Encoding&quot;: &quot;identity&quot;, \n &quot;Connection&quot;: &quot;close&quot;, \n &quot;Host&quot;: &quot;httpbin.org&quot;, \n &quot;User-Agent&quot;: &quot;Python-urllib/3.6&quot;\n &#125;, \n &quot;origin&quot;: &quot;183.21.190.87&quot;, \n &quot;url&quot;: &quot;http://httpbin.org/get&quot;\n&#125;\n&apos;In [18]: import socketIn [19]: import urllib.requestIn [20]: import urllib.errorIn [21]: try: ...: response = urllib.request.urlopen(&apos;http://httpbin.org/get&apos;, timeout=0.1) ...: except urllib.error.URLError as e: ...: if isinstance(e.reason, socket.timeout): ...: print(&apos;TIME OUT&apos;) ...:TIME OUT 超过设定的时间就会抛出异常。 请求直接使用urlopen()是不可以请求时加入headers的，要加headers需要构造一个Request对象：12345678910111213141516171819202122232425262728293031323334353637In [33]: from urllib import request, parseIn [34]: url = &apos;http://httpbin.org/post&apos;In [35]: headers = &#123; ...: &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63. ...: 0.3239.132 Safari/537.36&apos;, ...: &apos;Host&apos;: &apos;httpbin.org&apos; ...: &#125;In [36]: dict = &#123; ...: &apos;name&apos;: &apos;Jeff&apos; ...: &#125;In [37]: data = bytes(parse.urlencode(dict), encoding=&apos;utf8&apos;)In [38]: req = request.Request(url=url, headers=headers, method=&apos;POST&apos;)In [39]: response = request.urlopen(req)In [40]: print(response.read().decode(&apos;utf-8&apos;))&#123; &quot;args&quot;: &#123;&#125;, &quot;data&quot;: &quot;&quot;, &quot;files&quot;: &#123;&#125;, &quot;form&quot;: &#123;&#125;, &quot;headers&quot;: &#123; &quot;Accept-Encoding&quot;: &quot;identity&quot;, &quot;Connection&quot;: &quot;close&quot;, &quot;Content-Length&quot;: &quot;0&quot;, &quot;Host&quot;: &quot;httpbin.org&quot;, &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36&quot; &#125;, &quot;json&quot;: null, &quot;origin&quot;: &quot;183.21.190.87&quot;, &quot;url&quot;: &quot;http://httpbin.org/post&quot;&#125; 还有一个方法add_header是用于添加headers的：1req.add_header(&apos;User-Agent&apos;, &apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36&apos;) 响应得到Response之后可以看到响应的一些基本信息：123456789101112131415In [22]: import urllib.requestIn [23]: response = urllib.request.urlopen(&apos;https://www.baidu.com&apos;)In [24]: print(type(response))&lt;class &apos;http.client.HTTPResponse&apos;&gt;In [25]: print(response.status)200In [26]: print(response.getheader(&apos;Server&apos;))BWS/1.1In [27]: print(response.getheaders())[(&apos;Accept-Ranges&apos;, &apos;bytes&apos;), (&apos;Cache-Control&apos;, &apos;no-cache&apos;), (&apos;Content-Length&apos;, &apos;227&apos;), (&apos;Content-Type&apos;, &apos;text/html&apos;), (&apos;Date&apos;, &apos;Sun, 25 Feb 2018 14:38:41 GMT&apos;), (&apos;Last-Modified&apos;, &apos;Sun, 11 Feb 2018 04:46:00 GMT&apos;), (&apos;P3p&apos;, &apos;CP=&quot; OTI DSP COR IVA OUR IND COM &quot;&apos;), (&apos;Pragma&apos;, &apos;no-cache&apos;), (&apos;Server&apos;, &apos;BWS/1.1&apos;), (&apos;Set-Cookie&apos;, &apos;BD_NOT_HTTPS=1; path=/; Max-Age=300&apos;), (&apos;Set-Cookie&apos;, &apos;BIDUPSID=B15F4542153F7063286F0B770AC1501D; expires=Thu, 31-Dec-37 23:55:55 GMT; max-age=2147483647; path=/; domain=.baidu.com&apos;), (&apos;Set-Cookie&apos;, &apos;PSTM=1519569521; expires=Thu, 31-Dec-37 23:55:55 GMT; max-age=2147483647; path=/; domain=.baidu.com&apos;), (&apos;Strict-Transport-Security&apos;, &apos;max-age=0&apos;), (&apos;X-Ua-Compatible&apos;, &apos;IE=Edge,chrome=1&apos;), (&apos;Connection&apos;, &apos;close&apos;)] 使用代理设置代理用于欺骗目标网站,让服务器把请求识别成来自不同地区的请求防止爬虫被封：12345678910111213In [68]: import urllib.requestIn [69]: proxy_handler = urllib.request.ProxyHandler(&#123; ...: &apos;http&apos;: &apos;http://113.121.242.122:30041&apos;, ...: &apos;https&apos;: &apos;https://113.121.242.122:30041&apos; ...: &#125;)In [70]: opener = urllib.request.build_opener(proxy_handler)In [71]: response = opener.open(&apos;http://httpbin.org/get&apos;)In [72]: print(response.read())b&apos;&#123;\n &quot;args&quot;: &#123;&#125;, \n &quot;headers&quot;: &#123;\n &quot;Accept-Encoding&quot;: &quot;identity&quot;, \n &quot;Cache-Control&quot;: &quot;max-age=259200&quot;, \n &quot;Connection&quot;: &quot;close&quot;, \n &quot;Host&quot;: &quot;httpbin.org&quot;, \n &quot;User-Agent&quot;: &quot;Python-urllib/3.6&quot;\n &#125;, \n &quot;origin&quot;: &quot;113.121.242.122&quot;, \n &quot;url&quot;: &quot;http://httpbin.org/get&quot;\n&#125;\n&apos; 可以看到&quot;origin&quot;: &quot;113.121.242.122&quot;是我们设置的代理。 cookie用于保存我们的登录状态。以下代码可以输出访问百度的cookie内容。12345678910111213141516171819In [73]: import http.cookiejar, urllib.requestIn [74]: cookie = http.cookiejar.CookieJar()In [75]: handler = urllib.request.HTTPCookieProcessor(cookie)In [76]: opener = urllib.request.build_opener(handler)In [77]: response = opener.open(&apos;http://www.baidu.com&apos;)In [78]: for item in cookie: ...: print(item.name + &apos;=&apos; + item.value) ...:BAIDUID=B158E71ED10979B2873EAD4F92F69BD8:FG=1BIDUPSID=B158E71ED10979B2873EAD4F92F69BD8H_PS_PSSID=1469_21112_18559_20930PSTM=1519575810BDSVRTM=0BD_HOME=0 也可以把cookie保存起来：12345678910111213In [79]: import http.cookiejar, urllib.requestIn [80]: filename = &quot;cookie.txt&quot;In [81]: cookie = http.cookiejar.MozillaCookieJar(filename)In [82]: handler = urllib.request.HTTPCookieProcessor(cookie)In [83]: opener = urllib.request.build_opener(handler)In [84]: response = opener.open(&apos;http://www.baidu.com&apos;)In [85]: cookie.save(ignore_discard=True, ignore_expires=True) 这是MozillaCookieJar的保存格式，打开cookie.txt可以看到：12345678910# Netscape HTTP Cookie File# http://curl.haxx.se/rfc/cookie_spec.html# This is a generated file! Do not edit..baidu.com TRUE / FALSE 3667060046 BAIDUID 241959CDD6CF8465BDF2D583B828249D:FG=1.baidu.com TRUE / FALSE 3667060046 BIDUPSID 241959CDD6CF8465BDF2D583B828249D.baidu.com TRUE / FALSE H_PS_PSSID 1428_21107_17001_20930.baidu.com TRUE / FALSE 3667060046 PSTM 1519576400www.baidu.com FALSE / FALSE BDSVRTM 0www.baidu.com FALSE / FALSE BD_HOME 0 还有另一种保存格式LWPCookieJar：123456789101112131415In [86]: cookie.save(ignore_discard=True, ignore_expires=True)In [87]: import http.cookiejar, urllib.requestIn [88]: filename = &quot;cookie.txt&quot;In [89]: cookie = http.cookiejar.LWPCookieJar(filename)In [90]: handler = urllib.request.HTTPCookieProcessor(cookie)In [91]: opener = urllib.request.build_opener(handler)In [92]: response = opener.open(&apos;http://www.baidu.com&apos;)In [93]: cookie.save(ignore_discard=True, ignore_expires=True) 打开cookie.txt可以看到：1234567#LWP-Cookies-2.0Set-Cookie3: BAIDUID=&quot;04A75DA71590BD4D6D9839D727E42ABB:FG=1&quot;; path=&quot;/&quot;; domain=&quot;.baidu.com&quot;; path_spec; domain_dot; expires=&quot;2086-03-15 20:26:18Z&quot;; version=0Set-Cookie3: BIDUPSID=04A75DA71590BD4D6D9839D727E42ABB; path=&quot;/&quot;; domain=&quot;.baidu.com&quot;; path_spec; domain_dot; expires=&quot;2086-03-15 20:26:18Z&quot;; version=0Set-Cookie3: H_PS_PSSID=25641_1427_21103_22160; path=&quot;/&quot;; domain=&quot;.baidu.com&quot;; path_spec; domain_dot; discard; version=0Set-Cookie3: PSTM=1519578732; path=&quot;/&quot;; domain=&quot;.baidu.com&quot;; path_spec; domain_dot; expires=&quot;2086-03-15 20:26:18Z&quot;; version=0Set-Cookie3: BDSVRTM=0; path=&quot;/&quot;; domain=&quot;www.baidu.com&quot;; path_spec; discard; version=0Set-Cookie3: BD_HOME=0; path=&quot;/&quot;; domain=&quot;www.baidu.com&quot;; path_spec; discard; version=0 读取保存的cookie：12345678910111213141516171819In [94]: import http.cookiejar, urllib.requestIn [95]: cookie = http.cookiejar.LWPCookieJar()In [96]: cookie.load(&apos;cookie.txt&apos;, ignore_discard=True, ignore_expires=True)In [97]: handler = urllib.request.HTTPCookieProcessor(cookie)In [98]: opener = urllib.request.build_opener(handler)In [99]: response = opener.open(&apos;http://www.baidu.com&apos;)In [100]: print(response.read().decode(&apos;utf-8&apos;))&lt;!DOCTYPE html&gt;&lt;!--STATUS OK--&gt;......&lt;/body&gt;&lt;/html&gt; 这个过程就是把cookie保存下来，然后在下次请求时读取保存的cookie，它适用于需要登录页面。 urllib.error异常处理模块12345678In [102]: from urllib import request, errorIn [103]: try: ...: response = request.urlopen(&apos;http://jeffyang.top/404.html&apos;) ...: except error.URLError as e: ...: print(e.reason) ...:Not Found URLError只有reason属性，而HTTPError还有code和headers，具体可以查看官方文档：12345678910111213141516171819202122232425262728293031In [104]: from urllib import request, errorIn [105]: try: ...: response = request.urlopen(&apos;http://jeffyang.top/404.html&apos;) ...: except error.HTTPError as e: ...: print(e.reason, e.code, e.headers, sep=&apos;\n&apos;) ...: except error.URLError as e: ...: print(e.reason) ...: else: ...: print(&apos;Qequest Successfully!&apos;) ...:Not Found404Server: GitHub.comContent-Type: text/html; charset=utf-8ETag: &quot;5952c2dc-247c&quot;Access-Control-Allow-Origin: *Content-Security-Policy: default-src &apos;none&apos;; style-src &apos;unsafe-inline&apos;; img-src data:; connect-src &apos;self&apos;X-GitHub-Request-Id: EF30:11359:B2B55C:BCEF88:5A92F18BContent-Length: 9340Accept-Ranges: bytesDate: Sun, 25 Feb 2018 17:31:09 GMTVia: 1.1 varnishAge: 338Connection: closeX-Served-By: cache-hnd18736-HNDX-Cache: HITX-Cache-Hits: 1X-Timer: S1519579870.603620,VS0,VE0Vary: Accept-EncodingX-Fastly-Request-ID: 6603cd54ba10d110b51972d94037a3ecab5a6b17 前面使用timeout时也有用到：123456789101112131415In [107]: import socketIn [108]: import urllib.requestIn [109]: import urllib.errorIn [110]: try: ...: response = urllib.request.urlopen(&apos;https://www.baidu.com&apos;, timeout=0.01) ...: except urllib.error.URLError as e: ...: print(type(e.reason)) ...: if isinstance(e.reason, socket.timeout): ...: print(&apos;TIME OUT&apos;) ...:&lt;class &apos;socket.timeout&apos;&gt;TIME OUT 可以看到e.reason是一个&lt;class &#39;socket.timeout&#39;&gt;对象，因此可以使用isinstance判断。ps：isinstance()与type()区别： type()不会认为子类是一种父类类型，不考虑继承关系。 isinstance()会认为子类是一种父类类型，考虑继承关系。 urllib.parse模块urlparse()函数用于拆分URL：1urllib.parse.urlparse(urlstring, scheme=&apos;&apos;, allow_fragments=True) 例如：123456In [111]: from urllib.parse import urlparseIn [112]: result = urlparse(&apos;http://www.baidu.com/index.html;user?id=5#comment&apos;)In [113]: print(type(result), result)&lt;class &apos;urllib.parse.ParseResult&apos;&gt; ParseResult(scheme=&apos;http&apos;, netloc=&apos;www.baidu.com&apos;, path=&apos;/index.html&apos;, params=&apos;user&apos;, query=&apos;id=5&apos;, fragment=&apos;comment&apos;) scheme参数是协议类型：123456In [114]: from urllib.parse import urlparseIn [115]: result = urlparse(&apos;www.baidu.com/index.html;user?id=5#comment&apos;, scheme=&apos;https&apos;)In [116]: print(result)ParseResult(scheme=&apos;https&apos;, netloc=&apos;&apos;, path=&apos;www.baidu.com/index.html&apos;, params=&apos;user&apos;, query=&apos;id=5&apos;, fragment=&apos;comment&apos;) 如果URL里已经有协议类型了，那么这个参数就不会生效：123456In [117]: from urllib.parse import urlparseIn [118]: result = urlparse(&apos;http://www.baidu.com/index.html;user?id=5#comment&apos;, scheme=&apos;https&apos;)In [119]: print(result)ParseResult(scheme=&apos;http&apos;, netloc=&apos;www.baidu.com&apos;, path=&apos;/index.html&apos;, params=&apos;user&apos;, query=&apos;id=5&apos;, fragment=&apos;comment&apos;) allow_fragments参数是锚点链接的设置，为False则会把#后面的内容拼接到前面：123456In [120]: from urllib.parse import urlparseIn [121]: result = urlparse(&apos;http://www.baidu.com/index.html;user?id=5#comment&apos;, allow_fragments=False)In [122]: print(result)ParseResult(scheme=&apos;http&apos;, netloc=&apos;www.baidu.com&apos;, path=&apos;/index.html&apos;, params=&apos;user&apos;, query=&apos;id=5#comment&apos;, fragment=&apos;&apos;) 123456In [123]: from urllib.parse import urlparseIn [124]: result = urlparse(&apos;http://www.baidu.com/index.html#comment&apos;, allow_fragments=False)In [125]: print(result)ParseResult(scheme=&apos;http&apos;, netloc=&apos;www.baidu.com&apos;, path=&apos;/index.html#comment&apos;, params=&apos;&apos;, query=&apos;&apos;, fragment=&apos;&apos;) urlunparse()函数与urlparse()函数相反，用于组合URL：123456In [126]: from urllib.parse import urlunparseIn [127]: data = [&apos;http&apos;, &apos;www.baidu.com&apos;, &apos;index.html&apos;, &apos;user&apos;, &apos;a=6&apos;, &apos;comment&apos;]In [128]: print(urlunparse(data))http://www.baidu.com/index.html;user?a=6#comment urljoin()函数用于拼接URL：12345678910111213141516In [129]: from urllib.parse import urljoinIn [130]: print(urljoin(&apos;http://www.baidu.com&apos;, &apos;FAQ.html&apos;))http://www.baidu.com/FAQ.htmlIn [131]: print(urljoin(&apos;http://www.baidu.com&apos;, &apos;https://jeffyang.top/404.html&apos;))https://jeffyang.top/404.htmlIn [132]: print(urljoin(&apos;http://www.baidu.com?wd=abc&apos;, &apos;http://jeffyang.top/index.html&apos;))http://jeffyang.top/index.htmlIn [133]: print(urljoin(&apos;http://www.baidu.com&apos;, &apos;?category=2#comment&apos;))http://www.baidu.com?category=2#commentIn [134]: print(urljoin(&apos;www.baidu.com#comment&apos;, &apos;?category=2&apos;))www.baidu.com?category=2 前后的URL都可以分成六个部分，前后URL如果都有这些部分，会以后面的URL为准。 urlencode()可以把一个字典对象转换成GET请求参数：12345678910111213In [136]: from urllib.parse import urlencodeIn [137]: params = &#123; ...: &apos;name&apos;: &apos;jeff&apos;, ...: &apos;age&apos;: 21 ...: &#125;In [138]: base_url = &apos;http://www.baidu.com?&apos;In [139]: url = base_url + urlencode(params)In [140]: print(url)http://www.baidu.com?name=jeff&amp;age=21 urllib.robotparser解析robots.txt模块这个模块用于解析 robots.txt 规则，判断要爬取的 url 按照 robots.txt 文件是否合法。 这个模块平常使用的不是很多，具体的内容可以查看文档。 结语详细的说明和使用可以点击这里查看文档。]]></content>
      <categories>
        <category>Python</category>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python库Numpy使用入门]]></title>
    <url>%2FPython%2FPython%E5%BA%93Numpy%E4%BD%BF%E7%94%A8%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[本篇是慕课网Python人工智能常用库Numpy使用入门整理的学习笔记 Anaconda介绍与安装anaconda是一个开源的用于科学计算的Python发行版本，是一个包管理器、环境管理器，可以很方便地解决多版本python并存、切换以及各种第三方包安装问题。它里面预装好了conda、某个版本的python、众多packages、科学计算工具等等。 访问官网下载安装包直接安装，安装时把Add Anaconda to my PATH environment variable也勾上了。 jupyter notebook简单教程安装好anaconda之后点击开始，找到Anaconda文件夹，点击Jupyter Notebook就可以打开，这样打开的话可以看到命令行里：1234[I 23:31:32.533 NotebookApp] Writing notebook server cookie secret to C:\Users\ASUS\AppData\Roaming\jupyter\runtime\notebook_cookie_secret[I 23:31:34.383 NotebookApp] JupyterLab beta preview extension loaded from D:\ProgramFiles\Anaconda3\lib\site-packages\jupyterlab[I 23:31:34.383 NotebookApp] JupyterLab application directory is D:\ProgramFiles\Anaconda3\share\jupyter\lab[I 23:31:34.555 NotebookApp] Serving notebooks from local directory: C:\Users\ASUS 创建的文件都是放在C:\Users\ASUS目录里的。 要自行选择文件夹保存，可以cmd命令行切换到你要保存的目录下，运行jupyter notebook。 这里需要注意：如果前面安装时没有勾选Add Anaconda to my PATH environment variable就不能直接使用命令行的方式打开。 在打开的Jupyter noteb里点击右上角的New，选择Python3就可以新建一个notebook。点击Untitled可以给文件命名，然后可以看到文件夹里的文件了 可以点击选择编写代码还是markdown笔记，旁边还有一些快捷按钮。前面几个按顺序分别是：保存、新增一行输入框、删除选中行、复制、粘贴、上移、下移。此外，最后一个键盘样的按钮可以查看快捷键。 编写好代码或者markdown都可以使用alt + enter快捷键查看输出。 Numpy简单教程 - 数组以下内容导出自jupyter notebook，点击这里可以看得比较清晰。 数组Array（rank （数组的维数））： 1import numpy as np 1a = np.array([1, 2, 3]) 1a array([1, 2, 3]) 1type(a) numpy.ndarray 1a.shape # 数组的大小 (3,) 1a = a.reshape((1, -1)) # 1表示数组是1行，-1是占位符，其实代表的是3列 1a.shape (1, 3) 1a = np.array([1, 2, 3, 4, 5, 6]) 1a.shape (6,) 1a = a.reshape((2, -1)) # 这里的2代表2行，-1代表3列，因为6个元素变成2行 1a.shape (2, 3) 1a array([[1, 2, 3], [4, 5, 6]]) 1a = a.reshape((-1, 2)) # -1只是占位符，可以放在任意位置，这里-1表示3行 1a array([[1, 2], [3, 4], [5, 6]]) 1a[2, 0] 5 1a[2, 0] = 55 1a array([[ 1, 2], [ 3, 4], [55, 6]]) zeros()可以用于创建元素全为0的数组 1a = np.zeros((3, 2)) 1a array([[0., 0.], [0., 0.], [0., 0.]]) ones()可以用于创建元素全为1的数组 1a = np.ones((2, 3)) 1a array([[1., 1., 1.], [1., 1., 1.]]) full()可以用于创建元素全为指定值的数组 1a = np.full((2, 3), 3) 1a array([[3, 3, 3], [3, 3, 3]]) eye()可以用于创建指定维度的单位矩阵 1a = np.eye(3) 1a array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) random.random()可以用于创建元素值都是0-1之间的随机数组 1a = np.random.random((3, 4)) 1a array([[0.07850067, 0.26126437, 0.27118259, 0.99299099], [0.74982937, 0.3804697 , 0.14492257, 0.42120364], [0.34282385, 0.69722592, 0.06523827, 0.16707083]]) Numpy简单教程 - 索引操作indexing索引操作: 123a = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]) 1a[-2:, 1:3] # -2:表示倒数第二行到最后一行，1:3表示第二列到第三列，索引从0开始 array([[ 6, 7], [10, 11]]) 1a[1, -2] # 第二行，倒数第二列 7 1a.shape (3, 4) 1b = a[-2:, 1:3] 1b.shape # 维度变低了 (2, 2) 1b = a[2, 1:3] 1b array([10, 11]) 1b.shape (2,) 1b = a[1,2] 1b 7 1b.shape () 1a array([[ 1, 2, 3, 4], [ 5, 6, 7, 8], [ 9, 10, 11, 12]]) 1a[np.arange(3), 1] += 10 # 第二列所有元素加10 1a array([[ 1, 12, 3, 4], [ 5, 16, 7, 8], [ 9, 20, 11, 12]]) 1np.arange(3) array([0, 1, 2]) 1np.arange(3, 7) array([3, 4, 5, 6]) 1a array([[ 1, 22, 3, 4], [ 5, 26, 7, 8], [ 9, 30, 11, 12]]) 1a[np.arange(3), [1, 2, 3]] += 10 # 各行的第二三四列分别加10 1a array([[ 1, 32, 3, 4], [ 5, 26, 17, 8], [ 9, 30, 11, 22]]) 1a[[0, 1, 2], [1, 1, 1]] += 10 # 第二列所有元素加10 1a array([[ 1, 42, 3, 4], [ 5, 36, 17, 8], [ 9, 40, 11, 22]]) 1result_index = a&gt;10 # 找出元素值大于10的索引 1result_index array([[False, True, False, False], [False, True, True, False], [False, True, True, True]]) 1a[result_index] # 找出值大于10的元素 array([42, 36, 17, 40, 11, 22]) 1a[a&gt;10] # 找出值大于10的元素 array([42, 36, 17, 40, 11, 22]) Numpy简单教程 - 元素数据类型1a = np.array([1, 2]) # numpy会自动判断元素的数据类型 1a.dtype dtype(&apos;int32&apos;) 1a = np.array([1.1, 2.2]) 1a.dtype dtype(&apos;float64&apos;) 1a = np.array([1, 2.2]) # 如果既有整数又有浮点数会当成浮点数 1a.dtype dtype(&apos;float64&apos;) 1a = np.array([1.6, 2.2]) 1a = np.array([1.6, 2.2], dtype=np.int64) # 手动指定成int类型 1a # 去除小数部分，不是四舍五入！ array([1, 2], dtype=int64) 1a = np.array([1.6, 2.2]) 1b = np.array(a, dtype=np.int64) 1b array([1, 2], dtype=int64) Numpy简单教程 - 数组运算与常用函数数组运算12a = np.array([[1, 2], [3, 4]]) 12b = np.array([[5, 6], [6, 5]]) 1a+b # 对应位置的元素相加 array([[6, 8], [9, 9]]) 1np.add(a,b) # 和a+b相同 array([[6, 8], [9, 9]]) 1a-b # 对应位置元素相减 array([[-4, -4], [-3, -1]]) 1np.subtract(a, b) # 和a-b相同 array([[-4, -4], [-3, -1]]) 1a*b # 对应位置的元素相乘，并不是矩阵的乘法 array([[ 5, 12], [18, 20]]) 1np.multiply(a, b) # 和a*b相同 array([[ 5, 12], [18, 20]]) 1a/b # 对应位置的元素相除 array([[0.2 , 0.33333333], [0.5 , 0.8 ]]) 1np.divide(a, b) # 和a/b相同 array([[0.2 , 0.33333333], [0.5 , 0.8 ]]) 1np.sqrt(a) # a的每个元素开方 array([[1. , 1.41421356], [1.73205081, 2. ]]) 1a array([[1, 2], [3, 4]]) 12b = np.array([[1, 2, 3], [4, 5, 6]]) 1a.dot(b) # 矩阵乘法，a的列数要等于b的行数 array([[ 9, 12, 15], [19, 26, 33]]) 1np.dot(a, b) # 矩阵乘法 array([[ 9, 12, 15], [19, 26, 33]]) 1np.dot(b, a) # 报错 --------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-97-f65bf4b3a481&gt; in &lt;module&gt;() ----&gt; 1 np.dot(b, a) # 报错 ValueError: shapes (2,3) and (2,2) not aligned: 3 (dim 1) != 2 (dim 0) 常用函数sum 1a array([[1, 2], [3, 4]]) 1np.sum(a) # 对数组的所有元素求和 10 1np.sum(a, axis=0) # 对数组的每一列元素求和 array([4, 6]) 1np.sum(a, axis=1) # 对数组的每一行元素求和 array([3, 7]) mean 1np.mean(a) # 求a中元素的平均值 2.5 1np.mean(a, axis=0) # 求a中每一列元素的平均值， axis=0是对列进行操作 array([2., 3.]) 1np.mean(a, axis=1) # 求a中每一行元素的平均值， axis=1是对行进行操作 array([1.5, 3.5]) uniform 1np.random.uniform(3, 4) # 产生指定范围内的随机小数 3.079329791701754 1np.random.uniform(1, 100) 34.39661009456285 tile 1a array([[1, 2], [3, 4]]) 1np.tile(a, (1, 2)) # 将a当成一个元素生成一行两列的数组 array([[1, 2, 1, 2], [3, 4, 3, 4]]) 1np.tile(a, (2, 3)) # 两行三列的a数组 array([[1, 2, 1, 2, 1, 2], [3, 4, 3, 4, 3, 4], [1, 2, 1, 2, 1, 2], [3, 4, 3, 4, 3, 4]]) argsort 12a = np.array([[3, 6, 4, 11], [5, 10, 1, 3]]) 1a.argsort() # 对每一行元素从小到大排序，返回的是每个元素排序后的下标 array([[0, 2, 1, 3], [2, 3, 0, 1]], dtype=int32) 1a.argsort(axis=0) # 对每一列元素从小到大排序 array([[0, 0, 1, 1], [1, 1, 0, 0]], dtype=int32) 矩阵转置 1a array([[ 3, 6, 4, 11], [ 5, 10, 1, 3]]) 1a.T # 转置 array([[ 3, 5], [ 6, 10], [ 4, 1], [11, 3]]) 1np.transpose(a) # 和a.T相同 array([[ 3, 5], [ 6, 10], [ 4, 1], [11, 3]]) Numpy简单教程 - 广播1234a = np.array([[1, 2, 3], [2, 3, 4], [12, 31, 22], [2, 2, 2]]) 1b = np.array([1, 2, 3]) 12for i in range(4): # 数据量比较大的时候，循环会比较低效，比较慢 a[i,:] += b 1a array([[ 2, 4, 6], [ 3, 5, 7], [13, 33, 25], [ 3, 4, 5]]) 1a + np.tile(b, (4, 1)) array([[ 3, 6, 9], [ 4, 7, 10], [14, 35, 28], [ 4, 6, 8]]) 1a + b # 广播，自动将相加的数组转化为相同维度 array([[ 3, 6, 9], [ 4, 7, 10], [14, 35, 28], [ 4, 6, 8]]) 上面的内容都是从jupyter notebook导出的，点击这里可以看得比较清晰。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python操作MySql数据库]]></title>
    <url>%2FPython%2FPython%E6%93%8D%E4%BD%9CMySql%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[本篇是慕课网课程Python操作MySql数据库的笔记 Python DB APIPython DB API：Python访问数据库的统一接口规范 包括的内容： 数据库连接对象connection 数据库交互对象cursor 数据库异常类exceptions 使用Python DB API访问数据库流程： 开始 –&gt; 创建connection –&gt; 获取cursor –&gt; 执行查询、执行命令、获取数据、处理数据 –&gt; 关闭cursor –&gt; 关闭connection –&gt; 结束 Python MySQL开发环境 教程里是Python2.7，而我的是Python3.6 MySQL，我的是5.7 教程里开发工具是Eclipse+Pydev，我用的是Pycharm PyMySQL（Python2.x里是MySQLdb）可以通过pip安装：1pip install PyMySQL 因为Python统一了数据库连接的接口，所以pymysql和MySQLdb的使用方式是类似的 数据库连接对象connection连接对象connection：建立Python客户端与数据库的网络连接 创建方法：pymysql.Connect(参数) 参数名 类型 说明 host 字符串 MySQL服务器地址 post 数字 MySQL服务器端口号 user 字符串 用户名 passwd 字符串 密码 db 字符串 数据库名称 charset 字符串 连接编码 connection对象支持的方法： 方法名 说明 cursor() 使用该连接创建并返回游标 commit() 提交当前事务 rollback() 回滚当前事务 close() 关闭连接 数据库的游标对象cursor游标对象cursor：用于执行查询和获取结果 cursor对象支持的方法： 方法名 说明 execute(op[,args]) 执行一个数据库查询和命令(CRUD) fetchone() 取得结果集的下一行 fetchmany(size) 获取结果集的下几行 fetchall() 获取结果集中剩下的所有行（没有被遍历的所有行） rowcount 最近一次execute返回数据的行数或影响行数（不是方法） close() 关闭游标对象 execute方法：执行SQL、将结果从数据库获取到客户端 fetch*方法：移动rownumber（它相当于一个指针），返回数据 select查询数据过程： 开始 –&gt; 创建connection –&gt; 获取cursor –&gt; 使用cursor.execute()执行SELECT语句 –&gt; 使用cursor.fetch*()获取并处理数据 –&gt; 关闭cursor –&gt; 关闭connection –&gt; 结束 例：1234567891011121314151617181920conn = pymysql.Connect( host=&apos;localhost&apos;, port=3306, user=&apos;root&apos;, passwd=&apos;123456&apos;, db=&apos;pymysql&apos;, charset=&apos;utf8&apos; # 注意不是utf-8!)cursor = conn.cursor()sql = &quot;SELECT * FROM user&quot;cursor.execute(sql)rs = cursor.fetchall()for row in rs: print(&quot;userid = %s, username = %s&quot; % row)cursor.close()conn.close() Insert/Update/Delete数据过程： 开始 –&gt; 创建connection –&gt; 获取cursor –&gt; 使用cursor.execute()执行INSERT/UPDATE/DELETE语句 –&gt; 出现异常？（否：使用conn.commit()提交事务/是：使用conn.rollback()回滚事务） –&gt; 关闭cursor –&gt; 关闭connection –&gt; 结束 这里就要使用try...except...语句判断是否出现异常 事务：访问和更新数据库的一个程序执行单元 原子性：事务中包括的诸操作要么都做，要么都不做 一致性：事务必须使数据库从一致性状态变到另一个一致性状态 隔离性：一个事务的执行不能被其他事务干扰 持久性：食物一旦提交，它对数据库的改变就是永久性的 注意：MySQL引擎不能设置为MyISAM，因为它不支持事务 开发中怎样使用事务？ 关闭自动commit：设置conn.autocommit(FALSE) 正常结束事务：conn.commit() 异常结束事务：conn.rollback() 12345678910111213141516sql_insert = &quot;INSERT INTO user (userid, username) VALUES (10, &apos;name10&apos;)&quot;sql_update = &quot;UPDATE user SET username=&apos;name99&apos; WHERE userid=9&quot;sql_delete = &quot;DELETE FROM user WHERE userid&lt;3&quot;try: cursor.execute(sql_insert) print(cursor.rowcount) cursor.execute(sql_update) print(cursor.rowcount) cursor.execute(sql_delete) print(cursor.rowcount) conn.commit() # 上面三条语句就是一个事务，成功执行就可以commitexcept Exception as e: print(e) conn.rollback() # 出现异常就rollback回滚 银行转账实例（账户A给账户B转账100元）过程： 开始事务 –&gt; 检查账户A和账户B是否可用 –&gt; 检查账户A是否有100元 –&gt; 账户A减去100元，账户B加上100元 –&gt; 出现异常则回滚事务，否则提交事务 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091import sysimport pymysqlclass TransferMoney(): def __init__(self, conn): self.conn = conn def transfer(self, source_acctid, target_acctid, money): try: self.check_acct_available(source_acctid) self.check_acct_available(target_acctid) self.has_enough_money(source_acctid, money) self.reduce_money(source_acctid, money) self.add_money(target_acctid, money) self.conn.commit() except Exception as e: self.conn.rollback() raise e def check_acct_available(self, acctid): cursor = self.conn.cursor() try: sql = &quot;SELECT * FROM account WHERE acctid=%s&quot; % acctid cursor.execute(sql) print(&quot;check_acct_available: &quot; + sql) rs = cursor.fetchall() if len(rs) != 1: raise Exception(&quot;账号%s不存在&quot; % acctid) finally: cursor.close() def has_enough_money(self, acctid, money): cursor = self.conn.cursor() try: sql = &quot;SELECT * FROM account WHERE acctid=%s AND money&gt;%s&quot; % (acctid, money) cursor.execute(sql) print(&quot;has_enough_money: &quot; + sql) rs = cursor.fetchall() if len(rs) != 1: raise Exception(&quot;账号%s没有足够的金额&quot; % acctid) finally: cursor.close() def reduce_money(self, acctid, money): cursor = self.conn.cursor() try: sql = &quot;UPDATE account SET money=money-%s WHERE acctid=%s&quot; % (money, acctid) cursor.execute(sql) print(&quot;reduce_money: &quot; + sql) rs = cursor.fetchall() if cursor.rowcount != 1: # SQL语句影响的行数不是1 raise Exception(&quot;账号%s减款失败&quot; % acctid) finally: cursor.close() def add_money(self, acctid, money): cursor = self.conn.cursor() try: sql = &quot;UPDATE account SET money=money+%s WHERE acctid=%s&quot; % (money, acctid) cursor.execute(sql) print(&quot;add_money: &quot; + sql) rs = cursor.fetchall() if cursor.rowcount != 1: # SQL语句影响的行数不是1 raise Exception(&quot;账号%s加款失败&quot; % acctid) finally: cursor.close()if __name__ == &quot;__main__&quot;: source_acctid = sys.argv[1] # 这些参数在run configuration里找到parameters设置 target_acctid = sys.argv[2] money = sys.argv[3] conn = pymysql.Connect( host=&apos;localhost&apos;, port=3306, user=&apos;root&apos;, passwd=&apos;123456&apos;, db=&apos;pymysql&apos;, charset=&apos;utf8&apos; # 注意不是utf-8! ) tr_money = TransferMoney(conn) try: tr_money.transfer(source_acctid, target_acctid, money) except Exception as e: print(&quot;出现异常：&quot; + str(e)) finally: conn.close()]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>MySql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫基本原理]]></title>
    <url>%2FPython%2F%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[爬虫爬虫就是一个请求网站并提取指定数据的程序，相当于代替手动操作浏览器访问网址并获取需要信息的过程。 上网基本流程浏览器输入网址 –&gt; 回车（向目标URL发送一个HTTP请求） –&gt; 看到网页（服务器响应请求，把代码返回给浏览器解析成看到的页面） 本质是网络通信，即通过网络进行数据传递 浏览器经过通信后获取到该页面的源代码文档（HTML等） 浏览器解析文档后以适当的形式展现给用户 爬虫基本流程前面说爬虫相当于代替手动操作浏览器访问网址，所以，爬虫的过程也相当于一个上网的过程。 发送请求通过HTTP向目标站点发送一个Request请求，请求可以包含额外的headers等信息。 获取响应内容服务器正常响应后，会得到一个Response，响应包括目标页面的内容，如HTML文档、Json字符串或二进制数据（如图片、视频）等 处理内容可以通过正则表达式或其他网页解析库进行网页解析，然后提取出自己需要的内容进行保存，如保存到Excel、数据库等。 Request浏览器访问一个网址，就是向服务器发送Request的过程。 Request包含： 请求方法及URL：主要有GET和POST 请求头：包含许多有关的客户端环境和请求正文的有用信息，如User-Agent和cookies 请求体：请求携带的额外信息，如POST请求发送的表单数据 Response服务器接收Request通过后台代码进行处理，生成相应Response返回给浏览器。 Response包含： 响应状态：状态码及相应的状态消息。200表示成功，其他的还有404、500等 响应头：包含一些服务器信息，内容类型等许多内容。 响应体：包含了请求的资源，如HTML文档、css和其他二进制数据等 实例访问百度1234567891011121314151617181920In [1]: import requestsIn [2]: headers = &#123;&apos;User-Agent&apos;:&apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrom ...: e/63.0.3239.132 Safari/537.36&apos;&#125;In [3]: response = requests.get(&apos;http://baidu.com&apos;, headers = headers)In [4]: print(response.status_code)200In [5]: print(response.headers)&#123;&apos;Bdpagetype&apos;: &apos;1&apos;, &apos;Bdqid&apos;: &apos;0xa246c16500007bf8&apos;, &apos;Bduserid&apos;: &apos;0&apos;, &apos;Cache-Control&apos;: &apos;private&apos;, &apos;Connection&apos;: &apos;Keep-Alive&apos;, &apos;Content-Encoding&apos;: &apos;gzip&apos;, &apos;Content-Type&apos;: &apos;text/html; charset=utf-8&apos;, &apos;Cxy_all&apos;: &apos;baidu+f8cc963045aca76a5e616434e2aa01cb&apos;, &apos;Date&apos;: &apos;Sun, 25 Feb 2018 08:11:54 GMT&apos;, &apos;Expires&apos;: &apos;Sun, 25 Feb 2018 08:11:47 GMT&apos;, &apos;P3p&apos;: &apos;CP=&quot; OTI DSP COR IVA OUR IND COM &quot;&apos;, &apos;Server&apos;: &apos;BWS/1.1&apos;, &apos;Set-Cookie&apos;: &apos;BAIDUID=3F56499DC023D4234F4419B30E63B960:FG=1; expires=Thu, 31-Dec-37 23:55:55 GMT; max-age=2147483647; path=/; domain=.baidu.com, BIDUPSID=3F56499DC023D4234F4419B30E63B960; expires=Thu, 31-Dec-37 23:55:55 GMT; max-age=2147483647; path=/; domain=.baidu.com, PSTM=1519546314; expires=Thu, 31-Dec-37 23:55:55 GMT; max-age=2147483647; path=/; domain=.baidu.com, BDSVRTM=0; path=/, BD_HOME=0; path=/, H_PS_PSSID=25640_1441_21114_17001_22158; path=/; domain=.baidu.com&apos;, &apos;Strict-Transport-Security&apos;: &apos;max-age=172800&apos;, &apos;Vary&apos;: &apos;Accept-Encoding&apos;, &apos;X-Powered-By&apos;: &apos;HPHP&apos;, &apos;X-Ua-Compatible&apos;: &apos;IE=Edge,chrome=1&apos;, &apos;Transfer-Encoding&apos;: &apos;chunked&apos;&#125;In [6]: print(response.text)&lt;!DOCTYPE html&gt;&lt;!--STATUS OK--&gt;......&lt;/body&gt;&lt;/html&gt; requests.get()就是发送Request请求并把获取到服务器返回的Response赋值给response，接下来就可以对得到的请求进行操作了： response.status_code打印出状态码。 response.headers打印出响应头。 response.text打印出源码。 获取其他类型数据除了文本数据，还可以获取到图片：1234567891011In [7]: response = requests.get(&apos;https://www.bing.com/az/hprichbg/rb/WoolBaySeadragon_ZH-CN13348117046_1920x1080.jpg&apos;)In [8]: print(response.content)b&apos;\xff\xd8\xff\xe0\x00\x10JFIF\x00\x01\x01\x01\x00\x00\x00\x00\x00\x00\xff\xdb\x00C\x00\x06\x04\x04\x04\x04\x04\x06\x04\x04\x06\x08\x05\x05\x05\x08\n\x07\x06\x06\x07\n\x0b\t\t\n\t\t\x0b\x0e\x0b\x0c\x0c\x0c\x0c\x0b\x0e\x0c\r\r\x0e\r\r\x0c\x11\x11\x12\x12\x11\x11\x19\x18\x18\x18\x19\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\xff\xdb\x00C\x01\x06\x06\x06\x0b\n\x0b\x15\x0e\x0e\x15\x17\x13\x10\x13\x17\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c......In [9]: with open(&apos;E:/Notes/img.jpg&apos;, &apos;wb&apos;) as f: ...: f.write(response.content) ...:In [10]: 可以看到response.content就是图片的二进制流数据，可以通过操作文件的方法保存图片。 进阶有时候，我们通过上面的方法获取的内容和我们在浏览器看到的内容是不一样的，这是因为我们看到的内容是js渲染过以后的内容，是通过Ajax获取后台数据在前端显示的。 这时候常用的方法就是： 分析Ajax请求。可以借助浏览器开发者工具或者Fiddler等抓包工具。 使用selenium：12345678910111213In [7]: import seleniumIn [8]: from selenium import webdriverIn [10]: driver = webdriver.Chrome()DevTools listening on ws://127.0.0.1:12591/devtools/browser/518f43b6-3307-43df-83da-547e126a7d65In [11]: driver.get(&quot;http://www.baidu.com&quot;)In [12]: driver.page_sourceOut[12]: &apos;&lt;!DOCTYPE html&gt;&lt;!--STATUS OK--&gt;&lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot;&gt;&lt;head&gt;\n \n &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html;charset=utf-8&quot; /&gt;\n &lt;meta http-equiv=&quot;X...... 这时候的源码就是渲染之后的了。]]></content>
      <categories>
        <category>Python</category>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫常用工具]]></title>
    <url>%2FPython%2F%E7%88%AC%E8%99%AB%2FPython%E7%88%AC%E8%99%AB%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[下面是Python爬虫常用库和常用工具的简单介绍。 rePython自带，正则表达式模块，import不出错则可以正常使用：123In [1]: import reIn [2]: urllibPython自带，提供一系列用于操作URL的功能。123456In [2]: import urllibIn [3]: import urllib.requestIn [4]: urllib.request.urlopen(&quot;http://www.baidu.com&quot;)Out[4]: &lt;http.client.HTTPResponse at 0x6e8abd0&gt; requests用于请求的库。安装：pip install requests1234In [5]: import requestsIn [6]: requests.get(&quot;http://www.baidu.com&quot;)Out[6]: &lt;Response [200]&gt; 另外，使用 pip show xxx可以查看模块信息：12345678910C:\Users\ASUS&gt;pip show requestsName: requestsVersion: 2.18.4Summary: Python HTTP for Humans.Home-page: http://python-requests.orgAuthor: Kenneth ReitzAuthor-email: me@kennethreitz.orgLicense: Apache 2.0Location: d:\program files\python3.6.1\lib\site-packagesRequires: chardet, certifi, idna, urllib3 selenium驱动浏览器的库，多用于自动化测试。爬虫时遇到js渲染的话用requests就无法获取请求内容，这时就可以使用这个库。（新一点的版本似乎说不再支持下面要说的phantomjs了，所以这里可以指定版本，我的是3.3.1）安装：pip install selenium==3.3.1123In [7]: import seleniumIn [8]: from selenium import webdriver 但是使用Chrome时，会报错：123456In [9]: driver = webdriver.Chrome()---------------------------------------------------------------------------FileNotFoundError Traceback (most recent call last)......WebDriverException: Message: &apos;chromedriver&apos; executable needs to be in PATH. Please see https://sites.google.com/a/chromium.org/chromedriver/home 这里需要下载一个chromedriver.exe，官网需要科学上网，可以点击这里下载。注意要下载支持你Chrome版本的driver。 下载之后把解压完的chromedriver.exe放到配置了环境变量的目录下，比如Scripts。 之后就可以使用了：123In [10]: driver = webdriver.Chrome()DevTools listening on ws://127.0.0.1:12591/devtools/browser/518f43b6-3307-43df-83da-547e126a7d65 也可以使用指定路径的写法：1driver = webdriver.Chrome(&quot;D:\Program Files\Python3.6.1\Scripts\chromedriver.exe&quot;) 执行完会打开一个Chrome窗口。然后执行：1In [14]: driver.get(&quot;http://www.baidu.com&quot;) 这个Chrome浏览器就会访问该网址。 使用driver.page_source就会显示源码：1234In [17]: driver.page_sourceOut[17]: &apos;&lt;!DOCTYPE html&gt;&lt;!--STATUS OK--&gt;&lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot;&gt;&lt;head&gt;\n \n &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html;charset=utf-8&quot; /&gt;\n &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=Edge&quot; /&gt;\n\t&lt;meta content=&quot;always&quot; name=&quot;referrer&quot; /&gt;\n &lt;meta name=&quot;theme-color&quot; content=&quot;#2932e1&quot; /&gt;\n &lt;link rel=&quot;shortcut icon&quot; href=&quot;/favicon.ico&quot;...... phantomjs这是一个无界面浏览器，也就是不会弹出浏览器窗口。点击这里可以下载。 下载完成后解压到你想要的目录，然后把bin的路径配置到环境变量Path里。 打开命令行运行phantomjs可以执行js代码：12345C:\Users\ASUS&gt;phantomjsphantomjs&gt; console.log(&quot;Hello,World!&quot;)Hello,World!undefinedphantomjs&gt; 然后我们就可以使用了：12345678910In [1]: from selenium import webdriverIn [2]: driver = webdriver.PhantomJS()In [3]: driver.get(&quot;http://www.baidu.com&quot;)In [4]: driver.page_sourceOut[4]: &apos;&lt;!DOCTYPE html&gt;&lt;!--STATUS OK--&gt;&lt;html&gt;&lt;head&gt;\n \n &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html;charset=utf-8&quot;&gt;\n &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=Edge&quot;&gt;\n\t&lt;meta content=&quot;always&quot; name=&quot;referrer&quot;&gt;\n &lt;meta name=&quot;theme-color&quot; content=&quot;...... lxml提供xpath用于解析网页。安装：pip install lxml使用pip安装时，可能会网络等因素出现一些问题。也可以访问pypi下载.whl文件再用pip命令安装。不过你要先pip install wheel安装wheel。123In [5]: import lxmlIn [6]: beautifulsoup也是一个解析网页的库，它是依赖于lxml的，所以要先安装lxml库。安装：pip install beautifulsoup412345In [5]: import lxmlIn [6]: from bs4 import BeautifulSoupIn [7]: soup = BeautifulSoup(&apos;&lt;html&gt;&lt;/html&gt;&apos;, &apos;lxml&apos;) 然后就可以使用这个soup对象了。 pyquery也是一个网页解析库，语法和jquery相同。安装：pip install pyquery12345678In [8]: from pyquery import PyQuery as pqIn [9]: doc = pq(&apos;&lt;html&gt;Hello,World!&lt;/html&gt;&apos;)In [10]: result = doc(&apos;html&apos;).text()In [11]: resultOut[11]: &apos;Hello,World!&apos; pymysql用于操作MySQL数据库，Python2.x里是MySQLdb。安装：pip install pymysql12345678910111213141516171819In [2]: import pymysqlIn [3]: conn = pymysql.Connect(host=&apos;localhost&apos;, user=&apos;root&apos;, password=&apos;123456&apos;, port=3306, db=&apos;pymysql&apos;)In [4]: cursor = conn.cursor()In [5]: cursor.execute(&apos;SELECT * FROM user&apos;)Out[5]: 8In [6]: cursor.fetchall()Out[6]:((3, &apos;name3&apos;), (4, &apos;name4&apos;), (5, &apos;name5&apos;), (6, &apos;name6&apos;), (7, &apos;name7&apos;), (8, &apos;name8&apos;), (9, &apos;name99&apos;), (10, &apos;name10&apos;)) pymongo用于操作MongoDB数据库。安装：pip install pymongo1234567891011In [11]: import pymongoIn [12]: client = pymongo.MongoClient(&apos;localhost&apos;)In [13]: db = client[&apos;test&apos;]In [14]: db[&apos;table&apos;].insert_one(&#123;&apos;name&apos;:&apos;Jeff&apos;&#125;)Out[14]: &lt;pymongo.results.InsertOneResult at 0x7309828&gt;In [15]: db[&apos;table&apos;].find_one(&#123;&apos;name&apos;:&apos;Jeff&apos;&#125;)Out[15]: &#123;&apos;_id&apos;: ObjectId(&apos;5a92595d3c7e6c30847793dc&apos;), &apos;name&apos;: &apos;Jeff&apos;&#125; redis也是存储key - value的非关系型数据库，主要用于分布式爬虫。安装：pip install redis123456789In [18]: import redisIn [19]: r = redis.Redis(&apos;localhost&apos;, 6379)In [20]: r.set(&apos;name&apos;, &apos;Jeff&apos;)Out[20]: TrueIn [21]: r.get(&apos;name&apos;)Out[21]: b&apos;Jeff&apos; jupyter功能强大的notebook，可以记markdown，可以调试代码。安装：pip install jupyter cmd命令行执行jupyter notebook就可以在浏览器使用。 也可以使用pip同时安装多个库：1pip install requests selenium lxml beautifulsoup4 pyquery pymysql pymongo redis jupyter]]></content>
      <categories>
        <category>Python</category>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python基础语法篇（1）]]></title>
    <url>%2FPython%2FPython%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%E7%AF%87%EF%BC%881%EF%BC%89%2F</url>
    <content type="text"><![CDATA[简介Python是吉多·范罗苏姆在1989年的圣诞节创立的。经过二十多年的完善，已经成为了一个优秀并广泛使用的语言。 Python可以应用于数据分析、组件集成、网络服务、图像处理、数值计算和科学计算等众多领域。 诸如Youtube、Dropbox、Quora、豆瓣、知乎、Google、Yahoo!、Facebook、百度、腾讯等公司都有使用Python。 编译型&amp;解释型Python是解释型语言，程序不需要编译，在运行程序的时候才翻译成机器语言，每个语句都是执行的时候才翻译。这样解释性语言每执行一次就需要逐行翻译一次，效率比较低。 而编译型语言在程序执行之前，有一个单独的编译过程，将程序翻译成机器语言，以后执行这个程序的时候，就不用再进行翻译了。典型的有C、C++。 编译型语言就相当于做好一桌子菜再开吃，而编译型语言就相当于吃火锅… 我们还能从Python项目里看到__pycache__文件夹和里面的.pyc文件。在文档里是这么说的： To speed up loading modules, Python caches the compiled version of each module in the pycache directory under the name module.version.pyc, where the version encodes the format of the compiled file; it generally contains the Python version number. 也就是说import别的.py文件时，那个.py文件会被存一份.pyc提升下次加载的速度。注意并不是运行的速度。 例如： 新建文件a.py：123456class Greet(): def __init__(self, name): self.name = name def greet(self): print(&quot;Hello, &quot; + self.name + &quot;!&quot;) 新建文件b.py：1234from a import GreetA = Greet(&quot;Jack&quot;)A.greet() 这时候，命令行运行&gt;python b.py，当前目录下就会生成一个__pycache__目录，里面有文件a.cpython-36.pyc 静态语言&amp;动态语言Python是一门动态语言，动态类型语言是在运行期间才去做数据类型检查的语言，也就是说，在用动态类型的语言编程时，不用给变量指定数据类型。此外，Ruby、JavaScript也是动态语言。 静态语言则是在编译期间做类型判断，如C、C++等。 强类型定义语言&amp;弱类型定义语言强类型定义语言：强制数据类型定义的语言。也就是说，一旦一个变量被指定了某个数据类型，如果不经过强制转换，那么它就永远是这个数据类型了。举个例子：如果你定义了一个整型变量a,那么程序根本不可能将a当作字符串类型处理。强类型定义语言是类型安全的语言。 弱类型定义语言：数据类型可以被忽略的语言。它与强类型定义语言相反, 一个变量可以赋不同数据类型的值。 Python是动态语言，也是强类型定义语言（类型安全的语言）；VBScript是动态语言，是弱类型定义语言（类型不安全的语言）；JAVA是静态语言，是强类型定义语言（类型安全的语言）。 Python2.x&amp;Python3.x官方有一句话： In summary : Python 2.x is legacy, Python 3.x is the present and future of the language 2和3详细的区别这里就不说了，最大的改动就是Python3.x里所有文本字符串默认为Unicode编码，意味着可以直接写中文，开头不需要# -*- coding:utf-8 -*-了。 Hello World!Windows下在官网下载安装程序直接安装，安装时勾上Add Python 3.6 to PATH。 安装好后在命令行输入python输入:12&gt;&gt;&gt; print(&apos;hello, world&apos;)hello, world Linux下想类似于执行shell脚本一样执行python脚本，例：./hello.py要在第一行加上指定解释器，env指里寻找python环境变量：1#!/usr/bin/env python 记得执行前需给予hello.py执行权限，chmod 755 hello.py 变量 变量名只能是 字母、数字或下划线的任意组合 变量名的第一个字符不能是数字 以下关键字不能声明为变量名：[&#39;and&#39;, &#39;as&#39;, &#39;assert&#39;, &#39;break&#39;, &#39;class&#39;, &#39;continue&#39;, &#39;def&#39;, &#39;del&#39;, &#39;elif&#39;, &#39;else&#39;, &#39;except&#39;, &#39;exec&#39;, &#39;finally&#39;, &#39;for&#39;, &#39;from&#39;, &#39;global&#39;, &#39;if&#39;, &#39;import&#39;, &#39;in&#39;, &#39;is&#39;, &#39;lambda&#39;, &#39;not&#39;, &#39;or&#39;, &#39;pass&#39;, &#39;print&#39;, &#39;raise&#39;, &#39;return&#39;, &#39;try&#39;, &#39;while&#39;, &#39;with&#39;, &#39;yield&#39;] 1234567name = &quot;Jack&quot;name2 = name # name2不是指向name而是指向name的值&quot;Jack&quot;print(&quot;My name is &quot;, name) # My name is Jackprint(name, name2) # Jack Jackname = &quot;Tom&quot; # 只有name变了，name2不会变print(name, name2) # Tom Jack 字符编码ASCII（American Standard Code for Information Interchange，美国标准信息交换代码）是基于拉丁字母的一套电脑编码系统，主要用于显示现代英语和其他西欧语言，其最多只能用 8 位来表示（一个字节），即：2**8 = 256-1，所以，ASCII码最多只能表示 255 个符号。 为了处理中文，程序员设计了用于简体中文的GB2312和用于繁体中文的big5。GB2312(1980年)一共收录了7445个字符，包括6763个汉字和682个其它符号。汉字区的内码范围高字节从B0-F7，低字节从A1-FE，占用的码位是72*94=6768。其中有5个空位是D7FA-D7FE。 1995年的汉字扩展规范GBK1.0收录了21886个符号，它分为汉字区和图形符号区。汉字区包括21003个字符。2000年的 GB18030是取代GBK1.0的正式国家标准。该标准收录了27484个汉字，同时还收录了藏文、蒙文、维吾尔文等主要的少数民族文字。现在的PC平台必须支持GB18030，对嵌入式产品暂不作要求。所以手机、MP3一般只支持GB2312。 显然ASCII码无法将世界上的各种文字和符号全部表示，所以，就需要新出一种可以代表所有字符和符号的编码，即：Unicode（统一码、万国码、单一码）是一种在计算机上使用的字符编码。它为每种语言中的每个字符设定了统一并且唯一的二进制编码，规定虽有的字符和符号最少由 16 位来表示（2个字节），即：2 **16 = 65536。 UTF-8，是对Unicode编码的压缩和优化，他不再使用最少使用2个字节，而是将所有的字符和符号进行分类：ascii码中的内容用1个字节保存、欧洲的字符用2个字节保存，东亚的字符用3个字节保存。 用户输入123# name = raw_input(&quot;What is your name?&quot;) # python 2.xname = input(&quot;What is your name?&quot;)print(&quot;Hello &quot; + name ) python2中用raw_input()，它接收输入转换成string返回，输入数字也会当成string python2中input()原理上是raw_input()调用eval()函数，输入字符串要用引号引起 python3中raw_input()和input()进行了整合,仅保留了input()函数，其接收任意输入并返回字符串类型。 eval()：把字符串当成有效的python表达式求值并计算结果 repr()：把变量和表达式转换成字符串表示]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自己写一个Chrome插件]]></title>
    <url>%2FChrome%2F%E8%87%AA%E5%B7%B1%E5%86%99%E4%B8%80%E4%B8%AAChrome%E6%8F%92%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[这篇文章也是之前在公众号上发布过的，现在整理一下发上来，如果这里分析的廖雪峰老师的网站结构有改变，代码可能也要重新分析编写。 另外，很多时候，使用Tampermonkey写用户脚本会更加方便，而且也可以使用别的许多大牛写的脚本，也推荐大家使用。我这篇文章只是写一下制作插件的过程。 前言前段时间在看廖雪峰老师网站的JavaScript教程时，发现有很多图片广告，看得我很是不舒服。于是决定写一个Chrome插件屏蔽一下。并不是说挂广告有什么不对，只是单纯地想折腾一下…… 所以这里就记录下我是怎么做的。 准备工作首先当然是F12看源码了，我们可以找到图片广告的div。可以看到两个div的class都有uk-clearfix,我们用hide()方法就可以这些图片隐藏了。 编写js代码以下代码就可以把图片去掉了：1$(&apos;.uk-clearfix&apos;).hide(); 然后我就想了，既然要去广告，那就再去得干净一点吧，左侧还有一个关于作者的廖雪峰老师的微博…… 于是找到class为x-sidebar-left-bottom：1$(&apos;.x-sidebar-left-bottom&apos;).hide(); 还有下面的分享部分，可以看到这些是分开的。 这里我是通过找到他们的前一个元素来去定位的：1$(&apos;.x-wiki-content&apos;).next().next().andSelf().next().andSelf().next().andSelf().hide(); 之后评论这一部分： 本来我想直接去除，但是觉得有些评论还是挺有用的，所以就做成了可以通过点击评论让评论列表隐藏或显示，还让鼠标移到“评论”上就变成手指哈哈：12345$(&apos;.x-anchor&apos;).next().mousemove(function()&#123; $(this).css(&quot;cursor&quot;,&quot;pointer&quot;);&#125;).on(&apos;click&apos;, function () &#123; $(&apos;#x-comment-list&apos;).toggle();&#125;); 还有一个提示登录评论的我就直接去掉了：1$(&apos;#x-comment-list&apos;).next().next().andSelf().hide(); 制作插件这里先准备以下文件： icon就是安装后显示的图标，一个jQuery的js文件，另一个js就是刚才自己编写的js脚本：1234567891011window.onload = function () &#123; $(&apos;.uk-clearfix&apos;).hide(); $(&apos;.x-anchor&apos;).next().mousemove(function()&#123; $(this).css(&quot;cursor&quot;,&quot;pointer&quot;); &#125;).on(&apos;click&apos;, function () &#123; $(&apos;#x-comment-list&apos;).toggle(); &#125;); $(&apos;.x-sidebar-left-bottom&apos;).hide(); $(&apos;.x-wiki-content&apos;).next().next().andSelf().next().andSelf().next().andSelf().hide(); $(&apos;#x-comment-list&apos;).next().next().andSelf().hide();&#125; manifest.json就相当于这个插件的配置文件了：1234567891011121314151617181920&#123; &quot;manifest_version&quot;: 2, &quot;name&quot;: &quot;LiaoXueFeng&quot;, &quot;description&quot;: &quot;Don&apos;t tell others!&quot;, &quot;version&quot;: &quot;1.0&quot;, &quot;permissions&quot;: [ &quot;https://www.liaoxuefeng.com/*&quot; ], &quot;browser_action&quot;: &#123; &quot;default_icon&quot;: &quot;icon.png&quot;, &quot;default_title&quot;: &quot;Remove somthing!&quot; &#125;, &quot;content_scripts&quot;: [ &#123; &quot;matches&quot;: [&quot;https://www.liaoxuefeng.com/*&quot;], &quot;js&quot;: [&quot;jquery-1.10.2.min.js&quot;,&quot;Liaoxuefeng.js&quot;], &quot;run_at&quot;: &quot;document_start&quot; &#125; ]&#125; 接下来就是打开Chrome的扩展程序界面，点击打包扩展程序这里选了根目录就行，密钥文件是用来更新扩展程序用的： 之后生成了crx文件，就完成了！pem文件就是刚才说的密钥文件了。 然后就可以安装使用了，以后打开这个网页就变得清爽多了：]]></content>
      <categories>
        <category>Chrome</category>
      </categories>
      <tags>
        <tag>Chrome</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python获取Bing图片做壁纸]]></title>
    <url>%2FPython%2Fpython%E8%8E%B7%E5%8F%96Bing%E5%9B%BE%E7%89%87%E5%81%9A%E5%A3%81%E7%BA%B8%2F</url>
    <content type="text"><![CDATA[这篇东西是之前发在微信公众号上的文章，现在整理一下发出来。 前言用过Bing搜索的都知道，它首页每天都会更新一张图片，点击这里可以访问。 先说说结果吧，写了一个有GUI界面的小工具，可以查看和保存壁纸，有兴趣的可以在公众号回复“bing”下载玩玩。也可以点击我的GitHub下载Main.exe试试。 需要注意的一些问题： 工具需要联网才能打开 暂时不支持更改保存的文件夹 设为壁纸之后注销或重启之后就无效了（用保存的图片手动设置壁纸的话就没问题。我在win10用的时候是这样，其他系统不知道，可能是用的pywin32这个库的问题。） 好了，不想看分析和代码的看到这里就好了。 准备工作（python 3.x环境） 获取网页信息的requests，urllib库 设置壁纸的pywin32，PIL库 编写GUI的pyqt5库 分析网页和以前一样，第一步就是打开网页开发者工具，发现在代码里是找不到图片的网址的，这时候就要看networks了：我们可以在XHR里发现下面这个请求：1https://www.bing.com/HPImageArchive.aspx?format=js&amp;idx=0&amp;n=1&amp;nc=1510118759864&amp;pid=hp 请求的结果格式化之后是这样的：123456789101112131415161718192021222324252627&#123; &quot;images&quot;: [ &#123; &quot;startdate&quot;: &quot;20171107&quot;, &quot;fullstartdate&quot;: &quot;201711071600&quot;, &quot;enddate&quot;: &quot;20171108&quot;, &quot;url&quot;: &quot;/az/hprichbg/rb/PointArenaLH_ZH-CN12332642727_1920x1080.jpg&quot;, &quot;urlbase&quot;: &quot;/az/hprichbg/rb/PointArenaLH_ZH-CN12332642727&quot;, &quot;copyright&quot;: &quot;波因特阿里纳灯塔，美国加利福尼亚州 (© plainpicture/Westend61/Spotcatch)&quot;, &quot;copyrightlink&quot;: &quot;/search?q=Point+Arena+Ligh&amp;form=hpcapt&amp;mkt=zh-cn&quot;, &quot;quiz&quot;: &quot;/search?q=Bing+homepage+quiz&amp;filters=WQOskey:%22HPQuiz_20171107_PointArenaLH%22&amp;FORM=HPQUIZ&quot;, &quot;wp&quot;: true, &quot;hsh&quot;: &quot;ea5b137418244ac63db363f7276c95bb&quot;, &quot;drk&quot;: 1, &quot;top&quot;: 1, &quot;bot&quot;: 1, &quot;hs&quot;: [ ] &#125; ], &quot;tooltips&quot;: &#123; &quot;loading&quot;: &quot;正在加载...&quot;, &quot;previous&quot;: &quot;上一个图像&quot;, &quot;next&quot;: &quot;下一个图像&quot;, &quot;walle&quot;: &quot;此图片不能下载用作壁纸。&quot;, &quot;walls&quot;: &quot;下载今日美图。仅限用作桌面壁纸。&quot; &#125;&#125; 没错，“url”加上域名就是图片的地址了，“enddate”的值就是当天日期，可以作为保存的文件名。另外，请求的链接里有个“idx”参数，0是今天的图片，1是昨天的，以此类推。知道这个，接下来的事就很简单了。 编写代码首先先写一个获取url的函数，因为要让程序可以获取到前些天的图片，参数的默认值为0，也就是今天的图片：123def get_url(day=0): url = &quot;https://www.bing.com/HPImageArchive.aspx?format=js&amp;idx=&quot; + str(day) + &quot;&amp;n=1&amp;nc=1509675905008&amp;pid=hp&amp;video=1&quot; return url 然后，写一个保存图片的函数，用“enddate”作为文件名：12345678def get_img(url, path=&quot;D://wallpaper/&quot;): isExists = os.path.exists(path) if not isExists: os.makedirs(path) html = requests.get(url) content = html.json() src = &quot;https://www.bing.com&quot; + content[&apos;images&apos;][0][&apos;url&apos;] urlretrieve(src, path + content[&apos;images&apos;][0][&apos;enddate&apos;] + &apos;.jpg&apos;) 设置壁纸的方法来自网上，原理是通过pywin32访问注册表修改壁纸，至于为什么注销或重启之后会无效我也不是太懂……12345678910111213141516def set_wallpaper_from_bmp(bmp_path): reg_key = win32api.RegOpenKeyEx(win32con.HKEY_CURRENT_USER, &quot;Control Panel\\Desktop&quot;, 0, win32con.KEY_SET_VALUE) win32api.RegSetValueEx(reg_key, &quot;WallpaperStyle&quot;, 0, win32con.REG_SZ, &quot;2&quot;) win32api.RegSetValueEx(reg_key, &quot;TileWallpaper&quot;, 0, win32con.REG_SZ, &quot;0&quot;) win32gui.SystemParametersInfo(win32con.SPI_SETDESKWALLPAPER, bmp_path, win32con.SPIF_SENDWININICHANGE) def set_wallpaper(img_path): isExists = os.path.exists(img_path) if isExists: img_dir = os.path.dirname(img_path) bmpImage = Image.open(img_path) new_bmp_path = os.path.join(img_dir, &apos;wallpaper.bmp&apos;) bmpImage.save(new_bmp_path, &quot;BMP&quot;) set_wallpaper_from_bmp(new_bmp_path) return True else: return False 最后是用pyqt5写GUI界面，我也是第一次使用，所以不是太好看，代码也有点乱，大家凑和看看吧哈哈……12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485class Bing(QMainWindow): def __init__(self): super().__init__() self.initUI() def initUI(self): self.setGeometry(400, 200, 500, 325) self.setFixedSize(self.width(), self.height()); self.setWindowTitle(&apos;Bing Wallpaper&apos;) self.setWindowIcon(QIcon(&apos;Bing.ico&apos;)) self.add_position_layout() # 添加布局部件 def add_position_layout(self): bg_label = QLabel(self) bg_label.resize(self.width(), self.height()) bg_label.setScaledContents(True) def set_background_image(img): photo = QtGui.QPixmap() photo.loadFromData(img) bg_label.setPixmap(photo)# img = time.strftime(&quot;%Y%m%d&quot;) + &apos;.jpg&apos; img, self.filename = sp.show_img(sp.get_url()) set_background_image(img)# label_1 = QLabel(&quot;&lt;p style=&apos;font-size:25px;text-align:center;&apos;&gt;&lt;b&gt;Path:&lt;/b&gt;&lt;/p&gt;&quot;, self) label_1 = QLabel(&quot; 保存目录: &quot;, self) label_1.setStyleSheet(&quot;QLabel&#123;background:rgb(255,255,255,100)&#125;&quot;) path_edit = QLineEdit() path_edit.setText(&apos;D:/wallpaper&apos;) path_edit.setToolTip(&apos;抱歉！暂时不支持更换路径！&apos;) path_edit.setReadOnly(True)# def btn_change():# filename = QFileDialog.getExistingDirectory(self, directory=path_edit.text())# path_edit.setText(filename) self.day = 0 def btn_previous(day): img,self.filename = sp.show_img(sp.get_url(self.day + 1)) set_background_image(img) self.day += 1 print(self.day) def btn_next(day): if self.day &gt; 0: img,self.filename = sp.show_img(sp.get_url(self.day - 1)) set_background_image(img) self.day -= 1 print(self.day) else: QMessageBox.information(self, &quot; &quot;, &quot;明天的壁纸还未更新哦！&quot;) def btn_save(day): img = sp.get_img(sp.get_url(self.day)) def btn_set_wallpaper(day): flag = sw.set_wallpaper(&apos;D:/wallpaper/&apos; + self.filename) if not flag: QMessageBox.information(self, &quot; &quot;, &quot;请先保存此壁纸!&quot;) # button_1 = QPushButton(&quot;更改路径&quot;, self)# button_1.clicked.connect(btn_change) button_2 = QPushButton(&quot;前一张&quot;, self) button_2.setToolTip(&apos;查看前一天的必应图片！&apos;) button_2.clicked.connect(btn_previous) button_3 = QPushButton(&quot;设为壁纸&quot;, self) button_3.setToolTip(&apos;设置壁纸要先保存哦！&apos;) button_3.clicked.connect(btn_set_wallpaper) button_4 = QPushButton(&quot;保存&quot;, self) button_4.setToolTip(&apos;保存在该目录下！&apos;) button_4.clicked.connect(btn_save) button_5 = QPushButton(&quot;后一张&quot;, self) button_5.setToolTip(&apos;查看后一天的必应图片！&apos;) button_5.clicked.connect(btn_next) grid = QGridLayout() grid.setSpacing(10) grid.addWidget(label_1, 2, 0) grid.addWidget(path_edit, 2, 1, 1, 2)# grid.addWidget(button_1, 2, 3) grid.addWidget(button_2, 3, 0) grid.addWidget(button_3, 3, 1, 1, 2) grid.addWidget(button_4, 2, 3) grid.addWidget(button_5, 3, 3) grid.setAlignment(Qt.AlignTop) layout_widget = QWidget() layout_widget.setLayout(grid) self.setCentralWidget(layout_widget)if __name__ == &apos;__main__&apos;: sp.get_img(sp.get_url()) app = QApplication(sys.argv) bing = Bing() bing.show() sys.exit(app.exec_()) 简单来说，程序会先通过网络获取图片显示，所以没有联网的话就用不了，要设置壁纸的话需要先点击保存会把图片保存下来，运行结果是这样的： 结语另外，完成之后，我用pyinstaller打包成了exe文件，打包完之后我发现左上角的那个小图标没了，不过不影响使用……大家在公众号回复“bing”就有下载地址了，或者点击我的GitHub，有源码也有exe程序，注意一下前面说的几个问题就好。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fiddler分析手机App请求Scrapy爬虫]]></title>
    <url>%2FPython%2F%E7%88%AC%E8%99%AB%2FFiddler%E5%88%86%E6%9E%90%E6%89%8B%E6%9C%BAApp%E8%AF%B7%E6%B1%82Scrapy%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[Fiddler是一款十分强大的调试代理工具，这个工具我就不详细介绍了。简单来说，它是通过创建代理，拦截http请求用于分析或修改。 这里就演示一下用Fiddler抓取手机端Bilibili的请求并使用Scrapy编写一个爬取图片的爬虫程序。 准备环境 Python，我用的是3.6，系统是win10 Scrapy框架，安装过程我之前的文章提过 Fiddler，点击这里下载安装 手机…我的是安卓 一个安卓软件，我这里就用Bilibili示范 Fiddler配置安装我就不说了，说说配置，Tools - Connections这里可以看到默认端口号是很吉利的8888。这里要勾上Allow remote computers to connect。然后要让手机和电脑连接上同一个网络，并给手机设置代理IP，设置成电脑的IP地址，cmd执行命令ipconfig可以查看ip：手机连接上wifi后，选择代理设置，把服务器设置成刚才看到的IP地址，端口号如果没改的话就是8888，然后保存。下面是给手机安装证书，不安装可能会出现手机设置了代理无法上网的问题。 然后手机浏览器访问ip地址:8888，下载证书安装：打开设置 - 安全 - 设备管理与凭证 - 从存储盘安装，差不多都是这个地方，找找应该能找到…安装完后可以在用户凭证可以看到： 分析请求接下来就可以打开Fiddler还有手机端的Bilibili，进行分析了。我这里选择相簿，准备爬取里面的图片。下面的精选热推就是我们的目标：刷新手机的时候，可以看到Fiddler界面出现了请求的链接返回的是json数据：复制链接到浏览器打开就可以看到图片地址了：点开这个地址，可以看到刚才手机端显示的那张图片：接下来手机端往下拉，在Fiddler可以看到刷出了相似的请求，可以看到请求的参数里，page_num参数增加，这就是表示页码的参数： 编写代码现在，图片的网址我们已经找出来了，接下来就可以编写代码下载图片了，这里我还是使用了Scrapy框架。 首先，还是新建项目：1scrapy startproject scrapy_bilibili 然后编写items.py定义存储字段：123456789import scrapyclass ScrapyBilibiliItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() image_urls = scrapy.Field() images = scrapy.Field() image_paths = scrapy.Field() 然后settings.py把ITEM_PIPELINES注释取消，还有增加IMAGES_STORE作为存储位置，还把USER_AGENT改成了Fiddler看到的值：1234567ITEM_PIPELINES = &#123; &apos;scrapy_bilibili.pipelines.ScrapyBilibiliPipeline&apos;: 300,&#125;IMAGES_STORE = &apos;E:/scrapy_bilibili/images&apos;USER_AGENT = &apos;Mozilla/5.0 BiliDroid/5.22.1 (bbcallen@gmail.com)&apos; 接下来是pipelines.py，这段代码改自官方文档：123456789101112131415161718import scrapyfrom scrapy.pipelines.images import ImagesPipelinefrom scrapy.exceptions import DropItemfrom scrapy.utils.project import get_project_settingsclass ScrapyBilibiliPipeline(ImagesPipeline): IMAGES_STORE = get_project_settings().get(&apos;IMAGES_STORE&apos;) def get_media_requests(self, item, info): image_url = item[&apos;image_urls&apos;] yield scrapy.Request(image_url) def item_completed(self, results, item, info): image_paths = [x[&apos;path&apos;] for ok, x in results if ok] if not image_paths: raise DropItem(&quot;Item contains no images&quot;) item[&apos;image_paths&apos;] = image_paths return item 最后在spiders文件夹里新建bilibili_spider.py放置爬虫的代码：123456789101112131415161718192021import jsonimport scrapyfrom scrapy_bilibili.items import ScrapyBilibiliItemclass BilibiliSpider(scrapy.Spider): name = &apos;bilibili_spider&apos; allowed_domains = [&apos;api.vc.bilibili.com&apos;] page_num = 0 start_urls = [&apos;http://api.vc.bilibili.com/link_draw/v2/Doc/...&amp;page_num=&apos; + str(page_num) + &apos;&amp;page_size=20...&apos;] # 链接太长我就不放完整了 def parse(self, response): datas = json.loads(response.text)[&apos;data&apos;][&apos;items&apos;] for data in datas: item = ScrapyBilibiliItem() for img in data[&apos;item&apos;][&apos;pictures&apos;]: item[&apos;image_urls&apos;] = img[&apos;img_src&apos;] yield item if self.page_num &gt;= 10: return self.page_num += 1 yield scrapy.Request(&apos;http://api.vc.bilibili.com/link_draw/v2/Doc/...&amp;page_num=&apos; + str(self.page_num) + &apos;&amp;page_size=20...&apos;, callback = self.parse) # 链接也是不完整的 运行代码在项目的目录下执行以下命令，注意bilibili_spider是bilibili_spider.py里class的name：1scrapy crawl bilibili_spider 然后，等到运行完，就能看到结果了，图片是使用它们URL的 SHA1 hash 作为文件名的： 对了，这里要注意一个问题，运行的时候如果有挂代理，比如一些科学^_^上网的工具什么的就先关掉吧，不然可能会出现以下问题：1Connection to the other side was lost in a non-clean fashion: Connection lost. 到这里，我们的程序就完成了。]]></content>
      <categories>
        <category>Python</category>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Scrapy</tag>
        <tag>爬虫</tag>
        <tag>Fiddler</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo博客设置以及Next主题美化]]></title>
    <url>%2FHexo%2FHexo%E4%B8%BB%E9%A2%98Next%E7%BE%8E%E5%8C%96%2F</url>
    <content type="text"><![CDATA[之前的一篇文章写了用Hexo和GitHub搭建博客，现在就来对博客主题做一些个性化的修改。 安装Next主题切换到博客的目录，使用Git Bash运行下载主题：1git clone https://github.com/iissnan/hexo-theme-next themes/next 然后打开根目录下的_config.yml，注意这个是整个博客站点的配置文件，而主题目录下也有一个_config.yml，这是主题的配置文件。在站点配置文件找到theme改为next：1theme: next 这样就启用了主题，可以运行hexo s查看效果了。 博客设置这里先对博客基本信息做一些设置，注意了，设置时冒号后面都要有一个空格，这是yml文件的格式 设置语言首先先设置博客站点的语言，这个是在站点配置文件，也就是根目录下的_config.yml设置的，找到language设置成中文：1language: zh-Hans 基本信息在站点配置文件的开头，填上自己博客的相应信息：123456title: # 标题subtitle: # 副标题description: # 站点描述author: # 作者language: zh-Hanstimezone: 主题设置Next已经自带了很多功能和集成了一些第三方服务，通过主题目录下的_config.yml就能对主题做一些设定及个性化。 设置主题的SchemeNext自带了几种外观，在主题目录的_config.yml里找到scheme，我比较喜欢Mist，把前面的注释符#去掉即可：1234#scheme: Musescheme: Mist#scheme: Pisces#scheme: Gemini 菜单设置我们可以看到首页有还有归档等菜单，这些在主题配置文件里设置，找到menu，把需要的菜单取消注释，我这里保留了categories分类，tags标签，archives归档，about关于，||后面的表示图标，使用Font Awesome图标名字。我就不作修改了：123456789menu: home: / || home categories: /categories/ || th tags: /tags/ || tags archives: /archives/ || archive about: /about/ || user #schedule: /schedule/ || calendar #sitemap: /sitemap.xml || sitemap #commonweal: /404/ || heartbeat 另外，也可以自己添加菜单，但我暂时没有这个需要。 创建页面设置完菜单但是没有页面的话点击菜单就会报错了。创建刚才建了菜单的页面执行以下命令：123hexo new page tagshexo new page categorieshexo new page about 然后在source目录下就会生成对应的文件夹，每个文件夹里都有一个index.md，打开将页面的type设置为相应的内容。12345title: 标签date: type: &quot;tags&quot; # 或者 &quot;categories&quot;/&quot;about&quot;comments: false--- comments是关闭这个页面的评论功能用的，评论后面会提到。 文章显示设置默认首页的文章会显示全文，在主题配置文件设置auto_excerpt为true再设置长度就会只显示你设置的字数了。123auto_excerpt: enable: true length: 100 但是，官方是不推荐这种做法的，我这里用的方法是发表的文章的开头加上description，这样，文章就会显示这个摘要：1234567891011title: # 文章标题author: # 作者tags: - Hexo - Nextcategories: - Hexo - Nextdescription: # 描述，首页文章显示的摘要date: --- 这里的tags和categories就是给文章加上标签和分类，两者的区别就是categories是有层级的，像上面那样分类里Next就是Hexo的子类，Hexo是不支持指定多个同级分类的。 使用RSS先在博客目录下执行以下命令安装插件：1npm install --save hexo-generator-feed 然后在主题配置文件里找到rss修改：1rss: /atom.xml 之后在右边的侧边栏就能看到RSS按钮了 设置头像在source目录下新建一个images目录，放一张名为avatar.png的头像，修改主题配置文件的avatar字段：1avatar: /images/avatar.png 设置博客favicon图标在images目录下放置图标，我这里放了两种大小的ico图标，然后在主题配置文件找到favicon并修改：1234567favicon: small: /images/favicon-16x16.ico medium: /images/favicon-32x32.ico #apple_touch_icon: /images/apple-touch-icon-next.png #safari_pinned_tab: /images/logo.svg #android_manifest: /images/manifest.json #ms_browserconfig: /images/browserconfig.xml 侧边栏社交链接在主题配置文件找到social把需要的取消注释，然后填好你的链接就可以了，||后面的是图标名称，和菜单的一样，也是使用Font Awesome图标名字。1234567891011social: GitHub: https://github.com/JianFengY || github GMail: mailto:yjianfengxy@gmail.com || envelope #Google: https://plus.google.com/yourname || google #Twitter: https://twitter.com/yourname || twitter Facebook: https://www.facebook.com/jf.young.1 || facebook #VK Group: https://vk.com/yourname || vk #StackOverflow: https://stackoverflow.com/yourname || stack-overflow #YouTube: https://youtube.com/yourname || youtube #Instagram: https://instagram.com/yourname || instagram #Skype: skype:yourname?call|chat || skype 你也可以自行添加其他社交方式，按照格式添加即可。 设置背景动画同样是主题配置文件，我这里用的是canvas_nest：1234567891011# Canvas-nestcanvas_nest: true# three_wavesthree_waves: false# canvas_linescanvas_lines: false# canvas_spherecanvas_sphere: false 修改文章底部的#号标签打开/themes/next/layout/_macro/post.swig，搜索 rel=&quot;tag&quot;&gt;#，将#换成Font Awesome图标:1&lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt; 评论系统我使用的是来必力 点击这里 注册账号，然后复制下面的data-uid： 在主题配置文件找到livere_uid：123# Support for LiveRe comments system.# You can get your uid from https://livere.com/insight/myCode (General web site)livere_uid: # 粘贴你的uid 这样，除了设置了comments: false的页面，其他都会有评论系统了。 网站底部加访问量这个我是通过修改\themes\next\layout\_partials\footer.swig文件实现的:12345678910&lt;div class=&quot;powered-by&quot;&gt; &lt;i class=&quot;fa fa-user-md&quot;&gt;&lt;/i&gt; &lt;span id=&quot;busuanzi_container_site_uv&quot;&gt; 本站访客数:&lt;span id=&quot;busuanzi_value_site_uv&quot;&gt;&lt;/span&gt; &lt;/span&gt; &lt;span class=&quot;post-meta-divider&quot;&gt;|&lt;/span&gt; &lt;span id=&quot;busuanzi_container_site_pv&quot;&gt; 本站访问量&lt;span id=&quot;busuanzi_value_site_pv&quot;&gt;&lt;/span&gt; &lt;/span&gt;&lt;/div&gt; 统计功能先在博客目录下安装插件：1npm install hexo-wordcount --save 然后在主题配置文件，找到post_wordcount，修改你想要的统计功能，有字数统计，阅读时长等：12345678# Post wordcount display settings# Dependencies: https://github.com/willin/hexo-wordcountpost_wordcount: item_text: true wordcount: true min2read: true totalcount: true separated_meta: true LeanCloud文章阅读次数统计点击这里 注册登录LeanCloud账号，先创建一个任意名称的应用，再创建一个名为Counter的Class。在应用Key这里分别复制App ID和App Key到主题配置文件里的leancloud_visitors，并把enable改为true。123456# Show number of visitors to each article.# You can visit https://leancloud.cn get AppID and AppKey.leancloud_visitors: enable: true app_id: app_key: 然后最好再设置一下安全域名就可以了： 搜索服务Local Search先安装hexo-generator-searchdb，博客目录执行：1npm install hexo-generator-searchdb --save 然后在站点配置文件，注意不是主题配置，添加以下代码：12345search: path: search.xml field: post format: html limit: 10000 然后主题配置文件找到local_search改为true，然后从上面提供的样式选一个填入pace_theme中就可以了：123# Local searchlocal_search: enable: true 然后菜单就会出现搜索了。 顶部加载条修改主题配置文件，找到pace改为true，并从上面提供的样式中选择一种填入pace_theme中就可以了：12345678910111213141516171819# Progress bar in the top during page loading.pace: true# Themes list:#pace-theme-big-counter#pace-theme-bounce#pace-theme-barber-shop#pace-theme-center-atom#pace-theme-center-circle#pace-theme-center-radar#pace-theme-center-simple#pace-theme-corner-indicator#pace-theme-fill-left#pace-theme-flash#pace-theme-loading-bar#pace-theme-mac-osx#pace-theme-minimal# For example# pace_theme: pace-theme-center-simplepace_theme: pace-theme-minimal 右上角加fork me on github点击这里或者这里挑选你喜欢的样式，修改成你的GitHub链接，复制到themes/next/layout/_layout.swig文件中的以下位置：123&lt;div class=&quot;&#123;&#123; container_class &#125;&#125; &#123;% block page_class %&#125;&#123;% endblock %&#125;&quot;&gt; &lt;div class=&quot;headband&quot;&gt;&lt;/div&gt; &lt;a href=&quot;https://github.com/JianFengY&quot; ......&lt;/a&gt; 浏览文章时显示浏览进度主题设置文件查找scrollpercent修改为true：12# Scroll percent label in b2t button.scrollpercent: true 修改文章内链接的样式文章里的链接样式默认是只加一条下划线，颜色和普通文字一样，为了方便区分，在themes/next/source/css/_custom/custom.styl文件加以下代码，自行添加的样式都可以写在这个文件里：12345678910.post-body p a&#123; color: #0593d3; border-bottom: none; border-bottom: 1px solid #0593d3; &amp;:hover &#123; color: #fc6423; border-bottom: none; border-bottom: 1px solid #fc6423; &#125; &#125; 致谢这个博客从搭建到美化，都参考了网上许多文章和教程，我就不一一罗列出来了，感谢网上各路大神！ 这里推荐一篇干货满满的文章：https://www.jianshu.com/p/f054333ac9e6 还有就是官方的文档了，里面也有很详细的主题配置和第三方服务的说明：http://theme-next.iissnan.com/third-party-services.html]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫Scrapy入门篇]]></title>
    <url>%2FPython%2F%E7%88%AC%E8%99%AB%2FPython%E7%88%AC%E8%99%ABScrapy%E5%85%A5%E9%97%A8%E7%AF%87%2F</url>
    <content type="text"><![CDATA[前言因为Python简单灵活的特点，一直是写爬虫的热门语言，我之前也在公众号上分享过几篇简单的爬虫例子和教程。 Scrapy是一款简单高效的Python网络爬虫框架，使用框架，我们就可以只关注数据的提取而不用去自己造轮子了。其实，对于这个框架，我也是个新手…这里，就用我们学校的图书馆系统做个简单的入门项目了。 安装先说一下我的Python是3.6版本的，电脑是win10。 Scrapy也可以直接用使用pip命令安装：1pip install scrapy 因为Scrapy依赖了其他一些包，所以会额外安装一些库，如lxml、Twisted和pyOpenSSL等，这在文档也有提到。我在安装这些库时有遇到一个问题，报了下面这个错误：1Command &quot;&quot;d:\program files\python3.6.1\python.exe&quot; -u -c &quot;import setuptools, tokenize;__file__=&apos;C:\\Users\\ASUS\\AppData\\Local\\Temp\\pip-build-qkm97m8f\\Twisted\\setup.py&apos;;f=getattr(tokenize, &apos;open&apos;, open)(__file__);code=f.read().replace(&apos;\r\n&apos;, &apos;\n&apos;);f.close();exec(compile(code, __file__, &apos;exec&apos;))&quot; install --record C:\Users\ASUS\AppData\Local\Temp\pip-e4cf8gml-record\install-record.txt --single-version-externally-managed --compile&quot; failed with error code 1 in C:\Users\ASUS\AppData\Local\Temp\pip-build-qkm97m8f\Twisted\ 可以看到是安装Twisted出的问题，解决方法是在 这里 下载对应版本的whl包，我的是Twisted-17.9.0-cp36-cp36m-win32.whl，cp后面是Python版本，win32表示位数，我的Python是32位的。 然后切换到下载目录运行以下命令：1pip install Twisted-17.9.0-cp36-cp36m-win32.whl 然后再安装Scrapy就可以了。你会看到：1Successfully installed PyDispatcher-2.0.5 asn1crypto-0.24.0 cffi-1.11.4 cryptography-2.1.4 cssselect-1.0.3 parsel-1.4.0 pyOpenSSL-17.5.0 pyasn1-0.4.2 pyasn1-modules-0.2.1 pycparser-2.18 queuelib-1.4.2 scrapy-1.5.0 service-identity-17.0.0 创建项目切换到你想放这个项目的目录，执行scrapy startproject projectname创建一个项目，如：1scrapy startproject scrapy_test 生成的项目结构是这样的：12345678910scrapy_test/ scrapy.cfg # deploy configuration file scrapy_test/ # project&apos;s Python module, you&apos;ll import your code from here __init__.py items.py # project items definition file middlewares.py # project middlewares file pipelines.py # project pipelines file settings.py # project settings file spiders/ # a directory where you&apos;ll later put your spiders __init__.py spiders目录用于放置我们的爬虫，items.py用于定义我们要获取的数据，pipelines.py定义存储，settings.py顾名思义就是配置文件了。 编写代码 第一步我们先定义要存储什么字段，这里就定义书名、作者和索引号吧，编写items.py：1234567891011import scrapyclass BooksItem(scrapy.Item): &apos;&apos;&apos;定义要存储的字段&apos;&apos;&apos; # 书名 name = scrapy.Field() # 作者 author = scrapy.Field() # 索引号 call_number = scrapy.Field() 接下来我们在spiders目录下编写爬虫文件，我这里新建一个books_spider.py文件，代码如下：1234567891011121314151617181920212223import scrapyfrom scrapy_test.items import BooksItemclass BooksSpider(scrapy.Spider): &apos;&apos;&apos;图书爬虫类&apos;&apos;&apos; # 这个name不能重复 name = &apos;books&apos; # allowed_domains = [&apos;202.116.174.108:8080&apos;] start_urls = [&quot;http://202.116.174.108:8080/top/top_lend.php?cls_no=ALL&quot;] def parse(self, response): &apos;&apos;&apos;处理下载的response的默认方法&apos;&apos;&apos; books = [] for item in response.xpath(&apos;//tr&apos;)[1:]: book = BooksItem() book_name = item.xpath(&apos;td[2]/a/text()&apos;).extract() book_author = item.xpath(&apos;td[3]/text()&apos;).extract() book_call_number = item.xpath(&apos;td[5]/text()&apos;).extract() book[&apos;name&apos;] = book_name[0] book[&apos;author&apos;] = book_author[0] book[&apos;call_number&apos;] = book_call_number[0] books.append(book) return books 这个类继承scrapy.Spider，需要指定name，它是运行爬虫用的，start_urls是要爬取的url列表，parse函数接收获取到的response解析用的是scrapy自带的xpath,这个网页的结构很简单，所以就不多解释了。总之，就是遍历每一本书的信息，然后存在books中。这些信息在之前的item中已经定义了。代码的编写到这里就结束了，接下来就可以运行了。 运行代码要运行代码，切换到项目的目录里，执行下面的命令，其中books就是上面的name，books.json就是生成的json文件：1scrapy crawl books -o books.json -t json 开始运行时，出现了一个问题，就是没报错，但没有结果，输出一片空白，然后发现下面的提示：1DEBUG: Forbidden by robots.txt 这是因为要获取的页面在robots中被禁止了，所以Scrapy自动忽略了，只需修改settings.py将：ROBOTSTXT_OBEY改为False就行了。 后来又遇到了乱码的问题，输出全是\uxxx这种字符，只要在settings.py加上FEED_EXPORT_ENCODING = &#39;utf-8&#39;就可以。 没报错执行完上面的命令之后就会在根目录生成一个books.json文件，打开，就会看到保存下来的json格式图书信息：123456789101112131415161718192021222324[ &#123; &quot;name&quot;: &quot;暗恋·橘生淮南&quot;, &quot;author&quot;: &quot;八月长安[著]&quot;, &quot;call_number&quot;: &quot;I247.57/5494&quot; &#125;, &#123; &quot;name&quot;: &quot;Time series and panel data econometrics / First edition.&quot;, &quot;author&quot;: &quot;M. Hashem Pesaran.&quot;, &quot;call_number&quot;: &quot;F224.0/EN4&quot; &#125;, &#123; &quot;name&quot;: &quot;杀死一只知更鸟.第2版&quot;, &quot;author&quot;: &quot;(美国) 哈珀·李著&quot;, &quot;call_number&quot;: &quot;I712.45/1231=2&quot; &#125;, ... ... &#123; &quot;name&quot;: &quot;众筹:互联网金融的下一个风口&quot;, &quot;author&quot;: &quot;陈晓暾, 白帆, 陈英编著&quot;, &quot;call_number&quot;: &quot;F830.45/77&quot; &#125;] 我们的Scrapy入门项目就完成了！]]></content>
      <categories>
        <category>Python</category>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Scrapy</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitHub+Hexo搭建个人博客]]></title>
    <url>%2FHexo%2FHexo%2BGitHub%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[其实网上已经有很多这样的教程了，我这里只是把自己一步步搭建的过程做一些记录而已。闲话不多说，马上开始。 准备环境 Git，先 点击这里 下载Git安装，安装过程我就不作说明了 Node.js，点击这里 下载安装，安装也很简单方便，这里也不作说明，记得安装上npm就好 GitHub账号当然不能少，这里就默认你已经有账号了…没有的 点击这里 去注册吧 Hexo安装与搭建首先，新建一个文件夹，你的博客就要放在这里，我这里是E:\hexo，然后，在这个文件夹里右键Git Bash Here，因为要使用到这个，所以就不用cmd命令行直接用这个了。执行以下命令安装Hexo：1npm install -g hexo-cli 再执行以下命令，生成建立网站所需要的所有文件并安装依赖包：12hexo initnpm install 然后，我们在根目录下的_config.yml文件，填上自己博客的相应信息，注意，冒号后面都要有一个空格，这是yml文件的格式：123456title: # 标题subtitle: # 副标题description: # 站点描述author: # 作者language: zh-Hanstimezone: 然后，以下命令生成静态页面并开启本地服务器12hexo generatehexo server 现在，浏览器输入http://localhost:4000就能查看你的博客了，当然，只是本地，接下来，就是放到GitHub上让别人能访问了。 部署到GitHub创建仓库首先New Repository，名称是username.github.io，username要与账号对应，比如我的就是JianFengY.github.io，所以，我的GitHub账号本来应该全部小写字母的，这样会比较协调… 生成SSH密钥执行以下命令，不出意外应该是一直回车就好。1ssh-keygen -t rsa -C &quot;Github的注册邮箱地址&quot; 然后，在C:\Users\ASUS\.ssh目录会有两个文件id_rsa和id_rsa.pub,打开id_rsa.pub，复制里面的所有内容到 SSH keys这里 的Key，Title随便填，然后Add SSH key就可以了 部署首先，在根目录下的_config.yml文件，找到deploy，填上相应信息，repo就是上面创建的仓库地址：1234deploy: type: git repo: https://github.com/username/username.github.io.git branch: master 安装hexo-deployer-git部署发布工具:1npm install hexo-deployer-git --save 然后就可以使用以下命令发布你的博客了，第一次发布会让你输入Github 的邮箱和密码：1hexo generate &amp;&amp; hexo deploy 这里贴一下Hexo常用命令：1234567hexo init # 初始化目录hexo server 或 hexo s # 本地服务器预览hexo new &quot;postName&quot; 或 hexo n &quot;postName&quot; # 新建文章hexo new page &quot;pageName&quot; 或 hexo n &quot;pageName&quot; # 新建页面hexo generate 或 hexo g # 生成网页在 public 目录有整个网站的文件hexo deploy 或 hexo d # 部署.deploy目录hexo clean # 清除缓存 完成现在，浏览器输入https://username.github.io/我的就是https://jianfengy.github.io/就能查看你的博客了。]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu配置JDK、MySQL和Tomcat遇到的一些坑]]></title>
    <url>%2FLinux%2FUbuntu%2FUbuntu%E9%85%8D%E7%BD%AEJDK%E3%80%81MySQL%E5%92%8CTomcat%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91%2F</url>
    <content type="text"><![CDATA[之前买了阿里云的Ubuntu服务器，安装JDK、MySQL和Tomcat的时候遇到了一些问题，折腾了挺长时间，这里是一些记录。 安装配置JDKwindows上下载的jdk-8u161-linux-i586.tar.gz用xshell执行rz命令上传到阿里云的ubuntu，然后执行mkdir /usr/java建一个文件夹，再执行tar zxvf jdk-8u161-linux-i586.tar.gz -C /usr/java解压到java目录下，之后执行vim ~/.bashrc在文件尾添加以下内容设置环境变量：1234export JAVA_HOME=/usr/java/jdk1.8.0_151export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:&#123;JAVA_HOME&#125;/lib:&#123;JRE_HOME&#125;/libexport PATH=&#123;JAVA_HOME&#125;/bin:PATH 这里遇到了报错-bash: java/ jdk1.8.0_131/bin/java: cannot execute binary file，是因为我下载的JDK是64位的，而ubuntu是32位的…使用getconf LONG_BIT命令可以查看linux位数。 安装配置Mysql安装时有遇到Err http://mirrors.aliyun.com/ubuntu 404 Not Found，执行sudo apt-get update就可以了。 网上学到的一个技巧：如果忘记了MySQL账户的密码，文件/etc/mysql/debian.cnf里有个MySQL用户debian-sys-maint。可以mysql -u debian-sys-maint -p使用文件提供的密码登录MySQL修改root的密码。 修改MySQL密码时因为设置的密码过于简单会报错Your password does not satisfy the current policy requirements，处理如下：123456789101112mysql&gt; update mysql.user set authentication_string=password(&apos;123456&apos;) where user=&apos;root&apos; and Host =&apos;localhost&apos;;ERROR 1819 (HY000): Your password does not satisfy the current policy requirementsmysql&gt; set global validate_password_policy=0;Query OK, 0 rows affected (0.00 sec)mysql&gt; set global validate_password_length=4;Query OK, 0 rows affected (0.00 sec)mysql&gt; update mysql.user set authentication_string=password(&apos;123456&apos;) where user=&apos;root&apos; and Host =&apos;localhost&apos;;Query OK, 1 row affected, 1 warning (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 1 安装配置tomcat安装运行之后不能使用外网访问，执行ufw allow 8080允许外网可以访问8080端口然后配置阿里云安全组，找到自己的实例，点击更多-安全组配置-配置规则-添加安全组规则配置端口范围8080/8080，授权对象0.0.0.0/0之后就可以了。]]></content>
      <categories>
        <category>Linux</category>
        <category>Ubuntu</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Ubuntu</tag>
        <tag>JDK</tag>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySql学习笔记]]></title>
    <url>%2FMySQL%2FMySql%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[这是我之前在慕课网学习 与MySQL的零距离接触 时的笔记，整理了一下发上来。 修改MySQL提示符 参数 描述 \p 完整的日期 \d 当前数据库 \h 服务器名称 \u 当前用户 各个参数可以结合使用。如prompt \u@\h \d&gt; 连接客户端时指定 1mysql -uroot -p123456 --prompt \h 连接上客户端后 1PROMPT &quot;提示符&quot; MySQL常用命令 显示当前服务器版本SELECT VERSION(); 显示当前日期时间SELECT NOW(); 显示当前用户SELECT USER(); 可用SHOW WARNINGS;查看警告信息 MySQL语句规范 关键字与函数名称全部大写 数据库名称、表名称、字段名称全部小写 SQL语句必须以分号结尾 操作数据库 “{}”表示必选项， “|”表示或， “[]”表示可选项 创建数据库1CREATE &#123;DATABASE | SCHEMA&#125; [IF NOT EXISTS] db_name [DEFAULT] CHARACTER SET [=] charset_name; 例如：1CREATE DATABASE IF NOT EXISTS db_name CHARACTER SET utf8; 查看当前服务器下数据表列表 1SHOW &#123;DATABASES | SCHEMAS&#125; [LIKE &apos;pattern&apos; | WHERE expr]; 修改数据库 1ALTER &#123;DATABASE | SCHEMA&#125; [db_name] [DEFAULT] CHARACTER SET [=] charset_name; 例如：1ALTER DATABASE db_name CHARACTER SET = gbk; 删除数据库1DROP &#123;DATABASE | SCHEMA&#125; [IF EXISTS] db_name; 数据类型 整型 数据类型 字节 TINYINT 1 SMALLINT 2 MEDIUMINT 3 INT 4 BIGINT 8 浮点型 FLOAT[(M,D)] DOUBLE[(M,D)] M是数字总位数，D是小数点后面的位数。 日期时间型 列类型 存储需求 YEAR 1 TIME 3 DATE 3 DATETIME 8 TIMESTAMP 4 字符型 列类型 存储需求 CHAR(M) M个字节，0&lt;=M&lt;=255 VARCHAR(M) L+1个字节，其中L&lt;=M且0&lt;=M&lt;=65535 TINYTEXT L+1个字节，其中L&lt;2^8 TEXT L+2个字节，其中L&lt;2^16 MEDIUMTEXT L+3个字节，其中L&lt;2^24 LONGTEXT L+4个字节，其中L&lt;2^32 ENUM(‘value1’,’value2’,…) 1或2个字节，取决于枚举值的个数(最多65536个值) SET(‘value1’,’value2’,…) 1、2、3、4或8个字节，取决于set成员数目(最多64个成员) 数据表创建数据表1234CREATE TABLE [IF NOT EXISTS] table_name (column_name data_type,...); 例如：12345CREATE TABLE table_name(username VARCHAR(20),age TINYINT UNSIGNED,salary FLOAT(8,2) UNSIGNED); UNSIGNED表示无符号位，即始终为正数。使用SHOW CREATE TABLE table_name;可以查看建表语句 查看数据表1SHOW TABLES [FROM db_name] [LIKE &apos;pattern&apos; | WHERE expr]; 查看数据表结构1SHOW COLUMNS FROM table_name; 或：1DESC table_name; 插入记录1INSERT [INTO] table_name[(col_name,...)] VALUES(val,...); 查找记录1SELECT expr,... FROM table_name; 空值与非空 NULL，字段值可以为空(可以不写，默认可以为空) NOT NULL，字段值禁止为空 自动编号AUTO_INCREMENT ,必须与主键组合使用，默认情况下，起始值为1，每次增量为1，不需要赋值 主键约束PRIMARY KEY ,每张表只能存在一个主键,用于保证记录的唯一性(不能有重复值)，主键自动为NOT NULL 唯一约束UNIQUE KEY ,可以保证记录唯一性，唯一约束的字段可以为空(NULL)，每张表可以存在多个唯一约束 默认约束DEFAULT ,默认值，插入记录时，如果没有明确为字段复制，则自动赋予默认值 外键约束作用：保证数据的一致性、完整性；实现一对一或一对多关系 要求： 父表（子表所参照的表）和子表（具有外键列的表）必须使用相同的存储引擎（InnoDB），且禁止使用临时表 外键列（有FOREIGN KEY关键字的列）和参照列必须具有相似的数据类型。其中数字的长度或是否有符号位必须相同；而字符的长度可以不同。否则会报错(errorno：150) 外键列和参照列必须创建索引。如果外键列不存在索引的话，MySQL会自动创建索引（主键都会自动创建索引）.以网格形式查看索引：SHOW INDEXES FROM db_name\G; 编辑数据表默认存储引擎在安装MySQL的磁盘的Server 5.7\my.ini```文件里，修改为```default-storage-engine12例： CREATE TABLE provinces(id SMALLINT UNSIGNED PRIMARY KEY AUTO_INCREMENT,pname VARCHAR(20) NOT NULL);1234567```CREATE TABLE users(id SMALLINT UNSIGNED PRIMARY KEY AUTO_INCREMENT,username VARCHAR(20) NOT NULL,pid SMALLINT UNSIGNED,FOREIGN KEY (pid) REFERENCES provinces (id)); 外键约束的参照操作 CASCADE：从父表删除或更新且自动删除或更新子表中匹配的行 SET NULL：从父表删除或更新行，并设置子表中的外键列为NULL。如果适用此项，必须保证兹表列没有制定NOT NULL RESTRICT：拒绝对父表的删除或更新操作 NO ACTION：标准SQL的关键字，在MySQL中与RESTRICT相同 例：123456CREATE TABLE users1(id SMALLINT UNSIGNED PRIMARY KEY AUTO_INCREMENT,username VARCHAR(10) NOT NULL,pid SMALLINT UNSIGNED,FOREIGN KEY (pid) REFERENCES provinces (id) ON DELETE CASCADE); 表示级联删除，此时删除provinces表中的记录，user1中相关的记录也会被删除，级联更新ON UPDATE CASCADE同理 表级约束与列级约束 对一个数据列建立的约束，称为列级约束。既可以在列定义时声明，也可以在列定义后声明（NOT NULL和DEFAULT约束只有列级约束，其他几种约束则可以有表级也可以有列级约束） 对多个数据列建立的约束，称为表级约束。只能在列定义后声明 修改数据表添加单列1ALTER TABLE table_name ADD [COLUMN] column_name column_definition [FIRST | AFTER column_name; 其中，FIRST指把新列放在最前面，AFTER指把新列放在指定列，不加此参数默认添加到所有列最后面 添加多列1ALTER TABLE table_name ADD [COLUMN] (column_name column_definition, ...); 添加多列时，要加小括号，且不能制定添加的位置 删除列1ALTER TABLE table_name DROP [COLUMN] column_name; 可以同时操作几个语句，用逗号隔开,如：1ALTER TABLE table_name DROP [COLUMN] column_name,DROP [COLUMN] column_name,ADDADD [COLUMN] column_name column_definition; 添加主键约束1ALTER TABLE table_name ADD [CONSTRAINT [symbol]] PRIMARY KEY [index_type] (index_col_name, ...); 例：1ALTER TABLE users2 ADD CONSTRAINT PK_users2_id PRIMARY KEY (id); 删除主键约束1ALTER TABLE table_name DROP PRIMARY KEY; 添加唯一约束1ALTER TABLE table_name ADD [CONSTRAINT [symbol]] UNIQUE [INDEX | KEY] [index_name] [index_type] (index_col_name, ...); 例：1ALTER TABLE users2 ADD UNIQUE (username); 删除唯一约束要删除唯一约束需要先知道约束的名字：1SHOW INDEXES FROM table_name\G; 1ALTER TABLE table_name DROP &#123;INDEX | KEY&#125; index_name; 添加外键约束1ALTER TABLE table_name ADD [CONSTRAINT [symbol]] FOREIGN KEY [index_name] (index_col_name, ...) reference_difinition; 例：1ALTER TABLE users2 ADD FOREIGN KEY (pid) REFERENCES provinces (id); 删除外键约束删除外键约束也要先知道约束的名字：1SHOW CREATE TABLE table_name; 1ALTER TABLE table_name DROP FOREIGN KEY fk_symbol; 添加/删除默认约束1ALTER TABLE table_name ALTER [COLUMN] col_name &#123;SET DEFAULT literal | DROP DEFAULT&#125;; 例：12ALTER TABLE users2 ALTER age SET DEFAULT 15;ALTER TABLE users2 ALTER age DROP DEFAULT; 修改列定义1ALTER TABLE table_name MODIFY [COLUMN] col_name column_difinition [FIRST | AFTER col_name]; 例：1ALTER TABLE users2 MODIFY id SMALLINT UNSIGNED NOT NULL FIRST; 注意： 由于存储范围不同，数据类型由大类型改为小类型时可能会导致数据丢失 修改列名称1ALTER TABLE table_name CHANGE [COLUMN] old_col_name new_col_name column_difinition [FIRST | AFTER col_name]; 使用CHANGE语句既可以修改列名称也可以修改列定义 修改数据表名1ALTER TABLE table_name RENAME [TO | AS] new_table_name; 或：1RENAME TABLE table_name TO new_table_name [, table_name2 TO new_table_name2] ... ; RENAME TABLE这个语句可以修改多个数据表的名字 例：12ALTER TABLE users2 RENAME users3;RENAME TABLE users3 TO users2,users1 TO users4; 尽量少使用修改数据列或数据表名的命令，因为在创建了索引或使用了视图或存储过程后，表名和列名被引用的情况下，更名可能会导致视图等无法正常工作 操作记录插入记录第一种方式：1INSERT [INTO] table_name [(col_name, ...)] &#123;VALUES | VALUE&#125; (&#123;expr | DEFAULE&#125;,...),(...),...; 插入的值可以写数学表达式也可以写一些函数如md5(&#39;123&#39;)，也可以一次插入多条记录插入时把AUTO_INCREMENT的字段赋值为NULL或DEFAULT都会自动递增 第二种方式：1INSERT [INTO] table_name SET col_name=&#123;expr | DEFAULT&#125;,...; 与第一种方式的区别在于，此方法可以使用子查询(SubQuery)，而且一次只能插入一条记录 第三种方式： 1INSERT [INTO] table_name [(col_name,...)] SELECT ...; 此方法可以将SELECT查询结果插入到指定数据表，见下文子查询 更新记录（单表更新）1UPDATE [LOW_PRIORITY] [IGNORE] table_reference SET col_name1=&#123;expr1 | DEFAULT&#125; [, col_name2=&#123;expr2 | DEFAULT&#125;] ... [WHERE where_condition]; 可以一次更新多条记录，不加WHERE会更新所有记录 例：1UPDATE users SET age = age + 10 WHERE id % 2 = 0; 删除记录（单表删除）1DELETE FROM table_name [WHERE where_condition]; 查询记录123456789SELECT select_expr [, select_expr ...][ FROM table_references [WHERE where_condition] [GROUP BY &#123;col_name | position&#125; [ASC | DESC],...] [HAVING where_condition] [ORDER BY &#123;col_name | expr |position&#125; [ASC | DESC],...] [LIMIT &#123;[offset,] row_count | row_count OFFSET offset&#125;]]; 查询表达式select_expr： 每一个表达式表示想要的一列，必须有至少一个 多个列之间以英文逗号分隔 星号(*)表示所有列。table_name.*可以表示命名表的所有列 查询表达式可以使用[AS] alias_name为其赋予别名 别名可用于GROUP BY, ORDER BY或HAVING子句 条件表达式对记录进行过滤，如果没有指定WHERE子句，则显示所有记录。在WHERE表达式中，可以使用MySQL支持的函数或运算符 查询结果分组[GROUP BY {col_name | position} [ASC | DESC],...]，ASC为升序DESC为降序，默认是升序 分组条件[HAVING where_condition],条件中若使用字段，要保证字段出现在select_expr中，或者使用聚合函数，如count()等 对查询结果进行排序[ORDER BY {col_name | expr |position} [ASC | DESC],...]，position指字段在select_expr中出现的位置，推荐尽量直接指定字段名 限制查询结果返回的数量1[LIMIT &#123;[offset,] row_count | row_count OFFSET offset&#125;] 例：SELECT * FROM users LIMIT 3,2;表示从第四条开始（编号从0开始，且顺序是查询结果的顺序，受ORDER BY等影响），返回共两条数据 子查询子查询(SubQuery) 是指出现在其他SQL语句内的SELECT子句如：1SELECT * FROM t1 WHERE col1 = (SELECT col2 FROM t2); 其中 SELECT * FROM t1 称为 Outer Query[外查询](或者Outer Statement) , SELECT col2 FROM t2 称为 SubQuery[子查询]。 子查询是嵌套在外查询内部，也有可能在子查询内部再嵌套子查询。而且子查询必须出现在圆括号之间 子查询可以包含多个关键字或条件，如DISTINCT, GROUP BY, ORDER BY, LIMIT, 函数等 子查询的外层查询可以是SELECT, INSERT, UPDATE, SET或DO 子查询的返回值可以是标量、一行、一列或子查询使用比较运算符的子查询比较运算符：=, &gt;, &lt;, &gt;=, &lt;=, &lt;&gt;, !=, &lt;=&gt; 语法结构：operand comparison_operator [ANY | SOME | ALL] subquery 例：1SELECT goods_id,goods_name,goods_price FROM tdb_goods WHERE goods_price &gt; (SELECT ROUND(AVG(goods_price),2) AS avg_price FROM tdb_goods) ORDER BY goods_price DESC; AVG(), MAX(), MIN(), COUNT(), SUM()等为聚合函数，聚合函数只有一个返回值使用比较运算符时，如果子查询返回值多于一行记录可能会报错，这时需要用ANY, SOME或ALL修饰： ANY SOME ALL &gt;, &gt;= 最小值 最小值 最大值 &lt;, &lt;= 最大值 最大值 最小值 = 任意值 任意值 &lt;&gt;, != 任意值 例：1SELECT goods_id,goods_name,goods_price FROM tdb_goods WHERE goods_price = ANY(SELECT goods_price FROM tdb_goods WHERE goods_cate = &apos;超级本&apos;) ORDER BY goods_price DESC; 使用[NOT] IN 的子查询语法结构：operand comparison_operator [NOT] IN (subquery) 注意： =ANY和=SOME运算符与IN等效；!=ALL或&lt;&gt;ALL运算符与NOT IN等效 使用[NOT] EXISTS的子查询如果子查询返回任何行，EXISTS将返回TRUE，否则返回FALSE 使用INSERT...SELECT插入记录例：1INSERT tdb_goods_cates (cate_name) SELECT goods_cate FROM tdb_goods GROUP BY goods_cate; 多表更新1UPDATE table_references SET col_name1=&#123;expr1 |DEFAULT&#125; [, col_name2=&#123;expr2 | DEFAULT&#125;]... [WHERE where_condition]; 多表更新时可能会出现字段名混淆，这时候一般会使用AS给表起别名，如： 1UPDATE tdb_goods AS g INNER JOIN tdb_goods_brands AS b ON g.brand_name = b.brand_name SET g.brand_name = b.brand_id; 创建表同时插入记录CREATE...SELECT1CREATE TABLE [IF NOT EXISTS] table_name [&#123;create_definition,...)] select_statement; 例：1234CREATE TABLE tdb_goods_brands (brand_id SMALLINT UNSIGNED PRIMARY KEY AUTO_INCREMENT,brand_name VARCHAR(40) NOT NULL) SELECT brand_name FROM tdb_goods GROUP BY brand_name; 连接MySQL在SELECT语句、多表更新、多表删除语句中支持JOIN操作 连接类型： INNER JOIN内连接。MySQL中，JOIN, CROSS JOIN和INNER JOIN是等价的 LEFT [OUTER] JOIN左外连接 RIGHT [OUTER] JOIN右外连接 连接条件ON或WHERE： 通常使用ON关键字来设定连接条件 使用WHERE关键字进行结果集记录的过滤 语法结构：table_reference {[INNER | CROSS] JOIN | {LEFT | RIGHT} [OUTER] JOIN} table_reference ON conditional_expr 数据表参照table_reference1table_name [[AS] alias] | table_subquery [AS] alias 数据表可以使用table_name AS alias_name或table_name alias_name赋予别名 table_subquery可以作为子查询使用在FROM子句中，这样的子查询必须为其赋予别名 内连接显示左表及右表符合连接条件的记录，即两表的公共部分 左外连接显示左表的全部记录及右表符合连接条件的记录 右外连接显示右表的全部记录及左表符合连接条件的记录 多表连接通过内连接实现查询所有商品的详细信息：123SELECT goods_id,goods_name,cate_name,brand_name,goods_price FROM tdb_goods AS gINNER JOIN tdb_goods_cates AS c ON g.cate_id = c.cate_idINNER JOIN tdb_goods_brands AS b ON g.brand_id = b.brand_id\G; 通过左外连接实现查询所有商品的详细信息：123SELECT goods_id,goods_name,cate_name,brand_name,goods_price FROM tdb_goods AS gLEFT JOIN tdb_goods_cates AS c ON g.cate_id = c.cate_idLEFT JOIN tdb_goods_brands AS b ON g.brand_id = b.brand_id\G; 通过右外连接实现查询所有商品的详细信息：123SELECT goods_id,goods_name,cate_name,brand_name,goods_price FROM tdb_goods AS gRIGHT JOIN tdb_goods_cates AS c ON g.cate_id = c.cate_idRIGHT JOIN tdb_goods_brands AS b ON g.brand_id = b.brand_id\G; 一些说明 对于左外连接A LEFT JOIN B join_condition（右外连接类似）： 数据表B的结果集依赖数据表A 数据表A的结果集根据左连接条件以来所有数据表（B表除外） 左外连接条件决定如何检索数据表B（在没有制定WHERE条件的情况下） 如果数据表A的某条记录符合WHERE条件，但是数据表B不存在符合连接条件的记录，将生成一个所有列为空的额外B行 如果使用内连接查找的记录在连接数据表中不存在，并且在WHERE子句中尝试以下操作：col_name IS NULL时，如果col_name被定义为NOT NULL，MySQL将在找到符合连接条件的记录后停止搜索更多的行 无限分类的数据表设计例：12345CREATE TABLE tdb_goods_types(type_id SMALLINT UNSIGNED PRIMARY KEY AUTO_INCREMENT,type_name VARCHAR(20) NOT NULL,parent_id SMALLINT UNSIGNED NOT NULL DEFAULT 0); parent_id 就是该类的父类，来自 type_id，没有父类（即最大类）的 type_id为0 这种数据表的查询用到自身连接（同一个数据表对其自身进行连接）如： 查找所有分类及其父类： 1SELECT s.type_id,s.type_name,p.type_name FROM tdb_goods_types AS s LEFT JOIN tdb_goods_types AS p ON s.parent_id = p.type_id; 查找所有分类及其子类： 1SELECT p.type_id,p.type_name,s.type_name FROM tdb_goods_types AS p LEFT JOIN tdb_goods_types AS s ON s.parent_id = p.type_id; 查找所有分类及其子类的数目 1SELECT p.type_id,p.type_name,count(s.type_name) AS children_count FROM tdb_goods_types AS p LEFT JOIN tdb_goods_types AS s ON s.parent_id = p.type_id GROUP BY p.type_name ORDER BY p.type_id; 多表删除1DELETE table_name[.*] [, table_name[.*]] ... FROM table_references [WHERE where_condition]; 例如删除重复记录：1DELETE t1 FROM tdb_goods AS t1 LEFT JOIN (SELECT goods_id,goods_name FROM tdb_goods GROUP BY goods_name HAVING count(goods_name) &gt;= 2 ) AS t2 ON t1.goods_name = t2.goods_name WHERE t1.goods_id &gt; t2.goods_id; 字符函数 函数名称 描述 CONCAT() 字符连接 CONCAT_WS() 使用指定的分隔符进行字符连接 FORMAT() 数字格式化,此函数返回值是字符型 LOWER() 转换成小写字母 UPPER() 转换成大写字母 LEFT() 获取左侧字符 RIGHT() 获取右侧字符 LENGTH() 获取字符串长度（包括空格） LTRIM() 删除字符串的前导空格 RTRIM() 删除字符串的后续空格 TRIM() 删除前导和后续空格 SUBSTRING() 字符串截取 [NOT] LIKE 模式匹配 REPLACE() 字符串替换 注意：表示字符位置的数字是从1开始而不是从0开始的，起始位置的数值可以是负值，表示从后往前数例： SELECT CONCAT_WS(&#39;|&#39;,&#39;A&#39;,&#39;B&#39;,&#39;C&#39;);结果：A|B|C SELECT FORMAT(12560.75, 1);结果：12,560.8 SELECT LOWER(LEFT(&#39;MySQL&#39;, 2));结果：my。表示把截取的前两位字符变为小写 SELECT TRIM(LEADING &#39;?&#39; FROM &#39;??My??SQL???&#39;);结果：My??SQL???。这里的LEADING表示删除前导的指定字符，TRAILING表示删除后续，BOTH表示前导和后续都删除 SELECT REPLACE(&#39;??My??SQL???&#39;, &#39;?&#39;, &#39;&#39;);结果：MySQL SELECT SUBSTRING(&#39;MySQL&#39;, 1, 2);结果：My。这里表示从第一位截取，截取两位 SELECT &#39;MYSQL&#39; LIKE &#39;M%&#39;;结果：1。1表示TRUE，%表示任意0个或多个字符，另外还有_下划线表示任意一个字符 假设数据表test有个first_name为tom%的数据，要查找出此记录，就要SELECT * FROM test WHERE first_name LIKE &#39;%1%%&#39; ESCAPE &#39;1&#39;;，表示1后面的%为普通字符串而非通配符 数值运算符与函数 名称 描述 CEIL() 进一取整（向上取整） DIV 整除，如3 DIV 4为0 FLOOR() 舍一取整（向下取整） MOD 取余数（取模，与%相同） POWER() 幂运算 ROUND() 四舍五入 TRUNCATE() 数字截取 例： SELECT POWER(2,3); 结果：8 SELECT TRUNCATE(125.59,1);结果：125.5。保留小数点后一位，舍去后面的，与四舍五入不同 比较运算符与函数 名称 描述 [NOT] BETWEEN…AND… [不]在范围之内 [NOT] IN() [不]在列出值范围内 IS [NOT] NULL [不]为空 例： SELECT 35 BETWEEN 1 AND 35;结果：1 SELECT 13 NOT IN(5, 10, 15);结果：1 SELECT 0 IS NOT NULL;结果：1 日期时间函数 名称 描述 NOW() 当前日期和时间 CURDATE() 当前日期 CURTIME() 当前时间 DATE_ADD() 日期变化 DATEDIFF() 日期差值 DATE_FORMAT() 日期格式化 例 ： SELECT DATE_ADD(&#39;2017-11-23&#39;, INTERVAL -365 DAY);结果：2016-11-23 SELECT DATE_ADD(&#39;2017-11-23&#39;, INTERVAL 3 WEEK);结果：2017-12-14 SELECT DATEDIFF(&#39;2017-11-23&#39;, &#39;2018-11-23&#39;);结果：-365 SELECT DATE_FORMAT(&#39;2017-11-23&#39;, &#39;%m/%d/%Y&#39;);结果：11/23/2017 信息函数 名称 描述 CONNECTION_ID() 连接ID DATABASE() 当前数据库 LAST_INSERT_ID() 最后插入的记录的ID号 USER() 当前用户 VERSION() 版本信息 对于LAST_INSERT_ID()函数，需要数据表里有一个AUTO_INCREMENT的id字段。另外，若是同时插入多条记录，如INSERT test(name,age) VALUES(&#39;Tom&#39;,20),(&#39;Jack&#39;,22);，这个函数只会返回第一个插入的也就是Tom的id 聚合函数 名称 描述 AVG() 平均值 COUNT() 计数 MAX() 最大值 MIN() 最小值 SUM() 求和 聚合函数的典型特点：只有一个返回值 信息函数 名称 描述 MD5() 信息摘要算法 PASSWORD() 密码算法 如果密码是为WEB页面准备的，建议使用MD5(),而PASSWORD()主要用于修改MySQL的用户密码 自定义函数 用户自定义函数(user-defined function, UDF)是一种对MySQL扩展的途径，其用法与内置函数相同 两个必要条件：参数（可以有0个或多个）、返回值（只能有一个） 函数可以返回任意类型的值，同样可以接收这些类型的参数；参数与返回值之间没有必然的内在联系 创建自定义函数1CREATE FUNCTION function_name RETURNS &#123;STRING | INTEGER | REAL | DECIMAL&#125; routine_body; 删除自定义函数用DROP FUNCTION [IF EXISTS] function_name; 关于函数体routine_body： 函数体由合法的SQL语句构成 函数体可以是简单的SELECT或INSERT语句 函数体如果为复合结构则使用BEGIN...END语句 复合结构可以包含声明、循环、控制结构 例：12CREATE FUNCTION f1() RETURNS VARCHAR(30)RETURN DATE_FORMAT(NOW(),&apos;%Y年%m月%d日 %H点:%i分:%s秒&apos;); 123CREATE FUNCTION f2(num1 SMALLINT UNSIGNED, num2 SMALLINT UNSIGNED)RETURNS FLOAT(10,2) UNSIGNEDRETURN (num1+num2)/2; 有时候需要创建有复合结构函数体的自定义函数，由于分隔符;被认为是SQL语句的结束，所以要先执行DELIMITER //修改SQL语句的结束符1234567CREATE FUNCTION adduser(username VARCHAR(20))RETURNS INT UNSIGNEDBEGININSERT test(username) VALUES (username);RETURN LAST_INSERT_ID();END// 之后就可以把分隔符修改回来和使用函数了：12DELIMITER ;SELECT adduser(&apos;Rose&apos;); 存储过程 存储过程是SQL语句和控制语句的预编译集合，以一个名称存储并作为一个单元处理 存储过程存储在数据库内，可以由应用程序调用执行，允许用户声明变量以及进行流程控制 存储过程可以接收输入和输出类型的参数且可以存在多个返回值 存储过程只在第一次进行语法分析和编译，以后都直接调用编译结果，省略了编译环节 优点： 增强SQL语句的功能和灵活性 实现较快的执行速度 减少网络流量 创建存储过程12345CREATE[DEFINER = &#123;user | CURRENT_USER&#125;]PROCEDURE sp_name ([proc_parameter[,...]])[characteristic ...]routine_body DEFINER是创建者，省略的话默认是当前登录到MySQL的用户 sp_name是存储过程的名字 proc_parameter是参数，可以有0到多个，写法：[IN | OUT | INOUT]param_name type。注意： 参数名不要和数据表字段名重名 IN，表示该参数的值必须在调用存储过程时指定，该值不能被返回 OUT，表示该参数的值可以被存储过程改变，并且可以被返回 INOUT，表示该参数的值在调用时指定，并且可以被改变和返回 characteristic特性，与自定义函数相似： COMMENT：注释 CONTAINS SQL：包含SQL语句，但不包含读或写数据的语句 NO SQL：不包含SQL语句 READS SQL DATA：包含读数据的语句 MODIFIES SQL DATA：包含写数据的语句 SQL SECURITY {DEFINER | INVOKER}：指明谁有权限来执行1COMMENT &apos;string&apos; | &#123;CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA&#125; | SQL SECURITY &#123;DEFINER | INVOKER&#125; routine_body过程体： 过程体由合法的SQL语句构成 过程体可以是任意SQL语句（主要指对记录的增删改查及多表连接等而非对数据库数据表本身的修改） 过程体如果为复合结构则使用BEGIN...END语句 复合结构可以包含声明、循环、控制结构 调用存储过程12CALL sp_name([parameter[,...]]);CALL sp_name[()]; 修改存储过程和修改自定义函数相似，只能修改以下简单的选项，不能修改过程体，要修改过程体只能删除该存储过程然后重新创建1ALTER PROCEDURE sp_name [characteristic ...] COMMENT &apos;string&apos; | &#123;CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA&#125; | SQL SECURITY &#123;DEFINER | INVOKER&#125;; 删除存储过程删除时不用带参数，只用存储过程名称1DROP PROCEDURE [IF EXISTS] sp_name; 创建不带参数的存储过程例：1CREATE PROCEDURE sp1() SELECT VERSION(); 调用：因为不带参数，所以CALL sp1;或CALL sp1();都可以 创建带有IN类型参数的存储过程和创建自定义函数一样要先使用DELIMITER //修改结束符123456789DELIMITER //CREATE PROCEDURE removeUserById(IN p_id INT UNSIGNED)BEGINDELETE FROM test WHERE id = p_id;END//DELIMITER ; 创建带有IN和OUT类型参数的存储过程12345678910DELIMITER //CREATE PROCEDURE removeUserAndReturnUserCount(IN p_id INT UNSIGNED,OUT userCount INT UNSIGNED)BEGINDELETE FROM test WHERE id = p_id;SELECT count(id) FROM test INTO userCount;END//DELIMITER ; 调用：12CALL removeUserAndReturnUserCount(37,@nums);SELECT @nums; 其中，@nums是变量，在BEGIN...END间用DECLEAR声明的变量叫局部变量，作用域只在BEGIN...END之间。而且在BEGIN...END间DECLEAR语句必须位于第一行。而像上面SELECT...INTO para_name或SET @para_name = ...声明的变量叫用户变量，对当前用户使用的客户端有效 创建带有多个OUT类型参数的存储过程ROW_COUNT() 函数可以得到被影响（即插入、删除或更新）的记录数1234567891011DELIMITER //CREATE PROCEDURE removeUserByAgeAndReturnInfos(IN p_age SMALLINT UNSIGNED, OUT deleteUsers SMALLINT UNSIGNED, OUT userCounts SMALLINT UNSIGNED)BEGINDELETE FROM test WHERE age = p_age;SELECT ROW_COUNT() INTO deleteUsers;SELECT COUNT(id) FROM test INTO userCounts;END//DELIMITER ; 调用：12CALL removeUserByAgeAndReturnInfos(20,@a,@b);SELECT @a,@b; 存储过程与自定义函数的区别 存储过程实现的功能要复杂一些；而函数的针对性更强（针对表做操作一般使用存储过程） 存储过程可以返回多个值；函数只能有一个返回值 存储过程一般独立来执行；而函数可以作为其他SQL语句的组成部分来出现 二者注意事项： 创建存储过程或者自定义函数时需要通过delimiter语句修改定界符 如果函数体或过程体有多个语句，需要包含在BEGIN...END语句块中 存储引擎MySQL可以将数据以不同的技术存储在文件（内存）中，这种技术就称为存储引擎。每一种存储引擎使用不同的存储机制、索引技巧、锁定水平，最终提供广泛且不同的功能 分类： MyISAM InnoDB Memory Archive CSV（不支持索引） BlackHole：黑洞引擎，写入的数据都会消失，一般用于做数据复制的中继 并发控制当多个连接对记录进行修改时要保证数据的一致性和完整性，就要使用并发控制。处理并发读或并发写时系统会使用锁系统： 共享锁（读锁）：在同一时间段内，多个用户可以读取同一个资源，读取过程中数据不会发生任何变化 排它锁（写锁）：在任何时候只能有一个用户写入资源，当进行写锁时会阻塞其他的读锁或者写锁操作 锁颗粒： 表锁，是一种开销最小的锁策略 行锁，是一种开销最大的锁策略 事务处理事务用于保证数据库的完整性 事务的特性： 原子性(Atomicity) 一致性(Consistency) 隔离性(Isolation) 持久性(Durability) 外键和索引外键是保证数据一致性的策略；索引是对数据表中一列或多列的值进行排序的一种结构，包括：普通索引、唯一索引、全文索引、btree索引、hash索引…… 各种存储引擎的特点 特点 MyISAM InnoDB Memory Archive 存储限制 256TB 64TB 有 无 事务安全 - 支持 - - 支持索引 支持 支持 支持 - 锁颗粒 表锁 行锁 表锁 行锁 数据压缩 支持 - - 支持 支持外键 - 支持 - - 使用得比较多的是MyISAM和InnoDB。MyISAM适用于事务的处理不多的情况；InnoDB适用于事务处理较多，需要有外键支持的情况 修改存储引擎 通过修改MySQL配置文件实现：default-storage-engine= engine_name，默认是InnoDB 通过创建数据表或修改数据表命令实现：1234CREATE TABLE table_name(......)ENGINE = engine_name; 1ALTER TABLE table_name ENGINE [=] engine_name;]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式的介绍与使用]]></title>
    <url>%2FPython%2F%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[介绍正则表达式是对字符串操作的⼀种逻辑公式，是一种文本模式，它使用单个字符串来描述、匹配一系列匹配某个句法规则的字符串。 一些常见的匹配模式如下： 模式 描述 \w 匹配字母数字及下划线 \W 匹配非字母数字下划线 \s 匹配任意空白字符，等价于 [\t\n\r\f]. \S 匹配任意非空字符 \d 匹配任意数字，等价于 [0-9] \D 匹配任意非数字 \A 匹配字符串开始 \Z 匹配字符串结束，如果是存在换行，只匹配到换行前的结束字符串 \z 匹配字符串结束 \G 匹配最后匹配完成的位置 \n 匹配一个换行符 \t 匹配一个制表符 ^ 匹配字符串的开头 $ 匹配字符串的末尾。 . 匹配任意字符，除了换行符，当re.DOTALL标记被指定时，则可以匹配包括换行符的任意字符。 […] 用来表示一组字符,单独列出：[amk] 匹配 ‘a’，’m’或’k’ [^…] 不在[]中的字符：[^abc] 匹配除了a,b,c之外的字符。 * 匹配0个或多个的表达式。 + 匹配1个或多个的表达式。 ? 匹配0个或1个由前面的正则表达式定义的片段，非贪婪方式 {n} 精确匹配n个前面表达式。 {n, m} 匹配 n 到 m 次由前面的正则表达式定义的片段，贪婪方式 a&#124;b 匹配a或b ( ) 匹配括号内的表达式，也表示一个组 当然正则表达式并不是某种语言独有的，在Python里，它的功能由re模块实现。 re.match()re.match()尝试从字符串的起始位置匹配一个模式，如果字符串不匹配这个模式，则返回None。用法：1re.match(pattern, string, flags=0) 三个参数分别是正则表达式、待匹配字符串和匹配模式。 常规匹配1234567891011121314In [1]: import reIn [2]: content = &apos;Hello 123 4567 World_This is a Regex Demo&apos;In [3]: result = re.match(&apos;^Hello\s\d\d\d\s\d&#123;4&#125;\s\w&#123;10&#125;.*Demo$&apos;, content)In [4]: print(result)&lt;_sre.SRE_Match object; span=(0, 41), match=&apos;Hello 123 4567 World_This is a Regex Demo&apos;&gt;In [5]: print(result.group())Hello 123 4567 World_This is a Regex DemoIn [6]: print(result.span())(0, 41) result.group()返回匹配的内容，result.span()返回匹配的范围。 泛匹配1234567891011121314In [7]: import reIn [8]: content = &apos;Hello 123 4567 World_This is a Regex Demo&apos;In [9]: result = re.match(&apos;^Hello.*Demo$&apos;, content)In [10]: print(result)&lt;_sre.SRE_Match object; span=(0, 41), match=&apos;Hello 123 4567 World_This is a Regex Demo&apos;&gt;In [11]: print(result.group())Hello 123 4567 World_This is a Regex DemoIn [12]: print(result.span())(0, 41) 使用.*也可以匹配出同样的内容。 匹配目标1234567891011121314151617In [13]: import reIn [14]: content = &apos;Hello 1234567 World_This is a Regex Demo&apos;In [15]: result = re.match(&apos;^Hello\s(\d+)\sWorld.*Demo$&apos;, content)In [16]: print(result)&lt;_sre.SRE_Match object; span=(0, 40), match=&apos;Hello 1234567 World_This is a Regex Demo&apos;&gt;In [17]: print(result.group())Hello 1234567 World_This is a Regex DemoIn [18]: print(result.group(1))1234567In [19]: print(result.span())(0, 40) 使用result.group(1)可以把匹配的第一个括号里的内容取出。所以可以把匹配目标使用括号括起来。 贪婪匹配1234567891011In [21]: import reIn [22]: content = &apos;Hello 1234567 World_This is a Regex Demo&apos;In [23]: result = re.match(&apos;^He.*(\d+).*Demo$&apos;, content)In [24]: print(result)&lt;_sre.SRE_Match object; span=(0, 40), match=&apos;Hello 1234567 World_This is a Regex Demo&apos;&gt;In [25]: print(result.group(1))7 贪婪匹配会匹配尽可能多的字符，也就是说He.*(\d+)的.*把数字123456也匹配了。所以result.group(1)结果是7。 非贪婪匹配1234567891011In [26]: import reIn [27]: content = &apos;Hello 1234567 World_This is a Regex Demo&apos;In [28]: result = re.match(&apos;^He.*?(\d+).*Demo$&apos;, content)In [29]: print(result)&lt;_sre.SRE_Match object; span=(0, 40), match=&apos;Hello 1234567 World_This is a Regex Demo&apos;&gt;In [30]: print(result.group(1))1234567 加?变成非贪婪匹配就会尽可能少的匹配字符。所以遇到后面是数字就不会匹配了。result.group(1)结果是1234567。 匹配模式12345678910In [36]: import reIn [37]: content = &apos;&apos;&apos;Hello 1234567 World_This ...: is a Regex Demo ...: &apos;&apos;&apos;In [38]: result = re.match(&apos;^He.*?(\d+).*?Demo$&apos;, content)In [39]: print(result)None 这是因为.是不能匹配换行符的。要匹配换行，需要加一个参数：12345678910In [40]: import reIn [41]: content = &apos;&apos;&apos;Hello 1234567 World_This ...: is a Regex Demo ...: &apos;&apos;&apos;In [42]: result = re.match(&apos;^He.*?(\d+).*?Demo$&apos;, content, re.S)In [43]: print(result.group(1))1234567 转义12345678In [44]: import reIn [45]: content = &apos;price is $5.00&apos;In [46]: result = re.match(&apos;price is $5.00&apos;, content)In [47]: print(result)None 因为$和.是特殊字符，由特殊字符需要使用\转义：12345678In [48]: import reIn [49]: content = &apos;price is $5.00&apos;In [50]: result = re.match(&apos;price is \$5\.00&apos;, content)In [51]: print(result)&lt;_sre.SRE_Match object; span=(0, 14), match=&apos;price is $5.00&apos;&gt; 小结在使用时尽量使用泛匹配、使用括号得到匹配目标、尽量使用非贪婪模式、有换行符就用re.S。 另外，使用re.match()要注意，它是从首个字符开始匹配的，如果第一个字符不匹配，那么它就会返回None了。这时，就要使用下面的方法了。 re.search()re.search()则是扫描整个字符串并返回第一个成功的匹配。例如：12345678In [52]: import reIn [53]: content = &apos;Extra stings Hello 1234567 World_This is a Regex Demo Extra stings&apos;In [54]: result = re.match(&apos;Hello.*?(\d+).*?Demo&apos;, content)In [55]: print(result)None 1234567891011In [56]: import reIn [57]: content = &apos;Extra stings Hello 1234567 World_This is a Regex Demo Extra stings&apos;In [58]: result = re.search(&apos;Hello.*?(\d+).*?Demo&apos;, content)In [59]: print(result)&lt;_sre.SRE_Match object; span=(13, 53), match=&apos;Hello 1234567 World_This is a Regex Demo&apos;&gt;In [60]: print(result.group(1))1234567 所以，为匹配方便，能用re.search()就不用re.match()。 实例123456789101112131415161718192021222324252627In [62]: html = &apos;&apos;&apos;&lt;div id=&quot;songs-list&quot;&gt; ...: &lt;h2 class=&quot;title&quot;&gt;经典老歌&lt;/h2&gt; ...: &lt;p class=&quot;introduction&quot;&gt; ...: 经典老歌列表 ...: &lt;/p&gt; ...: &lt;ul id=&quot;list&quot; class=&quot;list-group&quot;&gt; ...: &lt;li data-view=&quot;2&quot;&gt;一路上有你&lt;/li&gt; ...: &lt;li data-view=&quot;7&quot;&gt; ...: &lt;a href=&quot;/2.mp3&quot; singer=&quot;任贤齐&quot;&gt;沧海一声笑&lt;/a&gt; ...: &lt;/li&gt; ...: &lt;li data-view=&quot;4&quot; class=&quot;active&quot;&gt; ...: &lt;a href=&quot;/3.mp3&quot; singer=&quot;齐秦&quot;&gt;往事随风&lt;/a&gt; ...: &lt;/li&gt; ...: &lt;li data-view=&quot;6&quot;&gt;&lt;a href=&quot;/4.mp3&quot; singer=&quot;beyond&quot;&gt;光辉岁月&lt;/a&gt;&lt;/li&gt; ...: &lt;li data-view=&quot;5&quot;&gt;&lt;a href=&quot;/5.mp3&quot; singer=&quot;陈慧琳&quot;&gt;记事本&lt;/a&gt;&lt;/li&gt; ...: &lt;li data-view=&quot;5&quot;&gt; ...: &lt;a href=&quot;/6.mp3&quot; singer=&quot;邓丽君&quot;&gt;&lt;i class=&quot;fa fa-user&quot;&gt;&lt;/i&gt;但愿人长久&lt;/a&gt; ...: &lt;/li&gt; ...: &lt;/ul&gt; ...: &lt;/div&gt;&apos;&apos;&apos;In [63]: result = re.search(&apos;&lt;li.*?active.*?singer=&quot;(.*?)&quot;&gt;(.*?)&lt;/a&gt;&apos;, html, re.S)In [64]: if result: ...: print(result.group(1), result.group(2)) ...:齐秦 往事随风 1234567891011121314151617181920212223242526272829In [65]: import reIn [66]: html = &apos;&apos;&apos;&lt;div id=&quot;songs-list&quot;&gt; ...: &lt;h2 class=&quot;title&quot;&gt;经典老歌&lt;/h2&gt; ...: &lt;p class=&quot;introduction&quot;&gt; ...: 经典老歌列表 ...: &lt;/p&gt; ...: &lt;ul id=&quot;list&quot; class=&quot;list-group&quot;&gt; ...: &lt;li data-view=&quot;2&quot;&gt;一路上有你&lt;/li&gt; ...: &lt;li data-view=&quot;7&quot;&gt; ...: &lt;a href=&quot;/2.mp3&quot; singer=&quot;任贤齐&quot;&gt;沧海一声笑&lt;/a&gt; ...: &lt;/li&gt; ...: &lt;li data-view=&quot;4&quot; class=&quot;active&quot;&gt; ...: &lt;a href=&quot;/3.mp3&quot; singer=&quot;齐秦&quot;&gt;往事随风&lt;/a&gt; ...: &lt;/li&gt; ...: &lt;li data-view=&quot;6&quot;&gt;&lt;a href=&quot;/4.mp3&quot; singer=&quot;beyond&quot;&gt;光辉岁月&lt;/a&gt;&lt;/li&gt; ...: &lt;li data-view=&quot;5&quot;&gt;&lt;a href=&quot;/5.mp3&quot; singer=&quot;陈慧琳&quot;&gt;记事本&lt;/a&gt;&lt;/li&gt; ...: &lt;li data-view=&quot;5&quot;&gt; ...: &lt;a href=&quot;/6.mp3&quot; singer=&quot;邓丽君&quot;&gt;但愿人长久&lt;/a&gt; ...: &lt;/li&gt; ...: &lt;/ul&gt; ...: &lt;/div&gt;&apos;&apos;&apos;In [67]: result = re.search(&apos;&lt;li.*?singer=&quot;(.*?)&quot;&gt;(.*?)&lt;/a&gt;&apos;, html, re.S)In [68]: if result: ...: print(result.group(1), result.group(2)) ...:任贤齐 沧海一声笑 1234567891011121314151617181920212223242526272829In [69]: import reIn [70]: html = &apos;&apos;&apos;&lt;div id=&quot;songs-list&quot;&gt; ...: &lt;h2 class=&quot;title&quot;&gt;经典老歌&lt;/h2&gt; ...: &lt;p class=&quot;introduction&quot;&gt; ...: 经典老歌列表 ...: &lt;/p&gt; ...: &lt;ul id=&quot;list&quot; class=&quot;list-group&quot;&gt; ...: &lt;li data-view=&quot;2&quot;&gt;一路上有你&lt;/li&gt; ...: &lt;li data-view=&quot;7&quot;&gt; ...: &lt;a href=&quot;/2.mp3&quot; singer=&quot;任贤齐&quot;&gt;沧海一声笑&lt;/a&gt; ...: &lt;/li&gt; ...: &lt;li data-view=&quot;4&quot; class=&quot;active&quot;&gt; ...: &lt;a href=&quot;/3.mp3&quot; singer=&quot;齐秦&quot;&gt;往事随风&lt;/a&gt; ...: &lt;/li&gt; ...: &lt;li data-view=&quot;6&quot;&gt;&lt;a href=&quot;/4.mp3&quot; singer=&quot;beyond&quot;&gt;光辉岁月&lt;/a&gt;&lt;/li&gt; ...: &lt;li data-view=&quot;5&quot;&gt;&lt;a href=&quot;/5.mp3&quot; singer=&quot;陈慧琳&quot;&gt;记事本&lt;/a&gt;&lt;/li&gt; ...: &lt;li data-view=&quot;5&quot;&gt; ...: &lt;a href=&quot;/6.mp3&quot; singer=&quot;邓丽君&quot;&gt;但愿人长久&lt;/a&gt; ...: &lt;/li&gt; ...: &lt;/ul&gt; ...: &lt;/div&gt;&apos;&apos;&apos;In [71]: result = re.search(&apos;&lt;li.*?singer=&quot;(.*?)&quot;&gt;(.*?)&lt;/a&gt;&apos;, html)In [72]: if result: ...: print(result.group(1), result.group(2)) ...:beyond 光辉岁月 re.findall()re.findall()搜索字符串，以列表形式返回全部能匹配的子串。123456789101112131415161718192021222324252627282930313233343536373839404142434445In [73]: import reIn [74]: html = &apos;&apos;&apos;&lt;div id=&quot;songs-list&quot;&gt; ...: &lt;h2 class=&quot;title&quot;&gt;经典老歌&lt;/h2&gt; ...: &lt;p class=&quot;introduction&quot;&gt; ...: 经典老歌列表 ...: &lt;/p&gt; ...: &lt;ul id=&quot;list&quot; class=&quot;list-group&quot;&gt; ...: &lt;li data-view=&quot;2&quot;&gt;一路上有你&lt;/li&gt; ...: &lt;li data-view=&quot;7&quot;&gt; ...: &lt;a href=&quot;/2.mp3&quot; singer=&quot;任贤齐&quot;&gt;沧海一声笑&lt;/a&gt; ...: &lt;/li&gt; ...: &lt;li data-view=&quot;4&quot; class=&quot;active&quot;&gt; ...: &lt;a href=&quot;/3.mp3&quot; singer=&quot;齐秦&quot;&gt;往事随风&lt;/a&gt; ...: &lt;/li&gt; ...: &lt;li data-view=&quot;6&quot;&gt;&lt;a href=&quot;/4.mp3&quot; singer=&quot;beyond&quot;&gt;光辉岁月&lt;/a&gt;&lt;/li&gt; ...: &lt;li data-view=&quot;5&quot;&gt;&lt;a href=&quot;/5.mp3&quot; singer=&quot;陈慧琳&quot;&gt;记事本&lt;/a&gt;&lt;/li&gt; ...: &lt;li data-view=&quot;5&quot;&gt; ...: &lt;a href=&quot;/6.mp3&quot; singer=&quot;邓丽君&quot;&gt;但愿人长久&lt;/a&gt; ...: &lt;/li&gt; ...: &lt;/ul&gt; ...: &lt;/div&gt;&apos;&apos;&apos;In [75]: results = re.findall(&apos;&lt;li.*?href=&quot;(.*?)&quot;.*?singer=&quot;(.*?)&quot;&gt;(.*?)&lt;/a&gt;&apos;, html, re.S)In [76]: print(results)[(&apos;/2.mp3&apos;, &apos;任贤齐&apos;, &apos;沧海一声笑&apos;), (&apos;/3.mp3&apos;, &apos;齐秦&apos;, &apos;往事随风&apos;), (&apos;/4.mp3&apos;, &apos;beyond&apos;, &apos;光辉岁月&apos;), (&apos;/5.mp3&apos;, &apos;陈慧 琳&apos;, &apos;记事本&apos;), (&apos;/6.mp3&apos;, &apos;邓丽君&apos;, &apos;但愿人长久&apos;)]In [77]: print(type(results))&lt;class &apos;list&apos;&gt;In [78]: for result in results: ...: print(type(result)) ...: print(result[0], result[1], result[2]) ...:&lt;class &apos;tuple&apos;&gt;/2.mp3 任贤齐 沧海一声笑&lt;class &apos;tuple&apos;&gt;/3.mp3 齐秦 往事随风&lt;class &apos;tuple&apos;&gt;/4.mp3 beyond 光辉岁月&lt;class &apos;tuple&apos;&gt;/5.mp3 陈慧琳 记事本&lt;class &apos;tuple&apos;&gt;/6.mp3 邓丽君 但愿人长久 12345678910111213141516171819202122232425262728293031323334353637In [79]: import reIn [80]: html = &apos;&apos;&apos;&lt;div id=&quot;songs-list&quot;&gt; ...: &lt;h2 class=&quot;title&quot;&gt;经典老歌&lt;/h2&gt; ...: &lt;p class=&quot;introduction&quot;&gt; ...: 经典老歌列表 ...: &lt;/p&gt; ...: &lt;ul id=&quot;list&quot; class=&quot;list-group&quot;&gt; ...: &lt;li data-view=&quot;2&quot;&gt;一路上有你&lt;/li&gt; ...: &lt;li data-view=&quot;7&quot;&gt; ...: &lt;a href=&quot;/2.mp3&quot; singer=&quot;任贤齐&quot;&gt;沧海一声笑&lt;/a&gt; ...: &lt;/li&gt; ...: &lt;li data-view=&quot;4&quot; class=&quot;active&quot;&gt; ...: &lt;a href=&quot;/3.mp3&quot; singer=&quot;齐秦&quot;&gt;往事随风&lt;/a&gt; ...: &lt;/li&gt; ...: &lt;li data-view=&quot;6&quot;&gt;&lt;a href=&quot;/4.mp3&quot; singer=&quot;beyond&quot;&gt;光辉岁月&lt;/a&gt;&lt;/li&gt; ...: &lt;li data-view=&quot;5&quot;&gt;&lt;a href=&quot;/5.mp3&quot; singer=&quot;陈慧琳&quot;&gt;记事本&lt;/a&gt;&lt;/li&gt; ...: &lt;li data-view=&quot;5&quot;&gt; ...: &lt;a href=&quot;/6.mp3&quot; singer=&quot;邓丽君&quot;&gt;但愿人长久&lt;/a&gt; ...: &lt;/li&gt; ...: &lt;/ul&gt; ...: &lt;/div&gt;&apos;&apos;&apos;In [81]: results = re.findall(&apos;&lt;li.*?&gt;\s*?(&lt;a.*?&gt;)?(\w+)(&lt;/a&gt;)?\s*?&lt;/li&gt;&apos;, html, re.S)In [82]: print(results)[(&apos;&apos;, &apos;一路上有你&apos;, &apos;&apos;), (&apos;&lt;a href=&quot;/2.mp3&quot; singer=&quot;任贤齐&quot;&gt;&apos;, &apos;沧海一声笑&apos;, &apos;&lt;/a&gt;&apos;), (&apos;&lt;a href=&quot;/3.mp3&quot; singer=&quot;齐秦&quot;&gt;&apos;, &apos;往事随风&apos;, &apos;&lt;/a&gt;&apos;), (&apos;&lt;a href=&quot;/4.mp3&quot; singer=&quot;beyond&quot;&gt;&apos;, &apos;光辉岁月&apos;, &apos;&lt;/a&gt;&apos;), (&apos;&lt;a href=&quot;/5.mp3&quot; singer=&quot;陈慧琳&quot;&gt;&apos;, &apos;记事本&apos;, &apos;&lt;/a&gt;&apos;), (&apos;&lt;a href=&quot;/6.mp3&quot; singer=&quot;邓丽君&quot;&gt;&apos;, &apos;但愿人长久&apos;, &apos;&lt;/a&gt;&apos;)]In [83]: for result in results: ...: print(result[1]) ...:一路上有你沧海一声笑往事随风光辉岁月记事本但愿人长久 re.sub()re.sub()替换字符串中每一个匹配的子串后返回替换后的字符串。12345678In [84]: import reIn [85]: content = &apos;Extra stings Hello 1234567 World_This is a Regex Demo Extra stings&apos;In [86]: content = re.sub(&apos;\d+&apos;, &apos;&apos;, content)In [87]: print(content)Extra stings Hello World_This is a Regex Demo Extra stings re.sub(&#39;\d+&#39;, &#39;&#39;, content)把content里的数字替换成空。123456In [89]: content = &apos;Extra stings Hello 1234567 World_This is a Regex Demo Extra stings&apos;In [90]: content = re.sub(&apos;\d+&apos;, &apos;Replacement&apos;, content)In [91]: print(content)Extra stings Hello Replacement World_This is a Regex Demo Extra stings re.sub(&#39;\d+&#39;, &#39;Replacement&#39;, content)把content里的数字替换成Replacement。12345678In [92]: import reIn [93]: content = &apos;Extra stings Hello 1234567 World_This is a Regex Demo Extra stings&apos;In [94]: content = re.sub(&apos;(\d+)&apos;, r&apos;\1 8910&apos;, content)In [95]: print(content)Extra stings Hello 1234567 8910 World_This is a Regex Demo Extra stings re.sub(&#39;(\d+)&#39;, r&#39;\1 8910&#39;, content):(\d+)使用括号把数字变成一个组，\1则可以取出第一组的内容，因为\是转义字符，所以要在前面加一个r表示非转义的原始字符串。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061In [96]: import reIn [97]: html = &apos;&apos;&apos;&lt;div id=&quot;songs-list&quot;&gt; ...: &lt;h2 class=&quot;title&quot;&gt;经典老歌&lt;/h2&gt; ...: &lt;p class=&quot;introduction&quot;&gt; ...: 经典老歌列表 ...: &lt;/p&gt; ...: &lt;ul id=&quot;list&quot; class=&quot;list-group&quot;&gt; ...: &lt;li data-view=&quot;2&quot;&gt;一路上有你&lt;/li&gt; ...: &lt;li data-view=&quot;7&quot;&gt; ...: &lt;a href=&quot;/2.mp3&quot; singer=&quot;任贤齐&quot;&gt;沧海一声笑&lt;/a&gt; ...: &lt;/li&gt; ...: &lt;li data-view=&quot;4&quot; class=&quot;active&quot;&gt; ...: &lt;a href=&quot;/3.mp3&quot; singer=&quot;齐秦&quot;&gt;往事随风&lt;/a&gt; ...: &lt;/li&gt; ...: &lt;li data-view=&quot;6&quot;&gt;&lt;a href=&quot;/4.mp3&quot; singer=&quot;beyond&quot;&gt;光辉岁月&lt;/a&gt;&lt;/li&gt; ...: &lt;li data-view=&quot;5&quot;&gt;&lt;a href=&quot;/5.mp3&quot; singer=&quot;陈慧琳&quot;&gt;记事本&lt;/a&gt;&lt;/li&gt; ...: &lt;li data-view=&quot;5&quot;&gt; ...: &lt;a href=&quot;/6.mp3&quot; singer=&quot;邓丽君&quot;&gt;但愿人长久&lt;/a&gt; ...: &lt;/li&gt; ...: &lt;/ul&gt; ...: &lt;/div&gt;&apos;&apos;&apos;In [98]: html = re.sub(&apos;&lt;a.*?&gt;|&lt;/a&gt;&apos;, &apos;&apos;, html)In [99]: print(html)&lt;div id=&quot;songs-list&quot;&gt; &lt;h2 class=&quot;title&quot;&gt;经典老歌&lt;/h2&gt; &lt;p class=&quot;introduction&quot;&gt; 经典老歌列表 &lt;/p&gt; &lt;ul id=&quot;list&quot; class=&quot;list-group&quot;&gt; &lt;li data-view=&quot;2&quot;&gt;一路上有你&lt;/li&gt; &lt;li data-view=&quot;7&quot;&gt; 沧海一声笑 &lt;/li&gt; &lt;li data-view=&quot;4&quot; class=&quot;active&quot;&gt; 往事随风 &lt;/li&gt; &lt;li data-view=&quot;6&quot;&gt;光辉岁月&lt;/li&gt; &lt;li data-view=&quot;5&quot;&gt;记事本&lt;/li&gt; &lt;li data-view=&quot;5&quot;&gt; 但愿人长久 &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt;In [100]: results = re.findall(&apos;&lt;li.*?&gt;(.*?)&lt;/li&gt;&apos;, html, re.S)In [101]: print(results)[&apos;一路上有你&apos;, &apos;\n 沧海一声笑\n &apos;, &apos;\n 往事随风\n &apos;, &apos;光辉岁月&apos;, &apos;记事本&apos;, &apos;\n 但愿人长久\n &apos;]In [102]: for result in results: ...: print(result.strip()) ...:一路上有你沧海一声笑往事随风光辉岁月记事本但愿人长久 上面的例子先把&lt;a&gt;标签替换成空，再提取歌名，逻辑比较清晰。 re.compile()re.compile()将正则字符串编译成正则表达式对象,以便于复用该匹配模式。1234567891011In [103]: import reIn [104]: content = &apos;&apos;&apos;Hello 1234567 World_This ...: is a Regex Demo&apos;&apos;&apos;In [105]: pattern = re.compile(&apos;Hello.*Demo&apos;, re.S)In [106]: result = re.match(pattern, content)In [107]: print(result)&lt;_sre.SRE_Match object; span=(0, 40), match=&apos;Hello 1234567 World_This\nis a Regex Demo&apos;&gt; 实战12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849In [142]: import requests ...: import re ...: content = requests.get(&apos;https://book.douban.com/&apos;).text ...: results = re.findall(&apos;&lt;li.*?&quot;cover&quot;&gt;.*?href=&quot;(.*?)&quot;\stitle=&quot;(.*?)&quot;&gt;.*?author&quot;&gt;(.*?)&lt;/div&gt;&apos;, content, re.S) ...: for result in results: ...: url, name, author = result ...: author = re.sub(&apos;\s&apos;, &apos;&apos;, author) ...: print(url, name, author) ...:https://book.douban.com/subject/27147922/?icn=index-editionrecommend 南极 [爱尔兰]克莱尔·吉根https://book.douban.com/subject/27174130/?icn=index-editionrecommend 北野武的小酒馆 [日]北野武https://book.douban.com/subject/27617213/?icn=index-editionrecommend 手把手教你玩脱口秀 [美]格雷格·迪安https://book.douban.com/subject/27200268/?icn=index-editionrecommend 知日·世上只有一个京都 茶乌龙/主编https://book.douban.com/subject/27622571/?icn=index-editionrecommend 独立日：每天画一只小动物 元元https://book.douban.com/subject/27662422/?icn=index-latestbook-subject 台风天 陆茵茵https://book.douban.com/subject/30125605/?icn=index-latestbook-subject 从六艺到十三经 程苏东https://book.douban.com/subject/27593063/?icn=index-latestbook-subject 大河恋 [美]诺曼·麦克林恩https://book.douban.com/subject/26983954/?icn=index-latestbook-subject 文艺复兴 [英]杰克·古迪https://book.douban.com/subject/27601507/?icn=index-latestbook-subject 神枪手迪克 [美]库尔特·冯内古特https://book.douban.com/subject/27204782/?icn=index-latestbook-subject 对白 [美]罗伯特·麦基https://book.douban.com/subject/27199023/?icn=index-latestbook-subject 穿过欲望之城 [英]莫欣·哈米德https://book.douban.com/subject/27203767/?icn=index-latestbook-subject 柏拉图和鸭嘴兽一起去酒吧 [美]托马斯·卡斯卡特&amp;nbsp;/&amp;nbsp;[美]丹尼尔·克莱恩https://book.douban.com/subject/27108849/?icn=index-latestbook-subject 月光下的旅人 [匈]瑟尔伯·昂托https://book.douban.com/subject/27613219/?icn=index-latestbook-subject 生活上瘾指南 姚瑶https://book.douban.com/subject/27602480/?icn=index-latestbook-subject 狗女婿上门 [日]多和田叶子https://book.douban.com/subject/27616140/?icn=index-latestbook-subject 诗与它的山河 萧驰https://book.douban.com/subject/27600474/?icn=index-latestbook-subject 月光狂想曲 [美]迈克尔·夏邦https://book.douban.com/subject/27131695/?icn=index-latestbook-subject 一天中的百万年 [英]格雷格·詹纳https://book.douban.com/subject/26975428/?icn=index-latestbook-subject 故园风雨后 [英]伊夫林·沃https://book.douban.com/subject/27592521/?icn=index-latestbook-subject 简明大历史 [英]伊恩·克夫顿&amp;nbsp;/&amp;nbsp;[英]杰里 米·布莱克https://book.douban.com/subject/26274413/?icn=index-latestbook-subject 军方的怪物 [英]肯·福莱特https://book.douban.com/subject/27108800/?icn=index-latestbook-subject 纸上动物园 [英]夏洛特·斯莱https://book.douban.com/subject/27199470/?icn=index-latestbook-subject 刺杀骑士团长 [日]村上春树https://book.douban.com/subject/27624387/?icn=index-latestbook-subject 异类的天赋 [英]凯文·达顿https://book.douban.com/subject/27598516/?icn=index-latestbook-subject 批评家之死 [德]马丁·瓦尔泽https://book.douban.com/subject/27201290/?icn=index-latestbook-subject 啊！这样就能辞职了 [日]安倍夜郎https://book.douban.com/subject/27147920/?icn=index-latestbook-subject 皇室料理番 [日]杉森久英https://book.douban.com/subject/27131100/?icn=index-latestbook-subject 小王子的领悟 周保松&amp;nbsp;/&amp;nbsp;区华欣(插画)https://book.douban.com/subject/27179220/?icn=index-latestbook-subject 伤心咖啡馆之歌 [美]卡森·麦卡勒斯https://book.douban.com/subject/27156383/?icn=index-latestbook-subject 深度影响 [新加坡]凯伦·梁https://book.douban.com/subject/27120445/?icn=index-latestbook-subject 蒲公英王朝 [美]刘宇昆https://book.douban.com/subject/27622006/?icn=index-latestbook-subject 投资中不简单的事 邱国鹭&amp;nbsp;/&amp;nbsp;邓晓峰&amp;nbsp;/&amp;nbsp;卓利伟&amp;nbsp;/&amp;nbsp;孙庆瑞&amp;nbsp;/&amp;nbsp;冯柳&amp;nbsp;/&amp;nbsp;王世宏https://book.douban.com/subject/27621062/?icn=index-latestbook-subject 星之城堡：1869征服太空 [法]亚历克斯·埃利斯https://book.douban.com/subject/27101663/?icn=index-latestbook-subject 权力与特权 [美]格尔哈特·伦斯基https://book.douban.com/subject/27601754/?icn=index-latestbook-subject 大军师司马懿之军师联盟 常江https://book.douban.com/subject/27589366/?icn=index-latestbook-subject 马家辉家行散记 马家辉&amp;nbsp;/&amp;nbsp;张家瑜https://book.douban.com/subject/27175514/?icn=index-latestbook-subject 复明症漫记 [葡]若泽·萨拉马戈https://book.douban.com/subject/27593408/?icn=index-latestbook-subject 寡头 [美]戴维·霍夫曼https://book.douban.com/subject/27128590/?icn=index-latestbook-subject 阴郁的美男子 [法]朱利安·格拉克 结语更多关于正则表达式re模块的内容可以参考官方文档]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>正则表达式</tag>
      </tags>
  </entry>
</search>
